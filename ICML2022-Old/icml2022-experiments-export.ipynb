{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICML Paper Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all the ICML paper stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# python stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import scipy.sparse\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbs\n",
    "\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (16, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCI Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads and evalutes the models on some UCI datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load datasets and their characteristics into a `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata, load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "datasets = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# auto mpg dataset\n",
    "# https://archive.ics.uci.edu/ml/datasets/Auto+MPG\n",
    "data = fetch_mldata('mpg')\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "X_tr = scaler_x.fit_transform(X_tr)\n",
    "X_ts = scaler_x.transform(X_ts)\n",
    "\n",
    "# scaler_y = StandardScaler()\n",
    "# y_tr = scaler_y.fit_transform(y_tr[:,np.newaxis]).flatten()\n",
    "# y_ts = scaler_y.transform(y_ts[:,np.newaxis]).flatten()\n",
    "\n",
    "datasets['mpg'] = X_tr, y_tr, X_ts, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(True)\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "X_tr = scaler_x.fit_transform(X_tr)\n",
    "X_ts = scaler_x.transform(X_ts)\n",
    "\n",
    "# scaler_y = StandardScaler()\n",
    "# y_tr = scaler_y.fit_transform(y_tr[:,np.newaxis]).flatten()\n",
    "# y_ts = scaler_y.transform(y_ts[:,np.newaxis]).flatten()\n",
    "\n",
    "datasets['boston'] = X_tr, y_tr, X_ts, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data'\n",
    "df = pd.read_csv(url, delim_whitespace=True, header=None)\n",
    "df.columns = [str(i) for i in range(6)] + ['y']\n",
    "\n",
    "X = df.iloc[:,:6].as_matrix()\n",
    "y = df.iloc[:,6].as_matrix()\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_ts = scaler.transform(X_ts)\n",
    "\n",
    "# scaler_y = StandardScaler()\n",
    "# y_tr = scaler_y.fit_transform(y_tr[:,np.newaxis]).flatten()\n",
    "# y_ts = scaler_y.transform(y_ts[:,np.newaxis]).flatten()\n",
    "\n",
    "datasets['yacht'] = X_tr, y_tr, X_ts, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# https://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
    "url1 = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "# url2 = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "df1 = pd.read_csv(url1, sep=';')\n",
    "# df.columns = [str(i) for i in range(6)] + ['y']\n",
    "\n",
    "X = df1.iloc[:,:-6].as_matrix()\n",
    "y = df1.iloc[:,-1].as_matrix()\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_ts = scaler.transform(X_ts)\n",
    "\n",
    "# scaler_y = StandardScaler()\n",
    "# y_tr = scaler_y.fit_transform(y_tr[:,np.newaxis]).flatten()\n",
    "# y_ts = scaler_y.transform(y_ts[:,np.newaxis]).flatten()\n",
    "\n",
    "datasets['wine'] = X_tr, y_tr, X_ts, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1599, 6), (1599,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data\"\n",
    "df = pd.read_csv(url, header=None)\n",
    "df = df.replace('?', 0)\n",
    "\n",
    "X = df1.iloc[:,5:-1].as_matrix()\n",
    "y = df1.iloc[:,-1].as_matrix()\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_ts = scaler.transform(X_ts)\n",
    "\n",
    "# scaler_y = StandardScaler()\n",
    "# y_tr = scaler_y.fit_transform(y_tr[:,np.newaxis]).flatten()\n",
    "# y_ts = scaler_y.transform(y_ts[:,np.newaxis]).flatten()\n",
    "\n",
    "datasets['crime'] = X_tr, y_tr, X_ts, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # http://mldata.org/repository/data/viewslug/stockvalues/\n",
    "# data = fetch_mldata('stockvalues')\n",
    "\n",
    "# X = data.data[:,:-1]\n",
    "# y = data.data[:,-1]\n",
    "\n",
    "# X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_tr = scaler.fit_transform(X_tr)\n",
    "# X_ts = scaler.transform(X_ts)\n",
    "\n",
    "# scaler_y = StandardScaler()\n",
    "# y_tr = scaler_y.fit_transform(y_tr[:,np.newaxis]).flatten() + 5\n",
    "# y_ts = scaler_y.transform(y_ts[:,np.newaxis]).flatten() + 5\n",
    "\n",
    "# datasets['stocks'] = X_tr, y_tr, X_ts, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 159)\n",
      "(159, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# http://mldata.org/repository/data/viewslug/regression-datasets-auto_price/\n",
    "data = fetch_mldata('regression-datasets auto_price')\n",
    "print(data.target.shape)\n",
    "print(data.data.shape)\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_ts = scaler.transform(X_ts)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_tr = scaler_y.fit_transform(y_tr[:,np.newaxis]).flatten() + 10\n",
    "y_ts = scaler_y.transform(y_ts[:,np.newaxis]).flatten() + 10\n",
    "\n",
    "datasets['auto'] = X_tr, y_tr, X_ts, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# http://mldata.org/repository/data/viewslug/regression-datasets-machine_cpu/\n",
    "data = fetch_mldata('regression-datasets machine_cpu')\n",
    "\n",
    "X = data.data[:,:-1]\n",
    "y = data.data[:,-1]\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_ts = scaler.transform(X_ts)\n",
    "\n",
    "# scaler_y = StandardScaler()\n",
    "# y_tr = scaler_y.fit_transform(y_tr[:,np.newaxis]).flatten()\n",
    "# y_ts = scaler_y.transform(y_ts[:,np.newaxis]).flatten()\n",
    "\n",
    "datasets['cpu'] = X_tr, y_tr, X_ts, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8192, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# http://mldata.org/repository/data/viewslug/regression-datasets-bank8fm/\n",
    "data = fetch_mldata('regression-datasets bank8FM')\n",
    "print(data.data.shape)\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_ts = scaler.transform(X_ts)\n",
    "\n",
    "# scaler_y = StandardScaler()\n",
    "# y_tr = scaler_y.fit_transform(y_tr[:,np.newaxis]).flatten()\n",
    "# y_ts = scaler_y.transform(y_ts[:,np.newaxis]).flatten()\n",
    "\n",
    "datasets['bank'] = X_tr, y_tr, X_ts, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 23)\n",
      "(194,)\n",
      "dict_keys(['DESCR', 'COL_NAMES', 'int2', 'double3', 'int4', 'target', 'data'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# http://mldata.org/repository/data/viewslug/regression-datasets-wisconsin/\n",
    "data = fetch_mldata('regression-datasets wisconsin')\n",
    "\n",
    "print(data.data.shape)\n",
    "print(data.target.shape)\n",
    "print(data.keys())\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_ts = scaler.transform(X_ts)\n",
    "\n",
    "# scaler_y = StandardScaler()\n",
    "# y_tr = scaler_y.fit_transform(y_tr[:,np.newaxis]).flatten()\n",
    "# y_ts = scaler_y.transform(y_ts[:,np.newaxis]).flatten()\n",
    "\n",
    "datasets['wisconsin'] = X_tr, y_tr, X_ts, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = fetch_mldata('regression-datasets kin8nm')\n",
    "\n",
    "# print(data.data.shape)\n",
    "# print(data.keys())\n",
    "\n",
    "# X = data.data[:,:-1]\n",
    "# y = data.data[:,-1]\n",
    "\n",
    "# X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_tr = scaler.fit_transform(X_tr)\n",
    "# X_ts = scaler.transform(X_ts)\n",
    "\n",
    "# # scaler_y = StandardScaler()\n",
    "# # y_tr = scaler_y.fit_transform(y_tr[:,np.newaxis]).flatten()\n",
    "# # y_ts = scaler_y.transform(y_ts[:,np.newaxis]).flatten()\n",
    "\n",
    "# datasets['kinematics'] = X_tr, y_tr, X_ts, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpg 392 7\n",
      "boston 506 13\n",
      "yacht 308 6\n",
      "wine 1599 6\n",
      "crime 1599 6\n",
      "auto 1599 6\n",
      "cpu 209 6\n",
      "bank 209 6\n",
      "wisconsin 194 23\n"
     ]
    }
   ],
   "source": [
    "for dataset, (X_tr, y_tr, X_ts, y_ts) in datasets.items():\n",
    "    print(dataset, X_tr.shape[0] + X_ts.shape[0], X_ts.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test On One Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_calibration_error(p_exp, p_obs):\n",
    "    \"\"\"Generate calibration plot.\n",
    "\n",
    "      Parameters\n",
    "      ----------\n",
    "      p_exp : array-like of float\n",
    "          The expected probailities\n",
    "      p_obs : array-like of float\n",
    "          The true (observed) probabilities\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((p_exp - p_obs) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-28 00:11:10] {init_logging.py:65} WARNING afresh-core dependencies are out-of-date; run `make install`\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import \\\n",
    "(\n",
    "    mean_squared_log_error,\n",
    "    mean_absolute_error,\n",
    "    explained_variance_score,\n",
    "    r2_score\n",
    ")\n",
    "from src.eval import mean_calibration_error2\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    delta = np.abs(y_true-y_pred)\n",
    "    abs_y = np.abs(y_true)\n",
    "    errs = delta[abs_y>0] / (abs_y[abs_y>0] + 1e-4)\n",
    "    errs = errs[ np.abs(errs - errs.mean()) < 3*errs.std() ]\n",
    "    return np.mean(errs)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(y_true-y_pred)**2)\n",
    "\n",
    "metrics = {\n",
    "#     'msle' : mean_squared_log_error,\n",
    "    'mae' : mean_absolute_error,\n",
    "    'var' : explained_variance_score,\n",
    "    'r2' : r2_score,\n",
    "    'mape' : mean_absolute_percentage_error,\n",
    "    'rmse' : root_mean_squared_error,\n",
    "}\n",
    "\n",
    "from src.linreg import BayesianLinearRegressor\n",
    "from src.dnn import BayesianDNNForecaster\n",
    "from src.dnn2 import CalibratedBayesianDNNForecaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Bayesian DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianLinearRegressor(quantiles=[0.2, 0.4, 0.5, 0.6, 0.8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = BayesianLinearRegressor()\n",
    "# clf = BayesianDNNForecaster()\n",
    "X_tr, y_tr, X_ts, y_ts = datasets['boston']\n",
    "clf.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_pred = clf.predict(X_tr)['y_pred']\n",
    "y_ts_pred = clf.predict(X_ts)['y_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae\n",
      "  Train 3.1267120301975693\n",
      "  Test 3.4250765742195806\n",
      "var\n",
      "  Train 0.7445309236034652\n",
      "  Test 0.7232751504667847\n",
      "r2\n",
      "  Train 0.7445309236034652\n",
      "  Test 0.7124413578665383\n",
      "mape\n",
      "  Train 0.1380923796214813\n",
      "  Test 0.15479363611996366\n",
      "rmse\n",
      "  Train 1.246730657732102e-15\n",
      "  Test 0.9952790297515476\n"
     ]
    }
   ],
   "source": [
    "for name, metric in metrics.items():\n",
    "    print(name)\n",
    "    print('  Train', metric(y_tr, y_tr_pred))\n",
    "    print('  Test', metric(y_ts, y_ts_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "def prep_calibration(model, df_data, y, q_values=np.arange(0, 1.05, 0.1)):\n",
    "    \"\"\"Prepare data for calibration plot.\n",
    "\n",
    "    Model must implement predict_quantile method, which must return a\n",
    "    dataframe with a 'y_pred' column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        The model we want to evaluate.\n",
    "    df_data : pandas.DataFrame\n",
    "        Predictions for the model.\n",
    "    y_true : array-like, shape = [n_samples]\n",
    "        The true predictions.\n",
    "    q_values : array-like\n",
    "        The probability values at which we want to take observations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    q_pred : array-like of float\n",
    "        Returns self.\n",
    "    \"\"\"\n",
    "    p_obs = []\n",
    "    n_tot = len(y)\n",
    "    for q_val in q_values:\n",
    "        q = model.predict_quantile(df_data, q=q_val).y_pred.values\n",
    "        n_q = len([yi for yi, qi in zip(y, q) if yi < qi])\n",
    "        p_obs += [n_q / float(n_tot)]\n",
    "        # print q_val, n_q, p_obs[-1]\n",
    "\n",
    "    return p_obs\n",
    "\n",
    "\n",
    "def calibration_plot(*args, **kwargs):\n",
    "    \"\"\"Generate calibration plot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p_exp : array-like of float\n",
    "        The expected probabilities.\n",
    "    p_obs : array-like of float\n",
    "        The true (observed) probabilities.\n",
    "    \"\"\"\n",
    "    # plt.figure(figsize=(4,4))\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[2, 1])\n",
    "\n",
    "    # plot true vs. predicted\n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    for i in range(int(len(args) / 2)):\n",
    "        p_exp, p_obs = args[2 * i], args[2 * i + 1]\n",
    "        ax0.plot(p_exp, p_obs)\n",
    "        ax0.scatter(p_exp, p_obs)\n",
    "\n",
    "    # plot straight line (ideal case)\n",
    "    ax0.plot([0, 1], [0, 1], color='gray', alpha=0.5)\n",
    "    ax0.set_xlim([-0.02, 1.02])\n",
    "\n",
    "    # plot bucket sizes\n",
    "    ax1 = plt.subplot(gs[1])\n",
    "    n_obs = kwargs['n_obs']\n",
    "    ax1.bar(p_exp, n_obs, width=0.02)\n",
    "    ax1.set_xlim([-0.02, 1.02])\n",
    "\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl83PV54PHPM6P7PkYzsnVYhyWMsbExNraTLoSSUAeyptskFLI0kNCYpSHbTRu62Q27UJpu05J2m2zZxA6hIclyJZtDKSQ0IRA2gEEyxie+L0m2ZnTLkqxrfs/+MSMxFrI9OkYjjZ736+WX5/jOb57fT9Kjr77fZ75fUVWMMcYkLle8AzDGGBNbluiNMSbBWaI3xpgEZ4neGGMSnCV6Y4xJcJbojTEmwVmiN8aYBGeJ3hhjEpwlemOMSXBJ8Q4AwOPxaEVFRbzDMMaYeWXHjh1tqlp0qXZzItFXVFTQ0NAQ7zCMMWZeEZGT0bSzoRtjjElwluiNMSbBWaI3xpgEZ4neGGMS3KQSvYg8LiIBEdl7gedFRL4uIkdEZLeIrJmZMI0xZmGpr9tKy0NLcR7MpeWhpdTXbZ3ysSbbo/8OsOkiz38YqAn/2wJ8Y2phGWPMwlVft5UVOx6gmFZcAsW0smLHA1NO9pNK9Kr6CtBxkSa3AN/VkO1AnogsmlJkxhizQJW99QjpMkSr5vJ8cCPDKqTLEGVvPTKl4830GH0J0Bhxvyn82HuIyBYRaRCRhtbW1hkOwxhj5qehoSGG1c3PnOt4Q9aQ5oZdWgOAV9umdMy4fWBKVbcB2wDWrl1rG9caYxasYDBIW1sbfr+fljOn8XMNy1wnUWeEdPpY6ToBQEA8FE/h+DOd6JuBsoj7peHHjDHGRHAch87OTvx+P21tbTiOQ0fzIcoPf5f1HGNfcAlrXIdIkxEAzmkKjVffPycSfR1wn4g8DawHulX1zAy/hzHGzEuqSk9PD36/n9bWVoaHh0lKSiI3O5POl/+Jf9f9IxpdJZy56Xsknz5C11uP4NU2AuKh8er7Wbf5nim976QSvYg8BXwA8IhIE/AgkBw+gW8CzwM3AUeAfuBTU4rKGGMSSG9vL4FAgEAgwMDAAC6XC4/Hg8/no3nv/8P70y+wUtt4Y9G/Z/Un/460jCzgBggn9uLwv6maVKJX1dsv8bwCn51GPMYYkxAGBgYIBAL4/X76+voQEfLz86msrMTj8XCur4d9T/wp69t/SqMs5tDNP2TDug/GJJY5sXqlMcYkguHhYVpbW/H7/XR3dwOQk5NDTU0NRUVFpKSkALDnlZ9S9Os/Z522sb34dlbf+VXKMrJiFpclemOMmYZgMEh7ezt+v5+Ojg5UlYyMDCorK/F6vaSnp4+17e3pZN8T/4n17T8J9eI//Cwb1t8Y8xgt0RtjzCSNVswEAgHa2toIBoOkpqZSWlqKz+cjMzOThp9tI/WtR0jVVgJSxM5Ft7L6zLPhXvxtrPrkVynLzJ6VeC3RG2NMFEYrZkYnVUcrZrxeLz6fj9zcXEQEeHcJg3QZoo8UTgYL+fCZRzmFj4M3PcOG9b83q7FbojfGmAnU122l7K1HyNReDslSjpb+Ad6la3C5XBQWFuLz+SgoKMDlCi0wMDw0SNuZE3S1nGCg4Xvs0mpAKXcFWOc6yPbgMha5urh8lpM8WKI3xpj3eO3H36Do7f9NkyyiVzJxFAobf077UCelZWWcPdnE2Z7TpPS3kDXoJ3+klULtYpEoi2BscZk+TaVZPRwkkw3uAzgqcTkfS/TGGEOoYuboof0c3f82bXu2c0xWkafdVHCapZwiTYYg8CYEQu37NI02dxHdKV6OZy3lSHYJ7rwS0grLCf76b1gqjWTLALXy7uIAU13CYLos0RtjEtLo0Is3PBnauObdT5Z2tp7h9OGd9JzaTV/LMUZ622FkiDSGKeIcV7jayNBeOjSLs2SwU6txEySVYXI/8RgFiyrJySsk80Lv3dNG0o4HzntsOksYTJclemNMwhmdDBUcDmg5Z50MnPp/ZvfOxynR0wiCiyJGKCBIKgOuLJKz8pGiMtKWrKD/pYdZQhdeV9d5x22hiOLL117y/ddtvod6CP+imf4SBtNlid4Yk3BydvwTbzvVLHOdYrnrFAq0aCEHglXsz9zMSLqX1Dwf5dXLuGLZSvILCsYqZiDUIx/Y8QDpDI09Ntke+brN98zYEgbTZYneGJMwju19g/Zf/U9WcZoaV5A3dRlnNRuRJNyiuHBY+ZE/wev1UlhYOFYxM95c65FPlyV6Y8y85gSD7HnlR7i2P8rKwZ3kaxa/dNaT6nJIckEySj6deGkjKC5Kr7giquPOpR75dFmiN8bMSwP9vex+fhu+/Y9zhXOag1Tyw6I/JbtqPV2n3qHq9L9QSgte2khlmHOawt6rv0xpvAOPg0knehHZBHwNcAOPqepXxj1fDjwB5IXbfFFVn5+BWI0xhraWRg4/94/UNv6AStzscl3Ba4vvwlu1mrLsbHw+H//muuvZ96tskt96hGQdoUWK5vXQy3RJaGXhKBuLuIFDwIcI7QdbD9yuqvsj2mwDdqrqN0RkOfC8qlZc7Lhr167VhoaGKYRvjElU48sj91R+ipzOfVR0bqdT8tidvBrKN1JStRyfz4fX6yU7O/u8SdVEJyI7VPWSZUCT7dFfAxxR1WPhN3kauAXYH9FGgZzw7Vzg9CTfwxizwI2WR6YxxB6ngnOSTuXxZ2lWH7/M2EzeZe/nymUr8Xq95OfnL6jkPhWTTfQlQGPE/SZCWwZGegj4VxH5HJAJxGYlfWNMwirZ8Qi7nCqCkkqSW2gjjyNOFj5p5/c+/SUKCgpwu93xDnPeiMVk7O3Ad1T170VkI/A9EVmhqk5kIxHZAmwBKC8vj0EYxpj5ZnBggN8+932GuYw0d5AeTWfEgbWyj0WudtwaxFVUFO8w553JJvpmoCzifmn4sUh3A5sAVPV1EUkDPIytEBGiqtuAbRAao59kHMaYBDEyMkJrIED9Sz8lePQ35DtdOAhDTpAb5FUyXYNjbVukaF6XOcbLZBN9PVAjIpWEEvxtwCfGtTkF3AB8R0QuB9KA1ukGaoxJHI7jhHZlamlhX/3LJDe+xmI9g0uUoZUfJ83tYtXOB0mXqX8y1bxrspuDj4jIfcALhEonH1fVfSLyMNCgqnXAnwPfEpHPE5qYvUsnU9pjjElIqkpXV9fYxh1Nh3aS2fxb1jgHccRF96q7WfORLSQlh/ZVrXe5E+aTqfE2qfLKWLHySmMSk6rS29s7ltwHBgZoPbGPosafs8rZS5+k07Lqc1x182fGEryJXqzKK40x5pLOnTvHb370GEmHf4GbIP2aRkfuMi7vfZ1rnd2cES9HV9/PVTdvocwSfMxZojfGzIihoaGxnvs79b/Ge+YlCrWTbieFamni5p6XaFIvO1Y/zJqP3EOpJfhZY4neGDNlIyMjtLW1EQgE8Lecoaf9DNrXRnbTS+RLgEXSylXuTpq1kDeDtSx2dXHNv/tcvMNecCzRG2MmdKEdmnp7Ojm46w2aj+7jbFsL7oEO8oMBavQ4PtrI5BxDriSa1EOz4+EURVzlOkKJuz1ue6YudJbojTHvUV+3laqGv6SDXBqpZdhxQcMTvLbjSRxJZoQkChlmqXbhuJIZyFyEv/Bmuhcvp7ByFa7/83GqXC1Ay3nHjdeeqQudJXpjEtTF9kyN1N/bTfOR3XSd2M2Ifz8ZXYdZ3H+EAlcfqSgukjijXlrJZ0CTSfdWUlhSRcmyqymtvmLCapn6q79A4Y4HrA5+jrBEb0wCGl0ULF2GQKCYVlIa/pIX2looKPAw4j9AetchvAPHKXYC1EiozHpI3RxzVbLPWcpvZS2OuMlggCWcpkZOUEAnyZ/tuOT7J9oOTfOd1dEbk4AOPbiCfieZc6SSLoMUSTeL6MAVkdCb3aV0ZFYxlF+L27cMd34lmpxFb18fvS9/nZLwph1e2klmBAhvjv3QkXiemolgdfTGLBAjw0OcfKeBtgO/xdVcz6KePdTKGXDDsLppUg8tTj6n8JLCMJ47vs3iysspdyeR0daG3++ns7MTHVIyk6Gqqoqmjg+xbNd/t6GXBGGJ3pg56kJj7J2tZzi5+zecO/Y6Oa07qRw8QLUMUg20kUdjxhUcPVtEkXRSKS1UuvxU4gfgNF5SPWUcPnKUtrY2HMchNTWVsrIyvF4vWVlZAJSX30t9ksuGXhKEDd0YMweNjrGnMMQJLaZNcxGg2N1FeThpj6iLE0mVtOevxr1kPYtXXMeiJbWIy3XeGL0CPWTTqMXsLb2d4pqrSUpKwuv14vV6yc3NtY075ikbujFmHivc8Y+87VSxzNVItauFalro0CyOO4torv44OTXvp/LK97M0K5elE7x+3eZ7+O1wkLQ9TzKsybRLPmdL3s+q938Yn89Hfn4+Lpdr1s/LxIclemPmkLPdHez98d+xnC6q3KfZ5VRxVN0sop3F0kGeHMF154X/+h0YGAh9StXvZ6RwOX3X/zX5+flU+XwUFhaSlGQ/8guRfdWNmQPOdnew78dfZdmJJ9hIL2/pUrK1n1WuY+e18/PeDxwNDw/T2tqK3++nu7sbgJycHGpqaigqKiIlxdaUWegmnehFZBPwNULr0T+mql+ZoM2thPaOVWCXqo7fnMQYA/T2dLL3R4+w7MQTbKCXt9M3kHnjlwg2vkPpjgfOaxtZ9RIMBkMbd/j9dHR0oKpkZGRQWVmJ1+slPT09Pidk5qRJJXoRcQOPAh8itDF4vYjUqer+iDY1wH8B3q+qnSLincmAjUkEvT2d7PnxI1x+PJTgd6WvJ/ChL7F6zXWhBldd+54PHJ1a8wWq3/8x3nnnHdra2ggGg6SmplJaWorP5yMzM9MmVc2EJtujvwY4oqrHAETkaeAWYH9Em88Aj6pqJ4CqBt5zFGMWqMgEvzEiwa8aTfAR1m2+B/23W+jp6eFsIMBwIMCePXvGKmZ8Pp9VzJioTDbRlwCNEfebgPXj2tQCiMirhIZ3HlLVX0w5QmPmqcg6+BMs5lDWNazve5GNnGVX+jXhBP+BCV/b19c3Nqk6MDCAy+WisLAQn89HQUGBVcyYSYnFZGwSUAN8ACgFXhGRlaraFdlIRLYAWwDKy8tjEIYx8TNaxx5EeMNZxjJXI5v6fkKDayU5H/nyhAl+cHAQv99PIBCgt7cXgPz8fCoqKvB4PFYxY6Zsst85zUBZxP3S8GORmoA3VHUYOC4ihwgl/vrIRqq6DdgGoQ9MTTIOY+a0RTu+yi6nistcTWx0H2CXU0VABymlheKIJD9aMRMIBOjqCvWFsrOzWbp0KV6v1ypmzIyYbKKvB2pEpJJQgr8NGF9R8xPgduCfRcRDaCjnGMYsAOo4vP2rJ/Gowwb3AfY6Swho7liZpKMyVjETCARob29HVUlPT6eiogKfz2cVM2bGTSrRq+qIiNwHvEBo/P1xVd0nIg8DDapaF37uRhHZDwSB+1W1faYDN2auOfTWbxj5xZe4amgPJ/CxK1jJla7jiITqjDvJ5aBUM/LaawSDQVJSUigpKcHn85GVlWWTqiZmbK0bY6bpzMmDNP/f/8ranl/RQQ6Hl/9HNCWTK3c+yIik4MdDK4X0kU774t9l9bU34/P5yMvLs+RupsXWujEmxnq62tn37EOsaX6KfOD10rtYceuDrExOw+/381zbAOlNvyVbe0mSIEnLb+LWj91rFTNm1lmiN2aShocGeevH/0jtO//ERnqoz7sR3+a/pDQlm0NHjnP27FkAatfdgPfm2ykqKrKKGRNX9t1nTJTUcdj162fIf+3LrHea2J28ikPv/yKpeYs51hgAAmRnZ1NdXY3X6yU1NTXeIRsDWKI3ZkLjN/3YXf5HlLS8yJVDu9kry3jusr8ja/EyVBUGB6moqMDr9ZKRkRHv0I15D0v0xowTuWlHC7mcCuZz9cnHOU4pT/u+gO+yDeSnZ4xt3JGdnW2TqmZOs0RvzDhlbz1CMkO8HFyF19XHgCuX57mMHM6x5nc/htfrJT8/35K7mTcs0RsTob+/nxOOjzflCnLcg+zWxZTgZ5O8Sp52k7xsWbxDNGbSLNGbBW9oaIhAIMCZM2c4+PpzFEsuRXTR7yTxB/JLUmQEgBYpes+mH8bMB5bozYI0MjJCW1sbfr+fzs5OuttbcO//EdcFd3PKvZiykUZKXe9+oDty0w9j5htL9GbBcByHjo4O/H4/7e3tOI5DakoKfYd/w79p+hZucTi85r/xvo/cQ8O/fIukiE0/Gq++n3Wb74n3KRgzJbYEgkloqkpXVxeBQIDW1lZGRkZITk7G6/US7G/H+dnnWT6yn7czNlJ6x1Y8i5fEO2RjomZLIJgFS1Xp7e0lEAgQCAQYHBzE7Xbj8Xjw+XzkZGfT8OzfsPrQ1xmSFOqv+hvW/tv/gNjSBCZBWaI3CePcuXNjuzL19/cjIhQUFFBdXU1hYSFut5umI3vxf2MLG4b38XbGBkr+aCvrFlfEO3RjYsoSvZnXRitmAoEAPT09AOTm5lJbW0tRURHJyckAOMEg25/8MqsOfo0cSaZ+9f9g7eZ7rRdvFoRJJ3oR2QR8jdB69I+p6lcu0O6jwA+BdapqA/BmxoxWzAQCATo7O1FVMjMzqaqqwuv1kpaWRn3dVlzhJQz2ajXidrNBD7ErYz2L7tjKupLKeJ+GMbNmUoleRNzAo8CHCG0ZWC8idaq6f1y7bOBPgTdmKlCzsI1WzAQCAdra2nAch7S0NMrKyvD5fGRmZo61HV3CIJlh3nSWscp1jGHHzfMFd/Dhz/0v68WbBWeyPfprgCOqegxARJ4GbgH2j2v3V8DfAvdPO0KzYKkq3d3dY0MzoxUzxcXFoUnVnJwJlyFIa/gmb2s1S12n2RDer3WRtLOm8wVL8mZBmmyiLwEaI+43AesjG4jIGqBMVZ8TEUv0ZlJUlb6+Pvx+/1jFjMvlGquYyc/Pn3DjjoFzfez91fdJ2/skK13HCKqwVytpcjysdh1FJLRfqzEL0YxOxoqIC/gH4K4o2m4BtgCUl5fPZBhmHhoYGBhL7n19fWMVM1VVVXg8Htxu94SvO7b3DVp/s41lrT9nLX2cFi+/CV7JMtepsQ25RwXEY59sNQvSZBN9M1AWcb80/NiobGAF8HL4T+pioE5ENo+fkFXVbcA2CH1gapJxmAQwNDREa2srfr//vIqZmpoavF7vWMXMeGe7O9j/r4+Tf/AZakcOUapJ7Mm5ltRr7mL5+z5C83OPkbPjgfNeY0sYmIVssom+HqgRkUpCCf424BOjT6pqN+AZvS8iLwNfsKobMyoYDJ63xsxoxUxlZSU+n4+0tDTgvRt/nFr9BXIWVXH29ce5ovPXrJdBjruWsL32fpbd+Mdc7Xk3ha/bfA/1EH69LWFgzKQSvaqOiMh9wAuEyisfV9V9IvIw0KCqdbEI0sxvjuPQ2dmJ3+8fq5hJTU2lrKwMr9dLVlbWee0jN/5oJ4sTwUIWv/VVyl2t9GkaewtvJO93/pia1ddSeYHJ1XWb74FwYi8O/zNmobK1bkxMqCo9PT34/X5aW1sZHh4mKSlpbFem3NzcC27ccfTBK+jQDFIY4XI5SYoEOeCU4dcC1v5FHZnZebN8NsbMTbbWjYmLyIqZgYGBsYoZr9dLQUHBhBUzAD1d7Rx8+WmSD9axnDNUu4K0aD5vObUskjaWuRqp1SZcluSNmTRL9GbaBgYGxtaYGa2Yyc/Pp7KyksLCQpKSJv426+lq5+BvniH5wE9Z3t/AOhmhBQ9vOMvxSge10kyxu3OsvVXNGDM1lujNlAwPD49VzHR3dwOQk5NDTU0NRUVF7PrFP6M/egSXttIiRTSuCU2GjiX3g3Us76sfS+5vFX+cvHUf57I119P4L9+ifMcDRI7sWNWMMVNnY/QmasFgkPb2dvx+Px0dHagqGRkZ+Hw+vF4v6enpwPmTqQBnNY19TgXu5FRWBfeRIiP4KeS470PkrbuV2qs+gGtcnfy7VTfhqpk1VjVjzHjRjtFbojcXparnVcwEg0FSU1Pxer1ja8yMn1RteWgpxbRywCnlHKksl5Okygh+zeN48Sby1n6c2jXXvye5G2MmxyZjzZSNVsyMrjETWTHj8/kuWjEDUOS0sl2XcbXrMN1kstNZSp70spRmfPduncUzMcaAJXoToa+vb2xSdbRiprCwEJ/Pd9GKmfOOcbaLfXoZG9wH2OlUUyWhhcUAWiiyMXZj4sAS/QI3ODg4ltx7e3sByM/Pp6KiAo/Hc8GKmYk0Ht5F8Kk7uNrVyKvB5Wx07ccV7vjbZKox8WOJfgEarZgJBAJ0dXUBkJ2dzdKlS/F6vaSkpEz6mG+98D1qX7ufYUlm/w1PkNLVQsCWIDBmTrBEv0CMVswEAgHa29tRVdLT06moqMDn841VzEzWyPAQ9Y9/no1nvs+h5Fpy7nyKlWVLQ0/aEgTGzAmW6BPYaMVMIBCgtbWVYDBISkoKJSUl+Hw+srKyLjqpeint/iZavv0JNg7t4o3C32f1Z75BalrGDJ6BMWYmWKJPMKrK2bNnx9aYGRoawu12U1RUhM/nIy8vb1rJfdSB+l9R8NxnqNaz1F/116z//ftmIHpjTCxYok8Q/f39Y2vMnDt3DhEZq5gpLCyMqmImGuo4vPns33LVO4/Q6vLQ/Ad1rLvyfTNybGNMbFiin8dGK2YCgQBnz54FQhUz5eXlFBUVTapiJhr9vd3s3/Zp1vf8il0Z66n4zP+hpKBoRt/DGDPzJp0JRGQT8DVC69E/pqpfGff8nwF/DIwArcCnVfXkDMRqgJGRkbE1ZiIrZqqrq/F6vaSmpsbkfRuP7GHkyTtYEzzJ6xX/gfWf/B/2yVZj5olJJXoRcQOPAh8itDF4vYjUqer+iGY7gbWq2i8i9wJ/B/zhTAW8EDmOc94aM47jkJ6ezpIlS/D5fGRkzPwEaOQOT685K1jlOkpQ3Oy9/tts/MBHZ/z9jDGxM9ke/TXAEVU9BiAiTwO3AGOJXlVfimi/HbhjukEuRKpKV1fX2BozIyMjpKSksHjxYrxeL9nZ2TMyqTqR0UXJUhjiDedyfse9l0NOCU01d/G7luSNmXcmm+hLgMaI+03A+ou0vxv4+URPiMgWYAtAeXn5JMNITKpKb2/v2KRqZMWM1+slPz8/Zsk9Ut6Or7PLqaJYOtnofoc3ncu4Uo6Sc2Qb8Bcxf39jzMyK2WSsiNwBrAWum+h5Vd0GbIPQ6pWximM+6O/vH1uGILJixuv1UlhYiHsWxsKdYJB9r/4LQ/X/zEpaqHGPcMgpocGp4RrXQQC82hbzOIwxM2+yib4ZKIu4Xxp+7Dwi8kHgS8B1qjo49fAS19DQ0FhyH62YycvLo7y8HI/HQ3Jy8qzE4W86yrFfbmPJqR+xUgN0k8mbzjIWSyu1rvO/tLbDkzHz02QTfT1QIyKVhBL8bcAnIhuIyFXAVmCTqgZmJMoEMTIyQltbG36/n87O0BZ5WVlZMa+YGW94aJA9Lz2L++3vsqK/Hp8oe1NXc/rKv2DF736C1F9+n0U7HjjvNbYomTHz16QSvaqOiMh9wAuEyisfV9V9IvIw0KCqdcAjQBbwg/B48ilV3TzDcc8bjuPQ0dGB3++nvb0dx3FIS0tjyZIleL1eMjMzY/K+kVUzgfBWfsWXb6Dp19uoPfMz1tBNgALeLL2L8hvuYUXV5WOvXbf5Hurh/B2ebFEyY+Yt22EqBlSV7u7usWUIRkZGSE5OHtu4I5YVM3D+Vn7nNJm9WkkmAyx3nWJEXezJ3Ihc/UlWXPsHJCVPfqVKY8zcYDtMzbLRipnRT6oODg7idrvxeDz4fL5Zq5iBUNXMfi0nqC4ul0bWuQ7R5Hh4ObiKFZ/9PlcVW5WTMQuJJfppOnfu3Nikan9/PyJCQUEB1dXVs1YxMzI8xJG3X6Fz9y8oOPMKS2nG7VJ6NIMDThmZMsDlcorF0o7LkrwxC44l+ikYGhoaW4agp6cHgNzcXGpraykqKpqVihl/01FOvvEzko6/yNLeHSyjj6AKR5Iv47fOSnzSwVJpZp370LuvwapmjFmILNFHabRiJhAI0NnZiaqSmZlJVVUVXq+XtLS0GX2/8ZOpx1b+J7I9i+nb/wLFra9S4TTiAwIUcCDvOty1H2Tp+o9wWaGPnrqtLNnxAEny7vyLVc0Ys3BZor+I0YqZQCBAW1vbWMVMWVkZPp8vphUzK3Y8QJvm8KYuI0MGWbP7IdJkmEFN5lDaSraXfQzfmpupWHY13nFLEFvVjDEmklXdjDNaMTM6qTpaMTO6cUdOTk5MJ1UbD+/i2Hfvo0wCVLlaADjpeDmjBQRJYs0Xf0F6ZnbM3t8YM39Y1c0kqCp9fX1ja8wMDg7icrnOq5iZqY07JtJ4eBdNrz6F79TPqXJOUOaGd5wytgeXscTlZ4krwBICOCq4LMkbYyZpQSf6gYGBseTe19c3VjFTVVWFx+OJacXMaHL3Nv6C6uBxyoB3kpezfen95B78AZe7TrznNbYEgTFmKhZcoh8eHh4blunu7gZCFTM1NTUUFRWRkjIzHyCa8JOpy99H82+foqjx+bHkfiB5Odur76fy2tu5vLQ6/NoizoU/8DTKJlONMVO1IMbog8HgeWvMjFbMjH5SNRYVM6OfTG10PDRrEUXSRbXrDBBK7l2VN1F57SfwhZP7RMc4bzJ1jU2mGmPOF+0YfcImesdx6OzsHNu4w3EcUlNTx5J7ZmZmTCZVhwYHeOvL16M4+KRrbEL1gFOKXwu5bMvjFJctnfH3NcYsPAtyMlZV6enpGVtjZnh4mKSkJIqLi/F6veTm5sYkuTcd2Utzw89IPfkytf072eAeZEjdHNZStgeXUeE6wzJXE7XajMuSvDFmliVEoo+smBkYGBirmPF6vRQUFEypYmaiMfbRoZPenk4Ob3+OoYO/pLTjdUrVTynQJMXsKbqZYf8hVrsOc4Xr/D3RbTLVGBMPk070IrIJ+BqhZYofU9WvjHs+FfgucDXQDvyhqp6YfqjnGxgYGFtjZrRiJj8/n4qKCjweD0lJU/8dFjnGjoBXW+mo/1+8cPBVSoePUTu4n6skSL+mcihzDU1LPk3J1R+hdOkKSsNYYr90AAAOfElEQVSvd9t67saYOWJS2VBE3MCjwIcI7RdbLyJ1qro/otndQKeqLhWR24C/Bf5wsoFN1KNe/eFPj60xM1oxk5OTM2HFzMV65Bcz0N9LSsNWjuoizpGKAJWuMyx3N7K8r5Gj7ioaFv97clZsombtDaxOfe9Ern0y1Rgzl0xqMlZENgIPqervhe//FwBV/ZuINi+E27wuIklAC1CkF3mj8ZOxkT3qIC7ayadRizlY8lEW115NRkYGPp8Pr9dLenr6e453Xo887JymsPPKBylf9QG6/Mc513qKYFczcraZ1P4Wsof8FATbyOfsecfq1CyOOotQESpooejhxvFvZ4wxcRGrydgSIDLTNQHrL9QmvCNVN1AIRL2zdNlbj9CsHvZrDb1k4YiLJB0hq+ll5Nw+HAnSIqHfIBMZCRxmN1WIKCkMk805CuQs79vzJdgT2uh2VCfZdLg9nE3x0ZaxCs0uYejY/6NAu8mVPkqkjbXuwwC0UBTtKRhjzJwRt8lYEdkCbAEoLz9/jXSvttJCNWkSZDEnKKSDbOlFADqjmFh1OQAoQrdmcpZ02pxcHISkq/+I9KJycn0VeBZVkJ+ZTf64l9fXbaXaPrBkjEkQk030zUBZxP3S8GMTtWkKD93kEpqUPY+qbgO2QWjoJvK5gBSxWo7icAwX7z7VQhHFDx25ZJAtDy2lmFYAPNJz/ut//7OXfL2NsRtjEslkE309UCMilYQS+m3AJ8a1qQPuBF4HPgb8+mLj8xNpXHM/udPoUU/39RBK9oQTe3H4nzHGzEeTSvThMff7gBcIlVc+rqr7RORhoEFV64BvA98TkSNAB6FfBpMy3R619ciNMeZdCbsEgjHGJLpoq25it8i6McaYOcESvTHGJDhL9MYYk+As0RtjTIKbE5OxItIKnLzA0x4m8anaBGTnv7DPH+wa2Plf+PyXqOolP7I/JxL9xYhIQzSzyonKzn9hnz/YNbDzn/7529CNMcYkOEv0xhiT4OZDot8W7wDizM7fLPRrYOc/TXN+jN4YY8z0zIcevTHGmGmYE4leRDaJyEEROSIiX5zg+VQReSb8/BsiUjH7UcZWFNfgz0Rkv4jsFpEXRWRJPOKMlUudf0S7j4qIikhCVWFEc/4icmv4e2CfiDw52zHGWhQ/A+Ui8pKI7Az/HNwUjzhjRUQeF5GAiOy9wPMiIl8PX5/dIrIm6oOralz/EVoF8yhQBaQAu4Dl49r8CfDN8O3bgGfiHXccrsH1QEb49r2JdA2iOf9wu2zgFWA7sDbecc/y178G2Ankh+974x13HK7BNuDe8O3lwIl4xz3D1+BaYA2w9wLP3wT8HBBgA/BGtMeeCz36a4AjqnpMVYeAp4FbxrW5BXgifPuHwA0iIrMYY6xd8hqo6kuq2h++u53zd0Sc76L5HgD4K0KbzQ/MZnCzIJrz/wzwqKp2AqhqYJZjjLVoroECOeHbucDpWYwv5lT1FUJLu1/ILcB3NWQ7kCcii6I59lxI9BPtQ1tyoTaqOgKM7kObKKK5BpHuJvSbPVFc8vzDf6aWqepzsxnYLInm618L1IrIqyKyXUQ2zVp0syOaa/AQcIeINAHPA5+bndDmjMnmiTFx2zPWTI2I3AGsBa6LdyyzRURcwD8Ad8U5lHhKIjR88wFCf829IiIrVbUrrlHNrtuB76jq34vIRkIbHK1QVSfegc11c6FHP5l9aLnYPrTzWDTXABH5IPAlYLOqDs5SbLPhUuefDawAXhaRE4TGJ+sSaEI2mq9/E1CnqsOqehw4RCjxJ4porsHdwLMAqvo6kEZoHZiFIqo8MZG5kOjH9qEVkRRCk61149qM7kMLU9yHdo675DUQkauArYSSfKKNz170/FW1W1U9qlqhqhWE5ig2q2qibEsWzc/ATwj15hERD6GhnGOzGWSMRXMNTgE3AIjI5YQSfeusRhlfdcAnw9U3G4BuVT0TzQvjPnSjs7QP7VwW5TV4BMgCfhCehz6lqpvjFvQMivL8E1aU5/8CcKOI7AeCwP2qmjB/1UZ5Df4c+JaIfJ7QxOxdidThE5GnCP0y94TnIR4EkgFU9ZuE5iVuAo4A/cCnoj52Al0nY4wxE5gLQzfGGGNiyBK9McYkOEv0xhiT4CzRG2NMgot71Q2Ax+PRioqKeIdhjDHzyo4dO9o0ij1j50Sir6iooKEhUUqijTFmdojIyWja2dCNMcYkuDnRozfGGPOuii9eeO2+E1+5edLHsx69McYkOEv0xhiT4CzRG2NMgrNEb4wxCc4SvTHGJDirujHGmBk201Uz02WJ3hhjxrlQoo5Hkp4JluiNMQkn0RL1dNkYvTHGJLhLJnoReVxEAiKyN+KxAhH5pYgcDv+fH35cROTrInJERHaLyJpYBm+MMebSounRfwfYNO6xLwIvqmoN8GL4PsCHCe1MXwNsAb4xM2EaY4yZqkuO0avqKyJSMe7hWwjvSA88AbwM/Ofw498Nb9i7XUTyRGRRtDuVG2MM2Bj7TJvqGL0vInm3AL7w7RKgMaJdU/ix9xCRLSLSICINra2tUwzDGGPMpUx7Mjbce9cpvG6bqq5V1bVFRZdcN98YY8wUTTXR+0VkEUD4/0D48WagLKJdafgxY4wxcTLVRF8H3Bm+fSfw04jHPxmuvtkAdNv4vDHGxNclJ2NF5ClCE68eEWkCHgS+AjwrIncDJ4Fbw82fB24CjgD9wKdiELMxZo6zydS5JZqqm9sv8NQNE7RV4LPTDcoYE1+WqBOLfTLWGGMSnK11Y0wCsh65iWQ9emOMSXDWozdmjplra5mb+c969MYYk+As0RtjTIKzRG+MMQnOxuiNmWE2xm7mGuvRG2NMgrNEb4wxCc4SvTHGJDhL9MYYk+BsMtaYcWwy1SQa69EbY0yCs0RvjDEJblpDNyJyAjgLBIERVV0rIgXAM0AFcAK4VVU7pxemMcaYqZqJHv31qrpaVdeG738ReFFVa4AXw/eNMcbESSyGbm4BngjffgL4/Ri8hzHGmChNt+pGgX8VEQW2quo2wBexIXgL4JvmexgzKVY1Y8z5ppvof0dVm0XEC/xSRA5EPqmqGv4l8B4isgXYAlBeXj7NMIwxxlzItIZuVLU5/H8A+DFwDeAXkUUA4f8DF3jtNlVdq6pri4qKphOGMcaYi5hyoheRTBHJHr0N3AjsBeqAO8PN7gR+Ot0gjTHGTN10hm58wI9FZPQ4T6rqL0SkHnhWRO4GTgK3Tj9Ms5DYGLsxM2vKiV5VjwGrJni8HbhhOkEZY4yZOfbJWGOMSXC2qJmZcRcaerFhF2Piw3r0xhiT4CzRG2NMgrNEb4wxCc4SvTHGJDhL9MYYk+Cs6sa8h1XNGJNYrEdvjDEJzhK9McYkOEv0xhiT4GyMPgHZGLsxJpL16I0xJsFZj34Osh65MWYmWY/eGGMSnCV6Y4xJcDEZuhGRTcDXADfwmKp+JRbvM1fZ0IsxZi6Z8R69iLiBR4EPA8uB20Vk+Uy/jzHGmOjEYujmGuCIqh5T1SHgaeCWGLyPMcaYKMRi6KYEaIy43wSsj8H7xIRtTG2MSTSiqjN7QJGPAZtU9Y/D9/8IWK+q941rtwXYEr57GXDwAof0AG0zGuT8Yue/sM8f7BrY+V/4/JeoatGlDhCLHn0zUBZxvzT82HlUdRuw7VIHE5EGVV07c+HNL3b+C/v8wa6Bnf/0zz8WY/T1QI2IVIpICnAbUBeD9zHGGBOFGe/Rq+qIiNwHvECovPJxVd030+9jjDEmOjGpo1fV54HnZ+hwlxzeSXB2/mahXwM7/2ma8clYY4wxc4stgWCMMQluTiR6EdkkIgdF5IiIfHGC51NF5Jnw82+ISMXsRxlbUVyDPxOR/SKyW0ReFJEl8YgzVi51/hHtPioiKiIJVYURzfmLyK3h74F9IvLkbMcYa1H8DJSLyEsisjP8c3BTPOKMFRF5XEQCIrL3As+LiHw9fH12i8iaqA+uqnH9R2jC9ihQBaQAu4Dl49r8CfDN8O3bgGfiHXccrsH1QEb49r2JdA2iOf9wu2zgFWA7sDbecc/y178G2Ankh+974x13HK7BNuDe8O3lwIl4xz3D1+BaYA2w9wLP3wT8HBBgA/BGtMeeCz36aJZMuAV4Inz7h8ANIiKzGGOsXfIaqOpLqtofvrud0OcTEkW0y2b8FfC3wMBsBjcLojn/zwCPqmongKoGZjnGWIvmGiiQE76dC5yexfhiTlVfATou0uQW4Lsash3IE5FF0Rx7LiT6iZZMKLlQG1UdAbqBwlmJbnZEcw0i3U3oN3uiuOT5h/9MLVPVC69RMX9F8/WvBWpF5FUR2R5eITaRRHMNHgLuEJEmQlV9n5ud0OaMyeaJMbbD1DwjIncAa4Hr4h3LbBERF/APwF1xDiWekggN33yA0F9zr4jISlXtimtUs+t24Duq+vcishH4noisUFUn3oHNdXOhRx/NkgljbUQkidCfbe2zEt3siGrZCBH5IPAlYLOqDs5SbLPhUuefDawAXhaRE4TGJ+sSaEI2mq9/E1CnqsOqehw4RCjxJ4porsHdwLMAqvo6kEZoHZiFIqo8MZG5kOijWTKhDrgzfPtjwK81PDuRIC55DUTkKmAroSSfaOOzFz1/Ve1WVY+qVqhqBaE5is2q2hCfcGdcND8DPyHUm0dEPISGco7NZpAxFs01OAXcACAilxNK9K2zGmV81QGfDFffbAC6VfVMNC+M+9CNXmDJBBF5GGhQ1Trg24T+TDtCaLLitvhFPPOivAaPAFnAD8Lz0KdUdXPcgp5BUZ5/wory/F8AbhSR/UAQuF9VE+av2iivwZ8D3xKRzxOamL0rkTp8IvIUoV/mnvA8xINAMoCqfpPQvMRNwBGgH/hU1MdOoOtkjDFmAnNh6MYYY0wMWaI3xpgEZ4neGGMSnCV6Y4xJcJbojTEmwVmiN8aYBGeJ3hhjEpwlemOMSXD/H+twO90as4HKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_exp = np.arange(0,1.05,0.05)\n",
    "p_obs = prep_calibration(clf, X_ts, y_ts, p_exp)\n",
    "n_obs = [po * X_ts.shape[0] for po in p_obs]\n",
    "# for pe, po in zip(p_exp, p_obs): print(pe, po)\n",
    "calibration_plot(p_exp, p_obs, p_exp, p_obs, n_obs=n_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae\n",
      "  Train 3.1267120301975693\n",
      "  Test 3.4250765742195806\n",
      "var\n",
      "  Train 0.7445309236034652\n",
      "  Test 0.7232751504667847\n",
      "r2\n",
      "  Train 0.7445309236034652\n",
      "  Test 0.7124413578665383\n",
      "mape\n",
      "  Train 0.1380923796214813\n",
      "  Test 0.15479363611996366\n",
      "rmse\n",
      "  Train 1.246730657732102e-15\n",
      "  Test 0.9952790297515476\n"
     ]
    }
   ],
   "source": [
    "for name, metric in metrics.items():\n",
    "    print(name)\n",
    "    print('  Train', metric(y_tr, y_tr_pred))\n",
    "    print('  Test', metric(y_ts, y_ts_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantile-Calibrated Bayesian DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n",
      "0.05 0.015748031496062992\n",
      "0.1 0.05511811023622047\n",
      "0.15000000000000002 0.12598425196850394\n",
      "0.2 0.13385826771653545\n",
      "0.25 0.1968503937007874\n",
      "0.30000000000000004 0.25196850393700787\n",
      "0.35000000000000003 0.30708661417322836\n",
      "0.4 0.3464566929133858\n",
      "0.45 0.3858267716535433\n",
      "0.5 0.4094488188976378\n",
      "0.55 0.4566929133858268\n",
      "0.6000000000000001 0.5196850393700787\n",
      "0.65 0.5433070866141733\n",
      "0.7000000000000001 0.6456692913385826\n",
      "0.75 0.6850393700787402\n",
      "0.8 0.7086614173228346\n",
      "0.8500000000000001 0.7795275590551181\n",
      "0.9 0.8661417322834646\n",
      "0.9500000000000001 0.8976377952755905\n"
     ]
    }
   ],
   "source": [
    "from src.calibrated1 import CalibratedRegressor\n",
    "calibrated_model = CalibratedRegressor(clf)\n",
    "calibrated_model.fit(X_tr, y_tr)\n",
    "\n",
    "\n",
    "p_exp = np.arange(0,1,0.05)\n",
    "p_obs = prep_calibration(calibrated_model, X_ts, y_ts, p_exp)\n",
    "p_raw = prep_calibration(clf, X_ts, y_ts, p_exp)\n",
    "n_obs = [po * X_ts.shape[0] for po in p_obs]\n",
    "for pe, po in zip(p_exp, p_obs): print(pe, po)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.056563582797395345\n",
      "0.06372056670040879\n",
      "0.06555262782945212\n",
      "0.06297567646804987\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VPW5+PHPM9lDQvZMICQQZJFFUAyCO4oo1VbschV7bWuvXlyqXbT2tret9Vq9ba+3m7/aVuq1Wm+tonXBilrFcnEDAZHIIqtIEshkgSyQdTLP748zYJYZMkkmGTJ53q8XL2bOOXPO95wkT06e7/c8X1FVjDHGRBdXpBtgjDEm/Cy4G2NMFLLgbowxUciCuzHGRCEL7sYYE4UsuBtjTBSy4G6MMVHIgrsxxkQhC+7GGBOFYiN14OzsbB03blykDm+MMUPShg0bqlU1p6ftIhbcx40bx/r16yN1eGOMGZJE5ONQtrO0jDHGRKEeg7uIPCwilSKyOch6EZH7RWSXiJSIyKzwN9MYY0xvhHLn/giw8DjrPwVM9P9bAvyu/80yxhjTHz0Gd1VdDRw8ziaLgD+pYw2QLiKjwtVAY4wZTpqbmyktLcXn8/VrP+HoUM0HSju8L/MvO9B1QxFZgnN3T2FhYRgObYwxQ5/X66WqqgqPx0NtbS0AI0eOJC0trc/7HNTRMqq6FFgKUFxcbLOEGGOGLZ/Px6FDh/B4PFRXV+Pz+UhKSqKoqIjc3FySkpL6tf9wBPdyoKDD+zH+ZcYYYzpQVQ4fPkxFRQWVlZW0tbURFxfHqFGjcLvdpKamIiJhOVY4gvty4BYReQKYA9SpareUjDHGDFfNzc14PB48Hg+NjY24XC6ysrJwu91kZmbicoV/VHqPwV1E/gLMA7JFpAz4ERAHoKq/B1YAlwK7gEbgq2FvpTHGDDGB8uhpaWlMmjSJ3NxcYmMHNive495V9eoe1ivwtbC1yBhjhqiBzqP3RsTKDxhjTDQYzDx6b1hwN8aYPohEHr03LLgbY0yIesyjb30GHrsK6sogbQzMvxNmXBmRtlpwN8aY4wg5j16yDO/ztxLb3uy8ryt13kNEArwFd2OMOapkGay8G60r43DqBCpOuZnKEZNDyqM3rvghyUcDu19sezONL91JsgV3Y4yJkJJlNC//Nh5vCh5m0tiQhGvt02Sd/jncxZeTGdeC6/AB2FcCdeVQX+6kX+rKoL6c5OaqTrtTBRFIbKqIyOlYcDfGDGvH8ugvPUKtdyoAadQzif3kttcQu24NrPsOaHvnD8an0JaSz6HYHMqS5rK5ronNWkSZ5nBAs/hT3E8okGr2+7IYE4HzsuBujIke/rTKcTs0vS34Krdz6KNNVJTupqa6Ct/hapKaj1BEFblUk0TLJ9srcO7t+FLzKfVlsKk+lberEnhjXxvl+500TGKcizRfLYvk/1gYs45Zrp2kyxEaNZ6H4q/hrkG7AJ+w4G6MiQ4ly+CFr0Nbk/O+rhSW3wLlGyApA/Vs5bBnDxWHjlCpmbQRR5y0M2pkIu5CNwm73ySh/fCx3TVoEht9E1gTM4uSjy5h475DHGltB1pxjxSKx2byL2MzKB6bwdTRI3mx5ABvPruZSZQxkkbKfNn8isWcc9mSiFwOC+7GmOiw8u5PAvtR3haa1/4RDzl4kifTmDwL19hssvLG4C6aRmbRTFzxiQDcdc+P+LbvdzzvO5vH2hewXQtQXEibMuVIK5+bNYbicRmcPjaD/PSkbh2qV5yWD9zMVa/MZ39tE6PTk7jjksn+5YPPgrsxZuir3uXcqft5iaGKLDxkU0sanHs7aZnZTHK7g9Z1eeTwGTzPyRxiJKfKLr4a8zJbfYWs0ems+Ma5ITXjitPyIxbMu7LgbowZuvathbfvhw9fxIdwiDQqyKGGTHy4SKKJohHN5J51TtC6Ls1t7fzm9V0AHGIkAO/rBN5vnwBAfvrg1YMJJwvuxpihxeeDHS/BW/ejpWtoSMzHM/12Kr0ptO1YSZyviVFU4qaK1Nh25JL7IUhgf3tXNf/+7AfsrWlk9rgMPiiro9n7yfR2SXEx3HHJ5ME6s7Cy4G6MGRramqHkCXj7NzTX7MOTMg3P1B/RmDUdV1yiU9elIJvMtT/DVX/8x/8PHmnl3he38df3yhiXlcyfr5/D2ROyeW5jOfe9sv2EyJn3lzgVe3vYSGQh8GsgBnhIVX/aZX0h8CiQ7t/mu6q64nj7LC4u1vXr1/e13caYaBRoKOPEBbDuf/Cu+QNVjT48abOozT8fsk8mLSODvLw8cnJyQqqPrqo8u7GcH/9tKw3NXm44fzy3XjiRxLiYQTi58BCRDapa3NN2oUzWEQM8ACzAmfx6nYgsV9WtHTb7AbBMVX8nIlNxJvAY16eWG2OGpwBDGX3P3sQhyaTCl0ZN1oX4Js4hafQUivLycLvdJCYmhrz7vdVH+P5zH/DWrhpmFabzk8/NYHJe6gCdTOSFkpY5A9ilqnsA/NPpLQI6BncFf08EpAH7w9lIY8ww4B/KqEADKXjIplKzaZNk4uZey6iTZvSpPnqr18cf3tjD/St3Eh/j4p4rpvPFMwpxuQa/xvpgCiW45wOlHd6X4cyV2tFdwN9F5FZgBHBRWFpnjBk2musq8ZCPhxwaScKFjywO4vbtIfPiz4ZUH71rzvwLp4/h5c0VbPc0cOkpefzoM9Nwjwz9bn8oC1eH6tXAI6r6cxE5E3hMRKarqq/jRiKyBFgCUFhYGKZDG2OGKq/XS9Wu9/G89WdqmQU4dV0ms58caoilHdIKIMTA/r1nPqCpzakBU17bxK9X7iQ9OY7/+Uox86e4B/RcTjShBPdyoKDD+zH+ZR1dBywEUNV3RCQRyAYqO26kqkuBpeB0qPaxzcaYIexoffSKPVup2fg3fAc+IFlaKBozBveB10lsr/9k47gkp1M1BPe9sv1YYO8oKS5m2AV2CC24rwMmikgRTlBfDHyxyzb7gPnAIyIyBUgEqjDGGJxRKg0NDXg8HipL99C2+w3iDrzLKK3BPXM+qRd+Cxk5KrTCXwH4fEp5bVPAdRV1zQGXR7seg7uqekXkFuAVnGGOD6vqFhG5G1ivqsuB24E/iMi3cDpXr9VQxlgaY6Jap3lG62pwla4hq/xV3L4KMk+9DNf5d0B6hxTtjCt7NWuRqvL6h5X8/O87gm4zeog+YdpfIeXc/WPWV3RZdmeH11uBs8PbNGPMUHS0PnpFRQV1dXXQ1kxa9Xom73mKnLYyYmd8Hs5/DLJO6vMxVJW3dtXw33/fzvultYzNSuaaOYU8vaEsap4w7S97QtUY02/H8ujvPkvNplfwNTeQnBhPUd5o3AdWkthSCVMXwbwnIHdKv4717kcH+fnft7P2o4Pkpyfxs8+fwudmjSEuxkXxuMyoecK0vyy4G2P6pFMevbKStrL3idvxAqPaK5y6Ls2Hkb1A3gxY9DSMmtmv420qreXnr+5g9Y4qclITuHvRNK6aXUBC7CdPl55IVRkjzYK7MeYTIXRoHsujVxygsWY/rqYasnzVuHc+SmZ7BS66dLc1HQo5sAeq7TLJncovXt3Ba9s8ZI6I5/uXTuGauWNJih86JQMiwYK7McYRaCajF74OqnjHzKFq10Yq9u2krroSjlSR1riXyb4O49GDqSsL6fCBxqnfvmwT7aqMTIzl2xdP4tqzi0hJsLAVCrtKxhhHl5mMfAgH2xLxPPvf1JCBDxfJNFGU7MOd6yZx1DlO/jx3CmRPht/O7TRhxjFpoU0PHWicersqqQmxvPFvF5KWFNev0xtuLLgbE01CHSfeXA9VH0LlVqjc5vxfV9q5rgvZzjyjtDFq9uW4i6aSOnYmMiIr8LHn34n3+VuJbf9kXLk3JpHYAA8hNbe1s6vyMDs8DWz3NLCjoiHoOPXDLV4L7H1gwd2YaBFwguhboXafE+iPBfJtne+w40bQnD0NT+wEPN6UTnVd8qgiY2Qqrstu6fHwz7WfzZtt1/NNnmC01LBfs/hF+9VMOnQ6BSUHjgXxHZ4G9tYcwedPzcfFCCflpJAUFxPwCdPhOk69vyy4GxMtXrsrwATRzfD6j53XMfGQPQkK50LuV/FmnUxVTB4VR4S6+npI30LajmVMbt/9SR49Lgkuui+kw//Xyx+yv/Usnuaszite+hAAl8C4rBFMcqfy6ZmjmexOZXJeCmOzRhAX4+qWc4fhPU69vyy4GzMUqTp35KVrYd8aKH0X6ruWfOrga+9C5nh8EsPBgwfxeDzUVNfg8zWQnJxMUVER7rlzSdyR40/r+JyCXT08/t/Y6uWd3TWs2l7F/uM85v+3W89hQm7KcSfFODqE0caph4cFd2NOJMFy5u1tcKDECeal/mDecMD5THwKjCmGhJHQUt9tlzqygIaEUXj27HXGo7e1ERcXx6hRo7rXR+/h8X9VZU/1EVZtr2LV9krWfnSQVq+PpLgYEmNdnZ4OPSo/PYnp+Wkhnb6NUw8fC+7GnCgC5cyfuwlW3we1peD1L08rhHHnQMEc5597Grhiun2+mQQ8sfl4im6g8b33cLlcZGVlkZeXR0ZGRsD66IHGmV8yLY939lSzansV/9heSelBZ/8n5YzgS3PHcsHkXGYXZfDSBxWWVjmBhDSH6kCwOVSN6eIX06A+wJjwmHgovg4K/cF85Oigu/BufIKqlb+h4rCPuoQxMP580iadFdI8o4Fy3i4Blwhen5IUF8NZJ2Uxb3IO8ybnUpCZHHAfllYZWGGbQ9UYM8BU4cO/BQ7s4KRkPvXTwOtw6rocy6M3jMZXfI+TR3e7ezXPaKBx5j6FpHgXf7zmdGaPy+xxImlLq5w4LLgbEymqsOs1eP0eOPA+uGLB5+2+XYCHgLrVdTleHj1EwcaZN7a0c+7EnF7ty0SeBXdjIuGj1U5QL13r1DO/4neAwIvf6jycsctMRJ3qozc2hpRH70ljq5c7n98SdL2NMx+aQgruIrIQ+DXOZB0PqWq3vxFF5EqcibIV2KSqXWdrMsbsW+uMO9/7BqSOhk//Ek69BmLjnfWumG6jZbxTP0fVgQOf1EcH0tPTKSgo6DGP3pMdnga+9uf32FV1mIunulm9o8rqoUeJHjtURSQG2AEsAMpwpt272j9Bx9FtJgLLgAtV9ZCI5KpqZcAd+lmHqhlW9m+E1++FXa/CiBw493Y4/asQFzgf3imPXlODz+cjOTkZdy/z6MGoKsvWl/Kj5VtISYjjV1edyjkTs61DdAgIZ4fqGcAuVd3j3/ETwCJga4dt/hV4QFUPAfQU2I2JWl3HqRdfB+XrnQ7TpAy46C44YwnEj+j20YHIowdyuMXLD579gOfe389ZJ2Xxq8Wnkpvq/LKwDtHoEUpwzwc6lnorA+Z02WYSgIi8hZO6uUtVX+66IxFZAiwBKCws7LramKEt0Dj1lXdBbBLM+3eYexMkjuz2saN59IqKCpqamsKSRw9m6/56bnn8PfbWHOG2BZP42gUTiHH1/xeGOfGEq0M1FpgIzAPGAKtF5BRVre24kaouBZaCk5YJ07GNOTF0KZl7TFIGzPu3Tou8Xi+VlZV4PJ5OefTCwsJ+59EDUVX+vHYfd/9tKxnJcTz+r3OZOz5IdUcTFUL5DioHCjq8H+Nf1lEZsFZV24CPRGQHTrBfF5ZWGnOia20MXMscjpUJCJZHLyoqCksePZj65ja+98wHvFhygPMn5fCLK2eSlZIwIMcyJ45Qgvs6YKKIFOEE9cVA15EwzwFXA38UkWycNM2ecDbUmBOSKmz+K7z6o8CrgYbUCXh27uyWR8/LyyMlJSUsefRgSspqueXxjZTXNvFvC0/mhvPG47I0zLDQY3BXVa+I3AK8gpNPf1hVt4jI3cB6VV3uX3exiGwF2oE7VLVmIBtuTMSVb4CXv+eMVc87BWYuhjUPQFuTU9eFbCpiRtM05rO4DhwYsDx6R0dHu5TXNpGWFMfhFi/u1ASW3TCX08dmDsgxzYkppMSeqq4AVnRZdmeH1wrc5v9nTHSrPwAr/wM2/cUZ1nj5/4NT/xmvT6mMLcSz5inqmryQMJL06RdRWPyZAcmjd9W1NkxdUxsuga9dMMEC+zBkT6gaE6q2JnjnN/DGL8HXBmd/E9/Z3+JgoxfPtg+dPLqOJfmCO3td16Wv6pvb2OlpYHvFYe59cWvA2jC/XbWbf547dkDbYU48FtyN6YkqbHnWyavX7UNP/gwNZ30PT0s8lRu3hDWPHuwhoqZWZ87R7R5nmrrtFQ3s9DQcd4KMo/YHqRljopsFd2OOCjRRRvZEJ6++7x2ac07Ds/BeKsRN056qsI9H75pWKa9t4rZl73PPi1upOdLK0YfJ42NdTMhJYc74LCb5p6qb5E7lyt+/EzDYW22Y4cmCuzEQ+AGkZ2/Eq0Jl4ng8p95LXdpUaHaRnp4Q9vHoqsqP/xY4rXK4xcs35k9ksjuVSXmpjM1MJjam+y+S7yw82SbLMMdYcDcGOj2A5EM4SDoezaEmJg9f8ddIHpk5IHn0tnYfL5Yc4MHVe6g50hpwm5Y2H9+8aFKP+7I5SE1HFtyNAbSujAZS8JBDJVm0EUccbYxqLyVvztlhH49+pMXLE+tKefjNjyivbWJCbgrpSXHUNrV127Y3aRWrDWOOsuBuhrWmpiY8FQfwxM2lqQ1c+MjmIG6qyKAOV9oYSE0N2/EqG5p55K29/O+aj6lv9nLGuEzuXjSNCybnsnzTfkurmLCx4G6GnU51XSr2wrYXSW+ro1BqyNFqYvEH1y4TZfTHrsrDPPTGHp55r5w2n4+F0/JYct54TivMOLaNpVVMOFlwN8NCt7ou7e0k13xA0YeP4HbVk/jZe50NX/9x59EyM67s1XG6DmX8/Kx8tlU08OpWDwmxLv6peAzXnzueouzuJX/B0iomfHqcrGOg2GQdZqAFq4+emxJL3sZfkrL3JWT8PFj0QMB5Snur61DGo5LjY7j+3PF8+cyxZFvBLtNP4Zysw5ghpamp6dg8o0fro2dnZ+N2u8nY/w9cL94O3hb41H0w+3oIU52X/1yxrVtgB0hLiuO2BT2PdjEmnCy4m6jQ1tZGVVVV8ProrfWw4ttOBcf8Yvjsg5A9od/HLTvUyIslB3ihZD+VDS0Bt6kI4SlSY8LNgrsZsjrm0aurq1HVwPXRd74Gz38NGqvhwh/A2d+CmL5/61c2NLOi5AAvlBxgw8eHAJhZkE5aYhx1zf0bymhMuFhwN0NKsDz66NGjyTv4Lilv/gSp93eInv8dZ2Lq9Q9DzhT44pMw+tSg+z7e5NCHjrTy0uYKXti0nzUf1aAKJ+elcsclk/nMjNEUZiUHzLnbUEYTKSF1qIrIQuDXOPXcH1LVnwbZ7vPA08BsVT1ub6l1qJreOG4ePSMD1+anO5cPAMD/0NFZt8AFP4C44E+WBgrMibEuPjdrDPvrmnhzZzVenzI+ewSfmTmaz8wcxYTc7uPfj/cLwphwCLVDtcfgLiIxwA5gAc50euuAq1V1a5ftUoEXgXjgFgvupr+C5dHdbnf3ui6/nB54mrsROXDHrh6PdfZPX6c8SPXE/PSkYwF96qiRAzpzkjE9CedomTOAXaq6x7/jJ4BFwNYu2/0Y+BlwRy/baswxIefRuwo2f+mR6m6LVJWqhha2+0vn7vA0BA3sAG/+2wUW0M2QE0pwzwc6/uSUAXM6biAis4ACVX1RRCy4m145bh79ePXRWw47o182/DHovvcnFFH20UGnDnpFw7F66LWNn3R8ZqfEkxDrosXr6/b5/PQkC+xmSOp3h6qIuIBfANeGsO0SYAlAYWFhfw9thrge8+jBxp8fKHECeslT0NoAOVNY4ZrHHO86Nupk1vimsF0L+NBXQFVzBjz4DgCpCbFMykvlU9NHMdmdwqS8VCa5U8lOSbDOUBN1Qgnu5UBBh/dj/MuOSgWmA6v8dzh5wHIRubxr3l1VlwJLwcm596PdZojqcTx6sProrUf8d+mPOBNTxyai066gfPxiXq4r5J4VH+Lieny4SKCVseIhAaeE7h+/OpvJ7lRGpSUGvQu3ui4m2oTSoRqL06E6HyeorwO+qKpbgmy/Cvi2daiao4Ll0d1d66MHmgkpdwqs/6OzrrUBX/Zkdhb8E39tO5uX9jRTetDJlce6BK+v+/dyfnoSb333wsE8XWMGVNg6VFXVKyK3AK/gDIV8WFW3iMjdwHpVXd7/5ppoczSPXlFRQVVV1bE8en5+Pm63u3sevWQZ3udvJbbd/zRnXSn6zBIExReTwJ6cBTzum8//7s+jtUxJiqvn7AlZLDnvJOZNymHDx4csrWJMByHl3FV1BbCiy7KAtVBVdV7/m2WGqr7m0RtX/JBkf2BXBRGo0pH8pX0+DzcvpO5ICifljOBLc3OZNzmHM4oySYiNOfb5gsxkwNIqxhxlT6iafutzHt3ng9K1sPmvxDdVgkCTxvOKr5g/ey9is47lTNc2vr1oDvMm5x4L4MFYuVxjPmHB3fRJn8ejq8L+jbSVPE17yV9JbKqghXhe9c3mhfYz+VjdnOXawtdjn2G2azvVmsaYM384uCdnTBSw4G5C1lMefdvrfyZhxX8Tr1VUSA6ls+5g9uU3OB/2bKV23RO4tjzDyKZSVGN4wzeTV+QLNI67mJq9m7k79gEmuz4ZiNWo8TwUfw13ReZ0jRnSLLibHoWSR1+3/EFOee+HJEkrCORRRdL6H7FuzyrGNm8jt/kjUtTF275prEm+Aj35MuZOm8A9RZkkxsXw3MYJ/OHZ/XzT9wSjpYb9msWvWMw5ly2J9OkbMyRZcDcB9TaPXvDefSRJKx5NY58vlxypZ5zLw+zaFazTKbyc9XUSTrmCuTNO5rys7lPMObnym7nqlfnWIWpMGFhwN8f0NY/e7lO2ePP5g17MdbEvMztmJ9t8hbzcXsxIjjDrzreYHRcT8LMdWYeoMeFjwX2Y6/V49A489c08ua6UJ9eVMsc3l5/EPUQz8ZT5spji2scU9lFBDokhBHZjTHhZcB+m+joevd2nrN5ZxV/W7mPlh5WIr43/l/0Mn4p/Dq+6SJNG0qTROYbGU3r6HeQN5okZYwAL7sNKn8ej49ylP7W+lL+8W0p5bRNZI+K5dW46N3h+TFL52zD3Zja2FFCw8RfkajWVkk3p6R1GyxhjBpUF9yh3NI9eUVFBTU3NcfPoXWcRuv3iSWSlJPD42o95bVsl7T7l7AlZfO/Sk7kkw0Pc01+CI1XOZNMzFzMbYNHNgFM9zu7YjYkcC+5RqC959K4lb8trm7h92SYUyBwRz/XnFLH4jEKKskc4RbwevRWSs+FfXobRp0XgLI0xx2PBPYr0uT468F8vf9ip6BaAAhnJcbzzvQudOi7tXnj532HNAzD2HPinRyAlZ2BPyhjTJxbch7j+5NEBKhuaeWp9GfvrmgOur21scwL7kRp4+lr4aDXMuREuvgdi4sJ9OsaYMLHgPgT1Jo8e+PPKW7ureXztPl7d6sHrU+JjXbQGmGZudHoSHNgET1wDhz1wxe/g1C8O1KkZY8IkpOAuIguBX+PUc39IVX/aZf1twPWAF6gC/kVVPw5zW4c1VaW+vv7YPKNerzfk8ehHVTW08NSGUp54t5R9BxvJHBHPv5xTxOLZBZSU1QWsh/7raTvhf74IyZlOfj1/1kCfqjEmDHoM7iISAzwALMCZHHudiCxX1a0dNtsIFKtqo4jcBPwXcNVANHi4OV4ePTMzs1NA7zra5Y5LJnP5zNG8vbuGx9/9mL9vce7S547P5PaLJ7Fwet6xmujjc1LIL/0bBe/dR65WUSnZkDuVvA2rofAsuPJRSMmN1GUwxvRSKNPsnQncpaqX+N9/D0BVfxJk+9OA36jq2cfbr02zF1ywPLrb7Q6aRw80wXOsS0hPjqP6cCsZyXF84fQxLD6jkJNyUroftGQZvPB1aGvqvHz8BfDPT1l+3ZgTRNim2QPygdIO78uAOcfZ/jrgpRD2azoIlkcfP348ubm5PebR73tle7fRLl6fUt/k5deLT+WSaXnHLwOw8u7ugR2gZpcFdmOGoLB2qIrINUAxcH6Q9UuAJQCFhYXhPPSQFI48OoC33Ud5bYDADLS1+1h0ag/FuJpqoa408Lq6sh6Pb4w58YQS3MuBgg7vx/iXdSIiFwHfB85X1ZZAO1LVpcBScNIyvW5tlOhNHv14Glu9PLW+jIfe3BN0m9HpSYFXqELZOtjwCGx+JvhB0saE1BZjzIkllOC+DpgoIkU4QX0x0GksnD/P/iCwUFUrw97KKNDf8egd1Rxu4dF3Puaxd/ZyqLGNWYXpLJji5vG1+2juMJwxKS6GOy6Z3PnDTbVOfn3DI1C5BeJTYOZiGJkPb/68c2omLgnmB5wH3RhzgusxoqiqV0RuAV7BGQr5sKpuEZG7gfWquhy4D0gBnvLfde5T1csHsN1DQn/z6F3trT7CH97Yw9Mbymjx+rhoipsbzx9P8bhMAGaMSe82WuaK0/L9d+nrYcMfnbt0bxOMOhU+/Ss45QuQkOocIGOsk3uvK3Pu2OffCTOuDPdlMcYMgh5HywyUaB0tEyyP7na7e5VH72jjvkMsXb2Hl7dUEOdy8blZ+Vx/7ngm5HYZ9VKyrHNwPu/b0N7m3KV7Njt36ad8AU6/1urBGDNEhXO0jAlBOPLoHcepj0pL5LIZo9hUWse7ew8yMjGWm84/iWvPGkfuyAB3/F2HMtaVwgvfcF6Pmtn9Lt0YE9UsuPfD0Tx6RUUF9fX1fFjRwMrdh9lzOJakkRl8e2EaU7OyQtpX13Hq++ua+cMbH5GeHMcPLpvC4jMKSUkI8uXytjgFvQINZRzhhhtW9/UUjTFDlAX3XgqWRy/zpvLbLYepbxvpbFjXwneeLmFTaS1TRo+kodlLQ3Mbh5u9zuuWNv8yZ/lH1UfwBciQJcfFcP2547uvOPQx7HoVdr7qFPNqawzc4CPWv23McGTBPQShjEf/+k9ep76tc+qltd3HH9/e22lZcnwMKQmxpCbGkpoYR2piLKPTE9lddYTxlHO+q4R5MRsZTwX3tV/JC3XnOB/0tsDHbzvBfNerUL3DWZ4+1inkteU5aKzu3ngbymjMsGTB/ThCyaPXN7fxy9d2UlEfuGQuwBvfuYCyk3+MAAARUklEQVSRiXGMSIghNiZwTfW77vkR32n7LcnSemzZz1x/4OKYbfD4Y/678yMQEw9jz3Y6RSdeDFkTQAQK5nQvH2BDGY0Ztiy4d9E1jw6Bx6MfafHyyNt7Wbp6D3VNbSTGuWhu614yNz89iYLM5B6P+524J0n2tnZaliRtfNr3D6gsdMaiT1wARedB/IjuOzg6ZNGGMhpjsOAOOHn0mpoaPB5Pj+PRm9va+d81H/O7VbupOdLKhSfnctuCSeyqPBywZG63h4i68j8pmtx0IPg23yhx7s57MuNKC+bGGGAYB/fe1nVp9fp4cn0pv3l9J576Fs6ekMVtCyZz+tgMAKbnpwEEfoio+8GhogQ2/xU2Pwt1+4I3NK0gtMBujDEdDLvg3tvx6N52H89sLOf+lTspO9RE8dgMfnXVaZx5Uvchjleclh84mB9Vtd0f0P/qVFt0xToldS/4d2hvgZe/azlzY0xYDIvgHiyPPnbsWLKzs4/l0bs+RDR/ips3d1XzUfURZoxJ497PnsJ5E7ODP5DU9QnR+XfCmNmw5RnnsX/PZkBg3Dlw5i0w5XIY0eGXRFyy5cyNMWERteUHguXR8/LyAtZ1CTTZBcCotET+4/JpLJjqPv5TpoEmuxBxUjDgjGaZ/nmYughS88J1msaYYWZYlh/oa330dp9y74vbugV2AAEuntZDMFaFv/+g+xOiqpCYBje+CelWv94YM3iiIrj3pa7LwSOtrN5RxartlazeWc3BI60B9gwH6oKMX2+ug93/8D8l+hoc9gTZrt4CuzFm0A3Z4B5qHv0on08pKa9j1fZKVm2vYlNZLaqQOSKe8yflsGp7JYca27od59hkF6pOznznq7DrNdi3BrTduTM/6ULY83/QdLB7Q+0JUWNMBAyp4N7TePSXt9XwtUe2HhuKePO8k0hJjGXV9ir+b0cVB4+0IgIzx6TzjfkTuWByLqfkp+FyCc9tLOfNZ3/LN3mC0VLNfs3m93yWy0+ZBs/fArtWQsN+pyF5M+Ccb8KEBU6HaUxs4Jy7jXYxxkRISB2qIrIQ+DXOZB0PqepPu6xPAP4EnA7UAFep6t7j7TNQh+q65Q9S8N595GoVlZJD6aw7KP7Mkm559Pj4eHJzczvl0Z99r4yVzz7Ep/UNPtQCXvUVs0XHAXLs7nze5BzOnZhD5oj47g3a9CTe528l1vfJDIGKk3MnIQ1OusB5QnTCRcE7RAONlrHRLsaYMAq1Q7XH4C4iMcAOYAFQhjPt3tWqurXDNjcDM1T1RhFZDHxWVa863n67Bvd1yx9k+oYfkOSvrdJEIvt0FCUFXyZnwiy8PpCkVFriUjnkTaCivpn9tc3sr23iQF0TpTWHacep2yL4mCm7Odu1mb3xk7j/h3cQ422EunKnznl9uf91GdSXOa9rduGE8y5G5MJt25y7c2OMibBwjpY5A9ilqnv8O34CWARs7bDNIuAu/+ungd+IiGgvxlkWvHcfdSTxpu9UKsimnpE0E0/Tx6VsLDtIY2vnkSwikJYUx5zkODJS40mse4s8OUiWNFAk+xnh/yXR6osh5r8egObaLkcUSHE7d9juqVCzM3DDjlRZYDfGDDmhRK18oLTD+zJgTrBt/HOu1gFZQKcatCKyBFgCUFjYeQRJrlaxUScR54rhZMpx8z65VJOIfxRLgEwKXqDe/y/ImcRJuzMD0ch8J5CnjXFep46C2A47/eV0566+K+sQNcYMQYN6S6qqS4Gl4KRlOq6rlBymsod9vkbGSgWJ4gWgikxybn2tx303Lb2EpJaq7suTRpF82c97btz8O61D1BgTNUIJ7uVAQYf3Y/zLAm1TJiKxQBpOx2rISmfdQdqGHzDZVXZsWZPGs/f075KTdVKPn0+67D+dDtH2T8ale2MSSf7U3aE1wErmGmOiSCjBfR0wUUSKcIL4YuCLXbZZDnwFeAf4AvB6b/LtALMvv4F14B8tU02lZFN6+h3MvvyG0HYw40rnZDoE59jeBmcrmWuMiRKhDoW8FPgVzlDIh1X1XhG5G1ivqstFJBF4DDgNOAgsPtoBG8xA15YxxphoFNbaMqq6AljRZdmdHV43A//U20YaY4wZGIEn9DTGGDOkWXA3xpgoZMHdGGOiUMQm6xCRKuDjIKuz6fIA1DBj5z+8zx/sGtj5Bz//saqa09MOIhbcj0dE1ofSGxyt7PyH9/mDXQM7//6fv6VljDEmCllwN8aYKHSiBvelkW5AhNn5m+F+Dez8++mEzLkbY4zpnxP1zt0YY0w/RCy4i8hCEdkuIrtE5LsB1ieIyJP+9WtFZNzgt3JghXANbhORrSJSIiIrRWRsJNo5UHo6/w7bfV5EVESiavREKOcvIlf6vwe2iMjjg93GgRbCz0ChiPxDRDb6fw4ujUQ7B4qIPCwilSKyOch6EZH7/denRERmhbxzVR30fzgFyHYD43Gm4dgETO2yzc3A7/2vFwNPRqKtEb4GFwDJ/tc3RdM1COX8/dulAquBNUBxpNs9yF//icBGIMP/PjfS7Y7ANVgK3OR/PRXYG+l2h/kanAfMAjYHWX8p8BLOdM5zgbWh7jtSd+7Hpu5T1Vbg6NR9HS0CHvW/fhqYLyIyiG0caD1eA1X9h6o2+t+uwamlHy1C+R4A+DHwM6A5wLqhLJTz/1fgAVU9BKCqlYPcxoEWyjVQYKT/dRqwfxDbN+BUdTVOJd1gFgF/UscaIF1ERoWy70gF90BT9+UH20ZVvcDRqfuiRSjXoKPrcH6DR4sez9//J2iBqr44mA0bJKF8/ScBk0TkLRFZIyILB611gyOUa3AXcI2IlOFUpr11cJp2wuhtnDjGZn4eAkTkGqAYOD/SbRksIuICfgFcG+GmRFIsTmpmHs5fbatF5BRV7TrbezS7GnhEVX8uImcCj4nIdFX1RbphJ7pI3bn3Zuo++jp13wkulGuAiFwEfB+4XFVbBqltg6Gn808FpgOrRGQvTr5xeRR1qoby9S8Dlqtqm6p+BOzACfbRIpRrcB2wDEBV3wEScequDBchxYlAIhXcj03dJyLxOB2my7tsc3TqPujj1H0nuB6vgYicBjyIE9ijLd963PNX1TpVzVbVcao6DqfP4XJVjZbpu0L5GXgO564dEcnGSdMcd4azISaUa7APmA8gIlNwgnvVoLYyspYDX/aPmpkL1KnqgZA+GcFe4ktx7kR2A9/3L7sb5wcYnC/iU8Au4F1gfKR7tiNwDV4DPMD7/n/LI93mwTz/LtuuIopGy4T49Rec1NRW4AOc6Ssj3u5BvgZTgbdwRtK8D1wc6TaH+fz/AhwA2nD+UrsOuBG4scP3wAP+6/NBb34G7AlVY4yJQvaEqjHGRCEL7sYYE4UsuBtjTBSy4G6MMVEoYg8xZWdn67hx4yJ1eGOMGZI2bNhQrSHMoRqx4D5u3DjWr4+WIcvGGDM4ROTjULaztIwxxkQhqy1jjAFg3HcD12fb+9PLBrklJhzszt0YY6KQBXdjjIlCFtyNMSYKWXA3xpgoZMHdGGOikAV3Y4yJQhbcjTEmCllwN8aYKGTB3RhjopA9oWqMMUTfE7q9vnMXkYdFpFJENndYlikir4rITv//GeFtpjHGmN7oS1rmEWBhl2XfBVaq6kRgpf+9McaYCOl1cFfV1cDBLosXAY/6Xz8KXNHPdhljjOmHcHWoulX1gP91BeAO036NMcb0QdhHy6iqAhponYgsEZH1IrK+qqoq3Ic2xhjjF67g7hGRUQD+/ysDbaSqS1W1WFWLc3J6nCXKGGNMH4UruC8HvuJ//RXg+TDt1xhjTB/0ZSjkX4B3gMkiUiYi1wE/BRaIyE7gIv97Y4wxEdLrh5hU9eogq+b3sy1DWrQ9AGHMUGM/g51Z+QFjjIlCFtyNMSYKWXA3xpgoZMHdGGOikFWFjBLWmTS0Bfv6gX0NTd/YnbsxxkQhC+7GGBOFLLgbY0wUsuBujDFRyDpUDdD/Dlnr0DXmxGJ37sYYE4UsuBtjTBSy4G6MMVHIgrsxxkQh61A1JgzsCVNzoglrcBeRvUAD0A54VbU4nPs3xhgTmoG4c79AVasHYL/GGGNCZDl3Y4yJQuEO7gr8XUQ2iMiSMO/bGGNMiMKdljlHVctFJBd4VUQ+VNXVR1f6A/4SgMLCwjAfuu+sMyzy+vs1sK9h5NlTyieWsN65q2q5//9K4FngjC7rl6pqsaoW5+TkhPPQxhhjOghbcBeRESKSevQ1cDGwOVz7N8YYE7pwpmXcwLMicnS/j6vqy2HcvzHGmBCFLbir6h5gZrj2Z4wxQ8mJ1u9jQyGNMSYKWXA3xpgoZMHdGGOikAV3Y4yJQlYV0hhOvM6wvhjqDxEN9fafaOzO3RhjopAFd2OMiUIW3I0xJgpZcDfGmCgUFR2q0dAZZowx4WR37sYYE4UsuBtjTBSy4G6MMVHIgrsxxkShqOhQjQb2dJ4xJpzCORPTQhHZLiK7ROS74dqvMcaY3gtLcBeRGOAB4FPAVOBqEZkajn0bY4zpvXDduZ8B7FLVParaCjwBLArTvo0xxvRSuIJ7PlDa4X2Zf5kxxpgIEFXt/05EvgAsVNXr/e+/BMxR1Vu6bLcEWOJ/OxnYHmSX2UB1vxs2dNn5D+/zB7sGdv7Bz3+squb0tINwjZYpBwo6vB/jX9aJqi4Flva0MxFZr6rFYWrbkGPnP7zPH+wa2Pn3//zDlZZZB0wUkSIRiQcWA8vDtG9jjDG9FJY7d1X1isgtwCtADPCwqm4Jx76NMcb0XtgeYlLVFcCKMO2ux9RNlLPzN8P9Gtj591NYOlSNMcacWKy2jDHGRKGIBfeeyhWISIKIPOlfv1ZExg1+KwdWCNfgNhHZKiIlIrJSRMZGop0DJdSSFSLyeRFREYmq0ROhnL+IXOn/HtgiIo8PdhsHWgg/A4Ui8g8R2ej/Obg0Eu0cKCLysIhUisjmIOtFRO73X58SEZkV8s5VddD/4XS67gbGA/HAJmBql21uBn7vf70YeDISbY3wNbgASPa/vimarkEo5+/fLhVYDawBiiPd7kH++k8ENgIZ/ve5kW53BK7BUuAm/+upwN5ItzvM1+A8YBawOcj6S4GXAAHmAmtD3Xek7txDKVewCHjU//ppYL6IyCC2caD1eA1U9R+q2uh/uwbn+YFoEWrJih8DPwOaB7NxgyCU8/9X4AFVPQSgqpWD3MaBFso1UGCk/3UasH8Q2zfgVHU1cPA4mywC/qSONUC6iIwKZd+RCu6hlCs4to2qeoE6IGtQWjc4eluy4Tqc3+DRosfz9/8JWqCqwSfJHbpC+fpPAiaJyFsiskZEFg5a6wZHKNfgLuAaESnDGY136+A07YTR59IuVs99CBCRa4Bi4PxIt2WwiIgL+AVwbYSbEkmxOKmZeTh/ta0WkVNUtTairRpcVwOPqOrPReRM4DERma6qvkg37EQXqTv3UMoVHNtGRGJx/iSrGZTWDY6QSjaIyEXA94HLVbVlkNo2GHo6/1RgOrBKRPbi5BuXR1Gnaihf/zJguaq2qepHwA6cYB8tQrkG1wHLAFT1HSARp+7KcBFSnAgkUsE9lHIFy4Gv+F9/AXhd/T0MUaLHayAipwEP4gT2aMu3Hvf8VbVOVbNVdZyqjsPpc7hcVddHprlhF8rPwHM4d+2ISDZOmmbPYDZygIVyDfYB8wFEZApOcK8a1FZG1nLgy/5RM3OBOlU9ENInI9hLfCnOnchu4Pv+ZXfj/ACD80V8CtgFvAuMj3TPdgSuwWuAB3jf/295pNs8mOffZdtVRNFomRC//oKTmtoKfAAsjnSbI3ANpgJv4YykeR+4ONJtDvP5/wU4ALTh/KV2HXAjcGOH74EH/Nfng978DNgTqsYYE4XsCVVjjIlCFtyNMSYKWXA3xpgoZMHdGGOikAV3Y4yJQhbcjTEmCllwN8aYKGTB3RhjotD/B+dKSOGhnNyyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_obs_bins = np.array([0] + list(np.diff(n_obs)))\n",
    "calibration_plot(p_exp, p_obs, p_exp, p_raw, n_obs=n_obs_bins)\n",
    "print(mean_calibration_error(p_exp, p_raw))\n",
    "print(mean_calibration_error(p_exp, p_obs))\n",
    "print(mean_calibration_error2(p_exp, p_raw, n_obs))\n",
    "print(mean_calibration_error2(p_exp, p_obs, n_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.41354138117001\n"
     ]
    }
   ],
   "source": [
    "from src.eval import pinball_loss\n",
    "\n",
    "alphas=np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "q_pred=np.empty([y_ts.shape[0], alphas.shape[0]])\n",
    "for i, alpha in enumerate(alphas):\n",
    "    q_pred[:,i] = clf.predict_quantile(X_ts, alpha).to_numpy().flatten()\n",
    "    \n",
    "print(pinball_loss(y_ts, q_pred, alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4170993907043843\n"
     ]
    }
   ],
   "source": [
    "from src.eval import pinball_loss\n",
    "\n",
    "alphas=np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "q_pred=np.empty([y_ts.shape[0], alphas.shape[0]])\n",
    "for i, alpha in enumerate(alphas):\n",
    "    q_pred[:,i] = calibrated_model.predict_quantile(X_ts, alpha).to_numpy().flatten()\n",
    "    \n",
    "print(pinball_loss(y_ts, q_pred, alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 13)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution-Calibrated Bayesian DNN with Quantile Output: Manual Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute list of quantiles from base model\n",
    "representation_quantiles=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "X_recal = np.zeros([y_tr.shape[0], len(representation_quantiles)])\n",
    "for i, alpha in enumerate(representation_quantiles):\n",
    "    X_recal[:, i] = clf.predict_quantile(X_tr, alpha).to_numpy().flatten()\n",
    "    \n",
    "# compute list of quantiles from base model\n",
    "X_recal_ts = np.zeros([y_ts.shape[0], len(representation_quantiles)])\n",
    "for i, alpha in enumerate(representation_quantiles):\n",
    "    X_recal_ts[:, i] = clf.predict_quantile(X_ts, alpha).to_numpy().flatten()    \n",
    "\n",
    "# create data\n",
    "# X_recal = np.hstack([mean[:, np.newaxis], var[:, np.newaxis]])\n",
    "X_recal = np.array(X_recal, dtype=np.float32)\n",
    "X_tr = np.array(X_tr, dtype=np.float32)\n",
    "y_tr = np.array(y_tr, dtype=np.float32)\n",
    "\n",
    "# create data\n",
    "# X_recal = np.hstack([mean[:, np.newaxis], var[:, np.newaxis]])\n",
    "X_recal_ts = np.array(X_recal_ts, dtype=np.float32)\n",
    "X_ts = np.array(X_ts, dtype=np.float32)\n",
    "y_ts = np.array(y_ts, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([24.189863, 26.26575 , 27.762611, 29.041624, 30.237087, 31.432549,\n",
       "        32.711563, 34.208424, 36.28431 ], dtype=float32), 32.5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_recal[2], y_tr[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5105748\n",
      "3.6270797\n"
     ]
    }
   ],
   "source": [
    "# y_tr_pred = clf.predict_quantile(X_tr, 0.5).to_numpy().flatten()\n",
    "print(np.abs(X_recal[:,5]-y_tr).mean())\n",
    "# y_ts_pred = clf.predict_quantile(X_ts, 0.5).to_numpy().flatten()\n",
    "print(np.abs(X_recal_ts[:,5]-y_ts).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.704947 49.544167 5.0 50.0\n",
      "-10.916882 49.489086 5.6 50.0\n"
     ]
    }
   ],
   "source": [
    "print(X_recal.min(), X_recal.max(), y_tr.min(), y_tr.max())\n",
    "print(X_recal_ts.min(), X_recal_ts.max(), y_ts.min(), y_ts.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.8821704 2.7018776 5.0 50.0\n",
      "-3.555615 2.6947308 5.6 50.0\n"
     ]
    }
   ],
   "source": [
    "X_recal = scaler.fit_transform(X_recal)\n",
    "X_recal_ts = scaler.transform(X_recal_ts)\n",
    "\n",
    "print(X_recal.min(), X_recal.max(), y_tr.min(), y_tr.max())\n",
    "print(X_recal_ts.min(), X_recal_ts.max(), y_ts.min(), y_ts.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1184/1184 [==============================] - 1s 743us/step - loss: 2.3015 - mae: 5.1521\n",
      "Epoch 2/2\n",
      "1184/1184 [==============================] - 1s 736us/step - loss: 1.0215 - mae: 2.6467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x169383518>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from src.simple import QuantileRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# set up recalibrator\n",
    "batch_size = 32\n",
    "recalibrator_ = QuantileRegressor(batch_size=batch_size)\n",
    "a_vals = 0.5*np.ones([X_recal.shape[0],1], dtype=np.float32)\n",
    "recalibrator_.predict([X_recal, a_vals])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# X_recal = scaler.fit_transform(X_recal)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_recal, y_tr[:,np.newaxis]))\n",
    "dataset = dataset.shuffle(400).repeat(100).batch(batch_size, drop_remainder=True)\n",
    "recalibrator_.compile(optimizer=keras.optimizers.Adam(1e-3))\n",
    "recalibrator_.fit(dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 9)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_recal_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(379,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run recalibrator\n",
    "a_vals = 0.5*np.ones([X_recal.shape[0],1], dtype=np.float32)\n",
    "Y_pred = recalibrator_([X_recal, a_vals])\n",
    "q_vals = np.array(Y_pred).flatten()\n",
    "\n",
    "# return output\n",
    "q_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6150377"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(q_vals.flatten()-y_tr).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.648355 25.725842 28.542845 19.232471 14.24001 ]\n",
      "0    27.601031\n",
      "1    28.388818\n",
      "2    30.237087\n",
      "3    19.684493\n",
      "4    13.716376\n",
      "Name: y_pred, dtype: float64\n",
      "[23.9 28.7 32.5 13.3 13.3]\n"
     ]
    }
   ],
   "source": [
    "print(q_vals[:5].flatten())\n",
    "print(y_tr_pred[:5])\n",
    "print(y_tr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run recalibrator\n",
    "# X_recal_ts = scaler.fit_transform(X_recal_ts)\n",
    "a_vals = 0.5*np.ones([X_recal_ts.shape[0],1], dtype=np.float32)\n",
    "Y_pred = recalibrator_([X_recal_ts, a_vals])\n",
    "q_vals = np.array(Y_pred).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.078633"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(q_vals.flatten()-y_ts).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution-Calibrated Bayesian DNN with Quantile Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1175/1184 [============================>.] - ETA: 0s - loss: 1.4655 - mae: 3.2006"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    /Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1233 test_function  *\n        return step_function(self, iterator)\n    /Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1224 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1217 run_step  **\n        outputs = model.test_step(data)\n    /Users/kuleshov/work/afresh/afresh-notebooks/experiments/icml2021/src/simple.py:373 test_step\n        q_pred[:,i] = np.array(self([x, a_vec])).flatten()\n    /Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:855 __array__\n        \" a NumPy call, which is not supported\".format(self.name))\n\n    NotImplementedError: Cannot convert a symbolic Tensor (quantile_regressor_1/dense_5/BiasAdd_2:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-af7d41724ae7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_ts2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_ts2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcalibrated_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_ts2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ts2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/afresh/afresh-notebooks/experiments/icml2021/src/calibrated2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, Xval, yval)\u001b[0m\n\u001b[1;32m     89\u001b[0m         self.recalibrator_.fit(\n\u001b[1;32m     90\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_recal_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         )\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1139\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1142\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    /Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1233 test_function  *\n        return step_function(self, iterator)\n    /Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1224 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:1217 run_step  **\n        outputs = model.test_step(data)\n    /Users/kuleshov/work/afresh/afresh-notebooks/experiments/icml2021/src/simple.py:373 test_step\n        q_pred[:,i] = np.array(self([x, a_vec])).flatten()\n    /Users/kuleshov/work/env/afresh-core4/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:855 __array__\n        \" a NumPy call, which is not supported\".format(self.name))\n\n    NotImplementedError: Cannot convert a symbolic Tensor (quantile_regressor_1/dense_5/BiasAdd_2:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\n"
     ]
    }
   ],
   "source": [
    "from src.calibrated2 import DistributionCalibratedQuantileRegressor\n",
    "\n",
    "calibrated_model = DistributionCalibratedQuantileRegressor(clf)\n",
    "X_ts2 = np.concatenate([X_ts, X_ts[[0],:]], axis=0)\n",
    "y_ts2 = np.concatenate([y_ts, y_ts[[0]]], axis=0)\n",
    "calibrated_model.fit(X_tr, y_tr, X_ts2, y_ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.079155672823219\n",
      "0.05 0.11609498680738786\n",
      "0.1 0.16094986807387862\n",
      "0.15000000000000002 0.19525065963060687\n",
      "0.2 0.2638522427440633\n",
      "0.25 0.316622691292876\n",
      "0.30000000000000004 0.37730870712401055\n",
      "0.35000000000000003 0.43007915567282323\n",
      "0.4 0.5145118733509235\n",
      "0.45 0.5778364116094987\n",
      "0.5 0.633245382585752\n",
      "0.55 0.6886543535620053\n",
      "0.6000000000000001 0.7387862796833773\n",
      "0.65 0.7810026385224275\n",
      "0.7000000000000001 0.8179419525065963\n",
      "0.75 0.8443271767810027\n",
      "0.8 0.8680738786279684\n",
      "0.8500000000000001 0.8891820580474934\n",
      "0.9 0.9050131926121372\n",
      "0.9500000000000001 0.9234828496042217\n"
     ]
    }
   ],
   "source": [
    "p_exp = np.arange(0,1,0.05)\n",
    "p_obs = prep_calibration(calibrated_model, X_tr, y_tr, p_exp)\n",
    "p_raw = prep_calibration(clf, X_tr, y_tr, p_exp)\n",
    "n_obs = [po * X_ts.shape[0] for po in p_obs]\n",
    "for pe, po in zip(p_exp, p_obs): print(pe, po)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.11023622047244094\n",
      "0.05 0.14173228346456693\n",
      "0.1 0.1889763779527559\n",
      "0.15000000000000002 0.2204724409448819\n",
      "0.2 0.25984251968503935\n",
      "0.25 0.31496062992125984\n",
      "0.30000000000000004 0.3779527559055118\n",
      "0.35000000000000003 0.48031496062992124\n",
      "0.4 0.5275590551181102\n",
      "0.45 0.5826771653543307\n",
      "0.5 0.6377952755905512\n",
      "0.55 0.7244094488188977\n",
      "0.6000000000000001 0.7401574803149606\n",
      "0.65 0.7874015748031497\n",
      "0.7000000000000001 0.8267716535433071\n",
      "0.75 0.8582677165354331\n",
      "0.8 0.8740157480314961\n",
      "0.8500000000000001 0.8818897637795275\n",
      "0.9 0.8818897637795275\n",
      "0.9500000000000001 0.8976377952755905\n"
     ]
    }
   ],
   "source": [
    "p_exp = np.arange(0,1,0.05)\n",
    "p_obs = prep_calibration(calibrated_model, X_ts, y_ts, p_exp)\n",
    "p_raw = prep_calibration(clf, X_ts, y_ts, p_exp)\n",
    "n_obs = [po * X_ts.shape[0] for po in p_obs]\n",
    "for pe, po in zip(p_exp, p_obs): print(pe, po)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0592771416952526\n",
      "0.1057051789042036\n",
      "0.07390566802485989\n",
      "0.05915173751030211\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_obs_bins = np.array([0] + list(np.diff(n_obs)))\n",
    "calibration_plot(p_exp, p_obs, p_exp, p_raw, n_obs=n_obs_bins)\n",
    "print(mean_calibration_error(p_exp, p_raw))\n",
    "print(mean_calibration_error(p_exp, p_obs))\n",
    "print(mean_calibration_error2(p_exp, p_raw, n_obs))\n",
    "print(mean_calibration_error2(p_exp, p_obs, n_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1105580563516027\n"
     ]
    }
   ],
   "source": [
    "from src.eval import pinball_loss\n",
    "\n",
    "alphas=np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "q_pred=np.empty([y_ts.shape[0], alphas.shape[0]])\n",
    "for i, alpha in enumerate(alphas):\n",
    "    q_pred[:,i] = calibrated_model.predict_quantile(X_ts, alpha).to_numpy().flatten()\n",
    "    \n",
    "print(pinball_loss(y_ts, q_pred, alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2619363588379542\n"
     ]
    }
   ],
   "source": [
    "from src.eval import pinball_loss\n",
    "\n",
    "alphas=np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "q_pred=np.empty([y_tr.shape[0], alphas.shape[0]])\n",
    "for i, alpha in enumerate(alphas):\n",
    "    q_pred[:,i] = calibrated_model.predict_quantile(X_tr, alpha).to_numpy().flatten()\n",
    "    \n",
    "print(pinball_loss(y_tr, q_pred, alphas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load a model and use it to make forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: mpg\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "918/918 [==============================] - 2s 2ms/step - loss: 1.2986 - mae: 2.7476\n",
      "Epoch 2/2\n",
      "918/918 [==============================] - 2s 2ms/step - loss: 0.9458 - mae: 2.3128\n",
      "Dataset: boston\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "1184/1184 [==============================] - 3s 2ms/step - loss: 1.6504 - mae: 3.4781\n",
      "Epoch 2/2\n",
      "1184/1184 [==============================] - 2s 2ms/step - loss: 1.2230 - mae: 3.1082\n",
      "Dataset: yacht\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "721/721 [==============================] - 2s 2ms/step - loss: 1.1760 - mae: 2.4171\n",
      "Epoch 2/2\n",
      "721/721 [==============================] - 1s 2ms/step - loss: 0.3463 - mae: 0.8636\n",
      "Dataset: wine\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "3746/3746 [==============================] - 7s 2ms/step - loss: 0.2113 - mae: 0.5955\n",
      "Epoch 2/2\n",
      "3746/3746 [==============================] - 7s 2ms/step - loss: 0.2085 - mae: 0.5945\n",
      "Dataset: crime\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "3746/3746 [==============================] - 7s 2ms/step - loss: 0.1979 - mae: 0.5382\n",
      "Epoch 2/2\n",
      "3746/3746 [==============================] - 7s 2ms/step - loss: 0.1947 - mae: 0.5361\n",
      "Dataset: auto\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "3746/3746 [==============================] - 7s 2ms/step - loss: 0.2545 - mae: 0.6905\n",
      "Epoch 2/2\n",
      "3746/3746 [==============================] - 7s 2ms/step - loss: 0.2426 - mae: 0.6778\n",
      "Dataset: cpu\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "487/487 [==============================] - 2s 2ms/step - loss: 15.1534 - mae: 30.0457\n",
      "Epoch 2/2\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 13.5947 - mae: 27.7935\n",
      "Dataset: bank\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "487/487 [==============================] - 2s 2ms/step - loss: 14.7627 - mae: 29.6435\n",
      "Epoch 2/2\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 13.7663 - mae: 28.3849\n",
      "Dataset: wisconsin\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 1.6062 - mae: 3.2696\n",
      "Epoch 2/2\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 1.4342 - mae: 3.2495\n"
     ]
    }
   ],
   "source": [
    "from src.linreg import BayesianLinearRegressor\n",
    "from src.calibrated1 import CalibratedRegressor\n",
    "from src.calibrated2 import DistributionCalibratedRegressor\n",
    "l_out = []\n",
    "metric_names = ['rmse', 'r2', 'mape']\n",
    "dataset_names = datasets.keys()\n",
    "# dataset_names.remove('wisconsin')\n",
    "# dataset_names.remove('yacht')\n",
    "alphas=np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "for dataset in dataset_names:\n",
    "    print('Dataset: %s' % dataset)\n",
    "    (X_tr, y_tr, X_ts, y_ts) = datasets[dataset]\n",
    "    clf = BayesianLinearRegressor()\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    y_ts_pred = clf.predict(X_ts)['y_pred']\n",
    "    l_out0 = [dataset] + [metrics[m](y_ts, y_ts_pred) for m in metric_names]\n",
    "    \n",
    "    print('Recal 1')\n",
    "    calibrated_model = CalibratedRegressor(clf)\n",
    "    calibrated_model.fit(X_tr, y_tr)\n",
    "\n",
    "    print('Recal 2')    \n",
    "    calibrated_model2 = DistributionCalibratedQuantileRegressor(clf)\n",
    "    calibrated_model2.fit(X_tr, y_tr)\n",
    "    \n",
    "    p_exp = np.arange(0,1,0.05)\n",
    "    p_obs = prep_calibration(calibrated_model, X_ts, y_ts, p_exp)\n",
    "    p_obs2 = prep_calibration(calibrated_model2, X_ts, y_ts, p_exp)    \n",
    "    p_raw = prep_calibration(clf, X_ts, y_ts, p_exp)\n",
    "    \n",
    "    \n",
    "    q_pred0=np.empty([y_ts.shape[0], alphas.shape[0]])\n",
    "    q_pred1, q_pred2 = q_pred0.copy(), q_pred0.copy()\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        q_pred0[:,i] = clf.predict_quantile(X_ts, alpha).to_numpy().flatten()\n",
    "        q_pred1[:,i] = calibrated_model.predict_quantile(X_ts, alpha).to_numpy().flatten()\n",
    "        q_pred2[:,i] = calibrated_model2.predict_quantile(X_ts, alpha).to_numpy().flatten()\n",
    "    \n",
    "    cal_err0 = mean_calibration_error(p_exp, p_raw)\n",
    "    cal_err1 = mean_calibration_error(p_exp, p_obs)\n",
    "    cal_err2 = mean_calibration_error(p_exp, p_obs2)\n",
    "    pbl_err0 = pinball_loss(y_ts, q_pred0, alphas)\n",
    "    pbl_err1 = pinball_loss(y_ts, q_pred1, alphas)\n",
    "    pbl_err2 = pinball_loss(y_ts, q_pred2, alphas)\n",
    "    l_out0 += [cal_err0, cal_err1, cal_err2, pbl_err0, pbl_err1, pbl_err2]\n",
    "    \n",
    "    l_out += [l_out0]\n",
    "    \n",
    "columns = ['dataset'] + metric_names + ['cal0', 'cal1', 'cal2', 'pbl0', 'pbl1', 'pbl2']\n",
    "df_lin = pd.DataFrame(l_out, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "      <th>mape</th>\n",
       "      <th>cal0</th>\n",
       "      <th>cal1</th>\n",
       "      <th>cal2</th>\n",
       "      <th>pbl0</th>\n",
       "      <th>pbl1</th>\n",
       "      <th>pbl2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mpg</td>\n",
       "      <td>0.450487</td>\n",
       "      <td>0.842001</td>\n",
       "      <td>0.113878</td>\n",
       "      <td>0.058150</td>\n",
       "      <td>0.057111</td>\n",
       "      <td>0.108137</td>\n",
       "      <td>0.921462</td>\n",
       "      <td>0.915590</td>\n",
       "      <td>0.813040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boston</td>\n",
       "      <td>0.146133</td>\n",
       "      <td>0.750254</td>\n",
       "      <td>0.170834</td>\n",
       "      <td>0.066061</td>\n",
       "      <td>0.029571</td>\n",
       "      <td>0.045723</td>\n",
       "      <td>1.392054</td>\n",
       "      <td>1.364643</td>\n",
       "      <td>1.174831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yacht</td>\n",
       "      <td>0.228579</td>\n",
       "      <td>0.697256</td>\n",
       "      <td>3.799502</td>\n",
       "      <td>0.079251</td>\n",
       "      <td>0.062512</td>\n",
       "      <td>0.067130</td>\n",
       "      <td>2.438080</td>\n",
       "      <td>2.378720</td>\n",
       "      <td>0.341987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wine</td>\n",
       "      <td>0.048067</td>\n",
       "      <td>0.143069</td>\n",
       "      <td>0.105653</td>\n",
       "      <td>0.037241</td>\n",
       "      <td>0.026511</td>\n",
       "      <td>0.175934</td>\n",
       "      <td>0.240240</td>\n",
       "      <td>0.239304</td>\n",
       "      <td>0.275805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crime</td>\n",
       "      <td>0.036404</td>\n",
       "      <td>0.283343</td>\n",
       "      <td>0.087373</td>\n",
       "      <td>0.030842</td>\n",
       "      <td>0.018464</td>\n",
       "      <td>0.052977</td>\n",
       "      <td>0.202428</td>\n",
       "      <td>0.201750</td>\n",
       "      <td>0.203892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>auto</td>\n",
       "      <td>0.073609</td>\n",
       "      <td>0.317768</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.042643</td>\n",
       "      <td>0.031647</td>\n",
       "      <td>0.076775</td>\n",
       "      <td>0.251485</td>\n",
       "      <td>0.250364</td>\n",
       "      <td>0.258581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cpu</td>\n",
       "      <td>11.802386</td>\n",
       "      <td>0.895540</td>\n",
       "      <td>0.647345</td>\n",
       "      <td>0.083220</td>\n",
       "      <td>0.074292</td>\n",
       "      <td>0.297215</td>\n",
       "      <td>15.871333</td>\n",
       "      <td>15.463072</td>\n",
       "      <td>14.676257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bank</td>\n",
       "      <td>7.854822</td>\n",
       "      <td>0.897250</td>\n",
       "      <td>0.589829</td>\n",
       "      <td>0.114589</td>\n",
       "      <td>0.068910</td>\n",
       "      <td>0.277675</td>\n",
       "      <td>17.478473</td>\n",
       "      <td>16.638647</td>\n",
       "      <td>14.338159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wisconsin</td>\n",
       "      <td>0.173360</td>\n",
       "      <td>-0.001090</td>\n",
       "      <td>1.231798</td>\n",
       "      <td>0.170843</td>\n",
       "      <td>0.035722</td>\n",
       "      <td>0.116190</td>\n",
       "      <td>1.484555</td>\n",
       "      <td>1.246294</td>\n",
       "      <td>1.302537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset       rmse        r2      mape      cal0      cal1      cal2  \\\n",
       "0        mpg   0.450487  0.842001  0.113878  0.058150  0.057111  0.108137   \n",
       "1     boston   0.146133  0.750254  0.170834  0.066061  0.029571  0.045723   \n",
       "2      yacht   0.228579  0.697256  3.799502  0.079251  0.062512  0.067130   \n",
       "3       wine   0.048067  0.143069  0.105653  0.037241  0.026511  0.175934   \n",
       "4      crime   0.036404  0.283343  0.087373  0.030842  0.018464  0.052977   \n",
       "5       auto   0.073609  0.317768  0.057800  0.042643  0.031647  0.076775   \n",
       "6        cpu  11.802386  0.895540  0.647345  0.083220  0.074292  0.297215   \n",
       "7       bank   7.854822  0.897250  0.589829  0.114589  0.068910  0.277675   \n",
       "8  wisconsin   0.173360 -0.001090  1.231798  0.170843  0.035722  0.116190   \n",
       "\n",
       "        pbl0       pbl1       pbl2  \n",
       "0   0.921462   0.915590   0.813040  \n",
       "1   1.392054   1.364643   1.174831  \n",
       "2   2.438080   2.378720   0.341987  \n",
       "3   0.240240   0.239304   0.275805  \n",
       "4   0.202428   0.201750   0.203892  \n",
       "5   0.251485   0.250364   0.258581  \n",
       "6  15.871333  15.463072  14.676257  \n",
       "7  17.478473  16.638647  14.338159  \n",
       "8   1.484555   1.246294   1.302537  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: mpg\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "918/918 [==============================] - 1s 854us/step - loss: 1.6300 - mae: 3.2984\n",
      "Epoch 2/2\n",
      "918/918 [==============================] - 1s 787us/step - loss: 1.0668 - mae: 2.5524\n",
      "Dataset: boston\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "1184/1184 [==============================] - 1s 783us/step - loss: 1.4301 - mae: 3.2402\n",
      "Epoch 2/2\n",
      "1184/1184 [==============================] - 1s 799us/step - loss: 1.1939 - mae: 3.0705\n",
      "Dataset: yacht\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "721/721 [==============================] - 3s 807us/step - loss: 0.9938 - mae: 2.1069\n",
      "Epoch 2/2\n",
      "721/721 [==============================] - 1s 926us/step - loss: 0.3358 - mae: 0.8628\n",
      "Dataset: wine\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "3746/3746 [==============================] - 3s 811us/step - loss: 0.2198 - mae: 0.6104\n",
      "Epoch 2/2\n",
      "3746/3746 [==============================] - 3s 814us/step - loss: 0.2114 - mae: 0.6026\n",
      "Dataset: crime\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "3746/3746 [==============================] - 4s 891us/step - loss: 0.2083 - mae: 0.5621\n",
      "Epoch 2/2\n",
      "3746/3746 [==============================] - 3s 838us/step - loss: 0.1951 - mae: 0.5406\n",
      "Dataset: auto\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "3746/3746 [==============================] - 4s 870us/step - loss: 0.3519 - mae: 0.8853\n",
      "Epoch 2/2\n",
      "3746/3746 [==============================] - 3s 865us/step - loss: 0.2449 - mae: 0.6821\n",
      "Dataset: cpu\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "487/487 [==============================] - 1s 857us/step - loss: 14.7360 - mae: 29.6498\n",
      "Epoch 2/2\n",
      "487/487 [==============================] - 0s 789us/step - loss: 13.1945 - mae: 26.9980\n",
      "Dataset: bank\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "487/487 [==============================] - 1s 820us/step - loss: 16.5224 - mae: 32.6230\n",
      "Epoch 2/2\n",
      "487/487 [==============================] - 0s 787us/step - loss: 14.0282 - mae: 28.4886\n",
      "Dataset: wisconsin\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "453/453 [==============================] - 1s 883us/step - loss: 1.4843 - mae: 3.2198\n",
      "Epoch 2/2\n",
      "453/453 [==============================] - 0s 813us/step - loss: 1.3528 - mae: 3.2925\n"
     ]
    }
   ],
   "source": [
    "from src.linreg import BayesianLinearRegressor\n",
    "from src.calibrated1 import CalibratedRegressor\n",
    "from src.calibrated2 import DistributionCalibratedRegressor\n",
    "l_out = []\n",
    "metric_names = ['rmse', 'r2', 'mape', 'mae']\n",
    "dataset_names = datasets.keys()\n",
    "# dataset_names.remove('wisconsin')\n",
    "# dataset_names.remove('yacht')\n",
    "alphas=np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "for dataset in dataset_names:\n",
    "    print('Dataset: %s' % dataset)\n",
    "    (X_tr, y_tr, X_ts, y_ts) = datasets[dataset]\n",
    "    \n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            clf = BayesianLinearRegressor()\n",
    "            clf.fit(X_tr, y_tr)\n",
    "            y_ts_pred = clf.predict(X_ts)['y_pred']\n",
    "            l_out0 = [dataset] + [metrics[m](y_ts, y_ts_pred) for m in metric_names]\n",
    "            current_model = clf\n",
    "        elif i == 1:\n",
    "            print('Recal 1')\n",
    "            current_model = CalibratedRegressor(clf)\n",
    "            current_model.fit(X_tr, y_tr)\n",
    "            y_ts_pred = current_model.predict(pd.DataFrame(X_ts))['y_pred']\n",
    "            l_out0 += [metrics[m](y_ts, y_ts_pred) for m in metric_names]\n",
    "        elif i == 2:\n",
    "            print('Recal 2')    \n",
    "            current_model = DistributionCalibratedQuantileRegressor(clf)\n",
    "            current_model.fit(X_tr, y_tr)\n",
    "            y_ts_pred = current_model.predict(X_ts).flatten()\n",
    "            l_out0 += [metrics[m](y_ts, y_ts_pred) for m in metric_names]\n",
    "    \n",
    "        p_exp = np.arange(0,1,0.05)\n",
    "        p_obs = prep_calibration(current_model, X_ts, y_ts, p_exp)\n",
    "        cal_err = mean_calibration_error(p_exp, p_obs)\n",
    "\n",
    "        q_pred=np.empty([y_ts.shape[0], alphas.shape[0]])\n",
    "        for i, alpha in enumerate(alphas):\n",
    "            q_pred[:,i] = current_model.predict_quantile(X_ts, alpha).to_numpy().flatten()\n",
    "        pbl_err = pinball_loss(y_ts, q_pred, alphas)\n",
    "\n",
    "        l_out0 += [cal_err, pbl_err]\n",
    "    l_out += [l_out0]\n",
    "    \n",
    "columns = ['dataset'] + [name + str(i) for i in range(3) for name in metric_names + ['cal', 'pbl']]\n",
    "df_lin = pd.DataFrame(l_out, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>rmse0</th>\n",
       "      <th>r20</th>\n",
       "      <th>mape0</th>\n",
       "      <th>mae0</th>\n",
       "      <th>cal0</th>\n",
       "      <th>pbl0</th>\n",
       "      <th>rmse1</th>\n",
       "      <th>r21</th>\n",
       "      <th>mape1</th>\n",
       "      <th>mae1</th>\n",
       "      <th>cal1</th>\n",
       "      <th>pbl1</th>\n",
       "      <th>rmse2</th>\n",
       "      <th>r22</th>\n",
       "      <th>mape2</th>\n",
       "      <th>mae2</th>\n",
       "      <th>cal2</th>\n",
       "      <th>pbl2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mpg</td>\n",
       "      <td>0.450487</td>\n",
       "      <td>0.842001</td>\n",
       "      <td>0.113878</td>\n",
       "      <td>2.455689</td>\n",
       "      <td>0.058150</td>\n",
       "      <td>0.921462</td>\n",
       "      <td>0.495173</td>\n",
       "      <td>0.841226</td>\n",
       "      <td>0.114325</td>\n",
       "      <td>2.464739</td>\n",
       "      <td>0.057111</td>\n",
       "      <td>0.915590</td>\n",
       "      <td>1.486417</td>\n",
       "      <td>0.829293</td>\n",
       "      <td>0.115312</td>\n",
       "      <td>2.522474</td>\n",
       "      <td>0.242171</td>\n",
       "      <td>1.019060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boston</td>\n",
       "      <td>0.146133</td>\n",
       "      <td>0.750254</td>\n",
       "      <td>0.170834</td>\n",
       "      <td>3.459052</td>\n",
       "      <td>0.066061</td>\n",
       "      <td>1.392054</td>\n",
       "      <td>0.357935</td>\n",
       "      <td>0.748852</td>\n",
       "      <td>0.165103</td>\n",
       "      <td>3.399369</td>\n",
       "      <td>0.029571</td>\n",
       "      <td>1.364643</td>\n",
       "      <td>1.505169</td>\n",
       "      <td>0.794497</td>\n",
       "      <td>0.169434</td>\n",
       "      <td>3.349344</td>\n",
       "      <td>0.185549</td>\n",
       "      <td>1.361577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yacht</td>\n",
       "      <td>0.228579</td>\n",
       "      <td>0.697256</td>\n",
       "      <td>3.799502</td>\n",
       "      <td>6.171114</td>\n",
       "      <td>0.079251</td>\n",
       "      <td>2.438080</td>\n",
       "      <td>1.841331</td>\n",
       "      <td>0.681586</td>\n",
       "      <td>4.801745</td>\n",
       "      <td>5.963778</td>\n",
       "      <td>0.062512</td>\n",
       "      <td>2.378720</td>\n",
       "      <td>0.191856</td>\n",
       "      <td>0.990460</td>\n",
       "      <td>0.336303</td>\n",
       "      <td>0.908247</td>\n",
       "      <td>0.093393</td>\n",
       "      <td>0.351628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wine</td>\n",
       "      <td>0.048067</td>\n",
       "      <td>0.143069</td>\n",
       "      <td>0.105653</td>\n",
       "      <td>0.627606</td>\n",
       "      <td>0.037241</td>\n",
       "      <td>0.240240</td>\n",
       "      <td>0.040553</td>\n",
       "      <td>0.144005</td>\n",
       "      <td>0.105478</td>\n",
       "      <td>0.627217</td>\n",
       "      <td>0.026511</td>\n",
       "      <td>0.239304</td>\n",
       "      <td>0.113267</td>\n",
       "      <td>0.125897</td>\n",
       "      <td>0.106314</td>\n",
       "      <td>0.636713</td>\n",
       "      <td>0.047556</td>\n",
       "      <td>0.241340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crime</td>\n",
       "      <td>0.036404</td>\n",
       "      <td>0.283343</td>\n",
       "      <td>0.087373</td>\n",
       "      <td>0.515993</td>\n",
       "      <td>0.030842</td>\n",
       "      <td>0.202428</td>\n",
       "      <td>0.012409</td>\n",
       "      <td>0.285247</td>\n",
       "      <td>0.086286</td>\n",
       "      <td>0.513618</td>\n",
       "      <td>0.018464</td>\n",
       "      <td>0.201750</td>\n",
       "      <td>0.045399</td>\n",
       "      <td>0.280358</td>\n",
       "      <td>0.086312</td>\n",
       "      <td>0.516507</td>\n",
       "      <td>0.047272</td>\n",
       "      <td>0.203950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>auto</td>\n",
       "      <td>0.073609</td>\n",
       "      <td>0.317768</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.635482</td>\n",
       "      <td>0.042643</td>\n",
       "      <td>0.251485</td>\n",
       "      <td>0.013924</td>\n",
       "      <td>0.322838</td>\n",
       "      <td>0.056884</td>\n",
       "      <td>0.628751</td>\n",
       "      <td>0.031647</td>\n",
       "      <td>0.250364</td>\n",
       "      <td>0.120291</td>\n",
       "      <td>0.298099</td>\n",
       "      <td>0.059504</td>\n",
       "      <td>0.635803</td>\n",
       "      <td>0.064585</td>\n",
       "      <td>0.256677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cpu</td>\n",
       "      <td>11.802386</td>\n",
       "      <td>0.895540</td>\n",
       "      <td>0.647345</td>\n",
       "      <td>39.896125</td>\n",
       "      <td>0.083220</td>\n",
       "      <td>15.871333</td>\n",
       "      <td>6.605865</td>\n",
       "      <td>0.898922</td>\n",
       "      <td>0.636421</td>\n",
       "      <td>39.086244</td>\n",
       "      <td>0.074292</td>\n",
       "      <td>15.463072</td>\n",
       "      <td>15.957352</td>\n",
       "      <td>0.933759</td>\n",
       "      <td>0.324434</td>\n",
       "      <td>28.159797</td>\n",
       "      <td>0.252678</td>\n",
       "      <td>13.615109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bank</td>\n",
       "      <td>7.854822</td>\n",
       "      <td>0.897250</td>\n",
       "      <td>0.589829</td>\n",
       "      <td>39.507993</td>\n",
       "      <td>0.114589</td>\n",
       "      <td>17.478473</td>\n",
       "      <td>6.126668</td>\n",
       "      <td>0.897812</td>\n",
       "      <td>0.568345</td>\n",
       "      <td>39.147699</td>\n",
       "      <td>0.068910</td>\n",
       "      <td>16.638647</td>\n",
       "      <td>2.482587</td>\n",
       "      <td>0.942842</td>\n",
       "      <td>0.386924</td>\n",
       "      <td>29.314110</td>\n",
       "      <td>0.258207</td>\n",
       "      <td>14.274109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wisconsin</td>\n",
       "      <td>0.173360</td>\n",
       "      <td>-0.001090</td>\n",
       "      <td>1.231798</td>\n",
       "      <td>3.722096</td>\n",
       "      <td>0.170843</td>\n",
       "      <td>1.484555</td>\n",
       "      <td>2.083975</td>\n",
       "      <td>-0.149321</td>\n",
       "      <td>0.452650</td>\n",
       "      <td>2.858201</td>\n",
       "      <td>0.035722</td>\n",
       "      <td>1.246294</td>\n",
       "      <td>1.563150</td>\n",
       "      <td>-0.084004</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>2.994933</td>\n",
       "      <td>0.103929</td>\n",
       "      <td>1.285391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset      rmse0       r20     mape0       mae0      cal0       pbl0  \\\n",
       "0        mpg   0.450487  0.842001  0.113878   2.455689  0.058150   0.921462   \n",
       "1     boston   0.146133  0.750254  0.170834   3.459052  0.066061   1.392054   \n",
       "2      yacht   0.228579  0.697256  3.799502   6.171114  0.079251   2.438080   \n",
       "3       wine   0.048067  0.143069  0.105653   0.627606  0.037241   0.240240   \n",
       "4      crime   0.036404  0.283343  0.087373   0.515993  0.030842   0.202428   \n",
       "5       auto   0.073609  0.317768  0.057800   0.635482  0.042643   0.251485   \n",
       "6        cpu  11.802386  0.895540  0.647345  39.896125  0.083220  15.871333   \n",
       "7       bank   7.854822  0.897250  0.589829  39.507993  0.114589  17.478473   \n",
       "8  wisconsin   0.173360 -0.001090  1.231798   3.722096  0.170843   1.484555   \n",
       "\n",
       "      rmse1       r21     mape1       mae1      cal1       pbl1      rmse2  \\\n",
       "0  0.495173  0.841226  0.114325   2.464739  0.057111   0.915590   1.486417   \n",
       "1  0.357935  0.748852  0.165103   3.399369  0.029571   1.364643   1.505169   \n",
       "2  1.841331  0.681586  4.801745   5.963778  0.062512   2.378720   0.191856   \n",
       "3  0.040553  0.144005  0.105478   0.627217  0.026511   0.239304   0.113267   \n",
       "4  0.012409  0.285247  0.086286   0.513618  0.018464   0.201750   0.045399   \n",
       "5  0.013924  0.322838  0.056884   0.628751  0.031647   0.250364   0.120291   \n",
       "6  6.605865  0.898922  0.636421  39.086244  0.074292  15.463072  15.957352   \n",
       "7  6.126668  0.897812  0.568345  39.147699  0.068910  16.638647   2.482587   \n",
       "8  2.083975 -0.149321  0.452650   2.858201  0.035722   1.246294   1.563150   \n",
       "\n",
       "        r22     mape2       mae2      cal2       pbl2  \n",
       "0  0.829293  0.115312   2.522474  0.242171   1.019060  \n",
       "1  0.794497  0.169434   3.349344  0.185549   1.361577  \n",
       "2  0.990460  0.336303   0.908247  0.093393   0.351628  \n",
       "3  0.125897  0.106314   0.636713  0.047556   0.241340  \n",
       "4  0.280358  0.086312   0.516507  0.047272   0.203950  \n",
       "5  0.298099  0.059504   0.635803  0.064585   0.256677  \n",
       "6  0.933759  0.324434  28.159797  0.252678  13.615109  \n",
       "7  0.942842  0.386924  29.314110  0.258207  14.274109  \n",
       "8 -0.084004  0.581281   2.994933  0.103929   1.285391  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>rmse0</th>\n",
       "      <th>rmse1</th>\n",
       "      <th>rmse2</th>\n",
       "      <th>r20</th>\n",
       "      <th>r21</th>\n",
       "      <th>r22</th>\n",
       "      <th>mape0</th>\n",
       "      <th>mape1</th>\n",
       "      <th>mape2</th>\n",
       "      <th>mae0</th>\n",
       "      <th>mae1</th>\n",
       "      <th>mae2</th>\n",
       "      <th>cal0</th>\n",
       "      <th>cal1</th>\n",
       "      <th>cal2</th>\n",
       "      <th>pbl0</th>\n",
       "      <th>pbl1</th>\n",
       "      <th>pbl2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mpg</td>\n",
       "      <td>0.450487</td>\n",
       "      <td>0.495173</td>\n",
       "      <td>1.486417</td>\n",
       "      <td>0.842001</td>\n",
       "      <td>0.841226</td>\n",
       "      <td>0.829293</td>\n",
       "      <td>0.113878</td>\n",
       "      <td>0.114325</td>\n",
       "      <td>0.115312</td>\n",
       "      <td>2.455689</td>\n",
       "      <td>2.464739</td>\n",
       "      <td>2.522474</td>\n",
       "      <td>0.058150</td>\n",
       "      <td>0.057111</td>\n",
       "      <td>0.242171</td>\n",
       "      <td>0.921462</td>\n",
       "      <td>0.915590</td>\n",
       "      <td>1.019060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boston</td>\n",
       "      <td>0.146133</td>\n",
       "      <td>0.357935</td>\n",
       "      <td>1.505169</td>\n",
       "      <td>0.750254</td>\n",
       "      <td>0.748852</td>\n",
       "      <td>0.794497</td>\n",
       "      <td>0.170834</td>\n",
       "      <td>0.165103</td>\n",
       "      <td>0.169434</td>\n",
       "      <td>3.459052</td>\n",
       "      <td>3.399369</td>\n",
       "      <td>3.349344</td>\n",
       "      <td>0.066061</td>\n",
       "      <td>0.029571</td>\n",
       "      <td>0.185549</td>\n",
       "      <td>1.392054</td>\n",
       "      <td>1.364643</td>\n",
       "      <td>1.361577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yacht</td>\n",
       "      <td>0.228579</td>\n",
       "      <td>1.841331</td>\n",
       "      <td>0.191856</td>\n",
       "      <td>0.697256</td>\n",
       "      <td>0.681586</td>\n",
       "      <td>0.990460</td>\n",
       "      <td>3.799502</td>\n",
       "      <td>4.801745</td>\n",
       "      <td>0.336303</td>\n",
       "      <td>6.171114</td>\n",
       "      <td>5.963778</td>\n",
       "      <td>0.908247</td>\n",
       "      <td>0.079251</td>\n",
       "      <td>0.062512</td>\n",
       "      <td>0.093393</td>\n",
       "      <td>2.438080</td>\n",
       "      <td>2.378720</td>\n",
       "      <td>0.351628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wine</td>\n",
       "      <td>0.048067</td>\n",
       "      <td>0.040553</td>\n",
       "      <td>0.113267</td>\n",
       "      <td>0.143069</td>\n",
       "      <td>0.144005</td>\n",
       "      <td>0.125897</td>\n",
       "      <td>0.105653</td>\n",
       "      <td>0.105478</td>\n",
       "      <td>0.106314</td>\n",
       "      <td>0.627606</td>\n",
       "      <td>0.627217</td>\n",
       "      <td>0.636713</td>\n",
       "      <td>0.037241</td>\n",
       "      <td>0.026511</td>\n",
       "      <td>0.047556</td>\n",
       "      <td>0.240240</td>\n",
       "      <td>0.239304</td>\n",
       "      <td>0.241340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crime</td>\n",
       "      <td>0.036404</td>\n",
       "      <td>0.012409</td>\n",
       "      <td>0.045399</td>\n",
       "      <td>0.283343</td>\n",
       "      <td>0.285247</td>\n",
       "      <td>0.280358</td>\n",
       "      <td>0.087373</td>\n",
       "      <td>0.086286</td>\n",
       "      <td>0.086312</td>\n",
       "      <td>0.515993</td>\n",
       "      <td>0.513618</td>\n",
       "      <td>0.516507</td>\n",
       "      <td>0.030842</td>\n",
       "      <td>0.018464</td>\n",
       "      <td>0.047272</td>\n",
       "      <td>0.202428</td>\n",
       "      <td>0.201750</td>\n",
       "      <td>0.203950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>auto</td>\n",
       "      <td>0.073609</td>\n",
       "      <td>0.013924</td>\n",
       "      <td>0.120291</td>\n",
       "      <td>0.317768</td>\n",
       "      <td>0.322838</td>\n",
       "      <td>0.298099</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.056884</td>\n",
       "      <td>0.059504</td>\n",
       "      <td>0.635482</td>\n",
       "      <td>0.628751</td>\n",
       "      <td>0.635803</td>\n",
       "      <td>0.042643</td>\n",
       "      <td>0.031647</td>\n",
       "      <td>0.064585</td>\n",
       "      <td>0.251485</td>\n",
       "      <td>0.250364</td>\n",
       "      <td>0.256677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cpu</td>\n",
       "      <td>11.802386</td>\n",
       "      <td>6.605865</td>\n",
       "      <td>15.957352</td>\n",
       "      <td>0.895540</td>\n",
       "      <td>0.898922</td>\n",
       "      <td>0.933759</td>\n",
       "      <td>0.647345</td>\n",
       "      <td>0.636421</td>\n",
       "      <td>0.324434</td>\n",
       "      <td>39.896125</td>\n",
       "      <td>39.086244</td>\n",
       "      <td>28.159797</td>\n",
       "      <td>0.083220</td>\n",
       "      <td>0.074292</td>\n",
       "      <td>0.252678</td>\n",
       "      <td>15.871333</td>\n",
       "      <td>15.463072</td>\n",
       "      <td>13.615109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bank</td>\n",
       "      <td>7.854822</td>\n",
       "      <td>6.126668</td>\n",
       "      <td>2.482587</td>\n",
       "      <td>0.897250</td>\n",
       "      <td>0.897812</td>\n",
       "      <td>0.942842</td>\n",
       "      <td>0.589829</td>\n",
       "      <td>0.568345</td>\n",
       "      <td>0.386924</td>\n",
       "      <td>39.507993</td>\n",
       "      <td>39.147699</td>\n",
       "      <td>29.314110</td>\n",
       "      <td>0.114589</td>\n",
       "      <td>0.068910</td>\n",
       "      <td>0.258207</td>\n",
       "      <td>17.478473</td>\n",
       "      <td>16.638647</td>\n",
       "      <td>14.274109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wisconsin</td>\n",
       "      <td>0.173360</td>\n",
       "      <td>2.083975</td>\n",
       "      <td>1.563150</td>\n",
       "      <td>-0.001090</td>\n",
       "      <td>-0.149321</td>\n",
       "      <td>-0.084004</td>\n",
       "      <td>1.231798</td>\n",
       "      <td>0.452650</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>3.722096</td>\n",
       "      <td>2.858201</td>\n",
       "      <td>2.994933</td>\n",
       "      <td>0.170843</td>\n",
       "      <td>0.035722</td>\n",
       "      <td>0.103929</td>\n",
       "      <td>1.484555</td>\n",
       "      <td>1.246294</td>\n",
       "      <td>1.285391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset      rmse0     rmse1      rmse2       r20       r21       r22  \\\n",
       "0        mpg   0.450487  0.495173   1.486417  0.842001  0.841226  0.829293   \n",
       "1     boston   0.146133  0.357935   1.505169  0.750254  0.748852  0.794497   \n",
       "2      yacht   0.228579  1.841331   0.191856  0.697256  0.681586  0.990460   \n",
       "3       wine   0.048067  0.040553   0.113267  0.143069  0.144005  0.125897   \n",
       "4      crime   0.036404  0.012409   0.045399  0.283343  0.285247  0.280358   \n",
       "5       auto   0.073609  0.013924   0.120291  0.317768  0.322838  0.298099   \n",
       "6        cpu  11.802386  6.605865  15.957352  0.895540  0.898922  0.933759   \n",
       "7       bank   7.854822  6.126668   2.482587  0.897250  0.897812  0.942842   \n",
       "8  wisconsin   0.173360  2.083975   1.563150 -0.001090 -0.149321 -0.084004   \n",
       "\n",
       "      mape0     mape1     mape2       mae0       mae1       mae2      cal0  \\\n",
       "0  0.113878  0.114325  0.115312   2.455689   2.464739   2.522474  0.058150   \n",
       "1  0.170834  0.165103  0.169434   3.459052   3.399369   3.349344  0.066061   \n",
       "2  3.799502  4.801745  0.336303   6.171114   5.963778   0.908247  0.079251   \n",
       "3  0.105653  0.105478  0.106314   0.627606   0.627217   0.636713  0.037241   \n",
       "4  0.087373  0.086286  0.086312   0.515993   0.513618   0.516507  0.030842   \n",
       "5  0.057800  0.056884  0.059504   0.635482   0.628751   0.635803  0.042643   \n",
       "6  0.647345  0.636421  0.324434  39.896125  39.086244  28.159797  0.083220   \n",
       "7  0.589829  0.568345  0.386924  39.507993  39.147699  29.314110  0.114589   \n",
       "8  1.231798  0.452650  0.581281   3.722096   2.858201   2.994933  0.170843   \n",
       "\n",
       "       cal1      cal2       pbl0       pbl1       pbl2  \n",
       "0  0.057111  0.242171   0.921462   0.915590   1.019060  \n",
       "1  0.029571  0.185549   1.392054   1.364643   1.361577  \n",
       "2  0.062512  0.093393   2.438080   2.378720   0.351628  \n",
       "3  0.026511  0.047556   0.240240   0.239304   0.241340  \n",
       "4  0.018464  0.047272   0.202428   0.201750   0.203950  \n",
       "5  0.031647  0.064585   0.251485   0.250364   0.256677  \n",
       "6  0.074292  0.252678  15.871333  15.463072  13.615109  \n",
       "7  0.068910  0.258207  17.478473  16.638647  14.274109  \n",
       "8  0.035722  0.103929   1.484555   1.246294   1.285391  "
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_columns = ['dataset'] + [name + str(i) for name in metric_names + ['cal', 'pbl'] for i in range(3)]\n",
    "df_lin.loc[:,ordered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: mpg\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "918/918 [==============================] - 1s 864us/step - loss: 1.8375 - mae: 3.8294\n",
      "Epoch 2/2\n",
      "918/918 [==============================] - 1s 776us/step - loss: 1.1430 - mae: 2.7622\n",
      "Dataset: boston\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "1184/1184 [==============================] - 1s 858us/step - loss: 1.6802 - mae: 3.6195\n",
      "Epoch 2/2\n",
      "1184/1184 [==============================] - 1s 749us/step - loss: 1.2565 - mae: 3.1324\n",
      "Dataset: yacht\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "721/721 [==============================] - 1s 720us/step - loss: 2.0073 - mae: 4.0370\n",
      "Epoch 2/2\n",
      "721/721 [==============================] - 1s 731us/step - loss: 1.6576 - mae: 3.4583\n",
      "Dataset: wine\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "3746/3746 [==============================] - 3s 820us/step - loss: 0.2057 - mae: 0.5769\n",
      "Epoch 2/2\n",
      "3746/3746 [==============================] - 3s 794us/step - loss: 0.2000 - mae: 0.5706\n",
      "Dataset: crime\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "3746/3746 [==============================] - 3s 727us/step - loss: 0.1901 - mae: 0.5120\n",
      "Epoch 2/2\n",
      "3746/3746 [==============================] - 3s 707us/step - loss: 0.1790 - mae: 0.4937\n",
      "Dataset: auto\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "3746/3746 [==============================] - 3s 811us/step - loss: 0.2428 - mae: 0.6522\n",
      "Epoch 2/2\n",
      "3746/3746 [==============================] - 3s 807us/step - loss: 0.2292 - mae: 0.6391\n",
      "Dataset: cpu\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "487/487 [==============================] - 1s 848us/step - loss: 33.5413 - mae: 68.0000\n",
      "Epoch 2/2\n",
      "487/487 [==============================] - 0s 808us/step - loss: 29.9270 - mae: 60.5539\n",
      "Dataset: bank\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "487/487 [==============================] - 1s 856us/step - loss: 29.8698 - mae: 59.7312\n",
      "Epoch 2/2\n",
      "487/487 [==============================] - 0s 809us/step - loss: 26.7524 - mae: 55.1105\n",
      "Dataset: wisconsin\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "453/453 [==============================] - 1s 877us/step - loss: 1.4395 - mae: 3.0649\n",
      "Epoch 2/2\n",
      "453/453 [==============================] - 0s 835us/step - loss: 1.3088 - mae: 3.0543\n"
     ]
    }
   ],
   "source": [
    "from src.dnn import BayesianDNNForecaster\n",
    "from src.calibrated1 import CalibratedRegressor\n",
    "from src.calibrated2 import DistributionCalibratedRegressor\n",
    "l_out = []\n",
    "metric_names = ['mae','rmse', 'r2', 'mape']\n",
    "dataset_names = datasets.keys()\n",
    "# dataset_names.remove('wisconsin')\n",
    "# dataset_names.remove('yacht')\n",
    "alphas=np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "for dataset in dataset_names:\n",
    "    print('Dataset: %s' % dataset)\n",
    "    (X_tr, y_tr, X_ts, y_ts) = datasets[dataset]\n",
    "    \n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            clf = BayesianDNNForecaster()\n",
    "            clf.fit(X_tr, y_tr)\n",
    "            y_ts_pred = clf.predict(X_ts)['y_pred']\n",
    "            l_out0 = [dataset] + [metrics[m](y_ts, y_ts_pred) for m in metric_names]\n",
    "            current_model = clf\n",
    "        elif i == 1:\n",
    "            print('Recal 1')\n",
    "            current_model = CalibratedRegressor(clf)\n",
    "            current_model.fit(X_tr, y_tr)\n",
    "            y_ts_pred = current_model.predict(pd.DataFrame(X_ts))['y_pred']\n",
    "            l_out0 += [metrics[m](y_ts, y_ts_pred) for m in metric_names]\n",
    "        elif i == 2:\n",
    "            print('Recal 2')    \n",
    "            current_model = DistributionCalibratedQuantileRegressor(clf)\n",
    "            current_model.fit(X_tr, y_tr)\n",
    "            y_ts_pred = current_model.predict(X_ts).flatten()\n",
    "            l_out0 += [metrics[m](y_ts, y_ts_pred) for m in metric_names]\n",
    "    \n",
    "        p_exp = np.arange(0,1,0.05)\n",
    "        p_obs = prep_calibration(current_model, X_ts, y_ts, p_exp)\n",
    "        cal_err = mean_calibration_error(p_exp, p_obs)\n",
    "\n",
    "        q_pred=np.empty([y_ts.shape[0], alphas.shape[0]])\n",
    "        for i, alpha in enumerate(alphas):\n",
    "            q_pred[:,i] = current_model.predict_quantile(X_ts, alpha).to_numpy().flatten()\n",
    "        pbl_err = pinball_loss(y_ts, q_pred, alphas)\n",
    "\n",
    "        l_out0 += [cal_err, pbl_err]\n",
    "    l_out += [l_out0]\n",
    "    \n",
    "columns = ['dataset'] + [name + str(i) for i in range(3) for name in metric_names + ['cal', 'pbl']]\n",
    "df_bayesian = pd.DataFrame(l_out, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>rmse0</th>\n",
       "      <th>rmse1</th>\n",
       "      <th>rmse2</th>\n",
       "      <th>r20</th>\n",
       "      <th>r21</th>\n",
       "      <th>r22</th>\n",
       "      <th>mape0</th>\n",
       "      <th>mape1</th>\n",
       "      <th>mape2</th>\n",
       "      <th>cal0</th>\n",
       "      <th>cal1</th>\n",
       "      <th>cal2</th>\n",
       "      <th>pbl0</th>\n",
       "      <th>pbl1</th>\n",
       "      <th>pbl2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mpg</td>\n",
       "      <td>0.101745</td>\n",
       "      <td>0.374772</td>\n",
       "      <td>0.765440</td>\n",
       "      <td>0.752171</td>\n",
       "      <td>0.767287</td>\n",
       "      <td>0.814811</td>\n",
       "      <td>0.123362</td>\n",
       "      <td>0.118249</td>\n",
       "      <td>0.112115</td>\n",
       "      <td>0.053389</td>\n",
       "      <td>0.030908</td>\n",
       "      <td>0.130347</td>\n",
       "      <td>1.102795</td>\n",
       "      <td>1.085243</td>\n",
       "      <td>0.996282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boston</td>\n",
       "      <td>0.770043</td>\n",
       "      <td>0.236568</td>\n",
       "      <td>1.161702</td>\n",
       "      <td>0.814857</td>\n",
       "      <td>0.807696</td>\n",
       "      <td>0.792002</td>\n",
       "      <td>0.152981</td>\n",
       "      <td>0.150017</td>\n",
       "      <td>0.167919</td>\n",
       "      <td>0.086470</td>\n",
       "      <td>0.070936</td>\n",
       "      <td>0.185014</td>\n",
       "      <td>1.248788</td>\n",
       "      <td>1.222206</td>\n",
       "      <td>1.328433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yacht</td>\n",
       "      <td>0.441172</td>\n",
       "      <td>1.818137</td>\n",
       "      <td>0.349457</td>\n",
       "      <td>0.745802</td>\n",
       "      <td>0.718463</td>\n",
       "      <td>0.813779</td>\n",
       "      <td>0.818732</td>\n",
       "      <td>0.953881</td>\n",
       "      <td>0.418976</td>\n",
       "      <td>0.103117</td>\n",
       "      <td>0.054998</td>\n",
       "      <td>0.088986</td>\n",
       "      <td>1.762397</td>\n",
       "      <td>1.701047</td>\n",
       "      <td>1.256813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wine</td>\n",
       "      <td>0.027204</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>0.218302</td>\n",
       "      <td>0.127929</td>\n",
       "      <td>0.126497</td>\n",
       "      <td>0.093571</td>\n",
       "      <td>0.105941</td>\n",
       "      <td>0.106188</td>\n",
       "      <td>0.104647</td>\n",
       "      <td>0.102485</td>\n",
       "      <td>0.031721</td>\n",
       "      <td>0.099573</td>\n",
       "      <td>0.249787</td>\n",
       "      <td>0.239068</td>\n",
       "      <td>0.246657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crime</td>\n",
       "      <td>0.045979</td>\n",
       "      <td>0.018046</td>\n",
       "      <td>0.116238</td>\n",
       "      <td>0.347111</td>\n",
       "      <td>0.345236</td>\n",
       "      <td>0.335582</td>\n",
       "      <td>0.082763</td>\n",
       "      <td>0.083644</td>\n",
       "      <td>0.087648</td>\n",
       "      <td>0.057720</td>\n",
       "      <td>0.018498</td>\n",
       "      <td>0.067326</td>\n",
       "      <td>0.193667</td>\n",
       "      <td>0.194426</td>\n",
       "      <td>0.194243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>auto</td>\n",
       "      <td>0.033105</td>\n",
       "      <td>0.024611</td>\n",
       "      <td>0.005830</td>\n",
       "      <td>0.338363</td>\n",
       "      <td>0.366093</td>\n",
       "      <td>0.360147</td>\n",
       "      <td>0.059168</td>\n",
       "      <td>0.056606</td>\n",
       "      <td>0.057393</td>\n",
       "      <td>0.042529</td>\n",
       "      <td>0.034718</td>\n",
       "      <td>0.046603</td>\n",
       "      <td>0.252281</td>\n",
       "      <td>0.250943</td>\n",
       "      <td>0.243464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cpu</td>\n",
       "      <td>65.730276</td>\n",
       "      <td>48.684415</td>\n",
       "      <td>16.996551</td>\n",
       "      <td>0.179376</td>\n",
       "      <td>0.323836</td>\n",
       "      <td>0.761710</td>\n",
       "      <td>0.537719</td>\n",
       "      <td>0.644296</td>\n",
       "      <td>0.666911</td>\n",
       "      <td>0.274646</td>\n",
       "      <td>0.062901</td>\n",
       "      <td>0.282889</td>\n",
       "      <td>35.342486</td>\n",
       "      <td>27.358981</td>\n",
       "      <td>25.522440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bank</td>\n",
       "      <td>79.826528</td>\n",
       "      <td>59.492879</td>\n",
       "      <td>31.837754</td>\n",
       "      <td>0.138028</td>\n",
       "      <td>0.278784</td>\n",
       "      <td>0.434274</td>\n",
       "      <td>0.676827</td>\n",
       "      <td>0.956758</td>\n",
       "      <td>1.101968</td>\n",
       "      <td>0.241424</td>\n",
       "      <td>0.072042</td>\n",
       "      <td>0.283555</td>\n",
       "      <td>43.961693</td>\n",
       "      <td>34.354989</td>\n",
       "      <td>34.950066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wisconsin</td>\n",
       "      <td>0.037515</td>\n",
       "      <td>1.889676</td>\n",
       "      <td>0.876523</td>\n",
       "      <td>-0.033453</td>\n",
       "      <td>-0.104536</td>\n",
       "      <td>0.028409</td>\n",
       "      <td>1.032966</td>\n",
       "      <td>0.652571</td>\n",
       "      <td>0.767421</td>\n",
       "      <td>0.270632</td>\n",
       "      <td>0.077789</td>\n",
       "      <td>0.126816</td>\n",
       "      <td>1.656004</td>\n",
       "      <td>1.354894</td>\n",
       "      <td>1.437394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset      rmse0      rmse1      rmse2       r20       r21       r22  \\\n",
       "0        mpg   0.101745   0.374772   0.765440  0.752171  0.767287  0.814811   \n",
       "1     boston   0.770043   0.236568   1.161702  0.814857  0.807696  0.792002   \n",
       "2      yacht   0.441172   1.818137   0.349457  0.745802  0.718463  0.813779   \n",
       "3       wine   0.027204   0.003530   0.218302  0.127929  0.126497  0.093571   \n",
       "4      crime   0.045979   0.018046   0.116238  0.347111  0.345236  0.335582   \n",
       "5       auto   0.033105   0.024611   0.005830  0.338363  0.366093  0.360147   \n",
       "6        cpu  65.730276  48.684415  16.996551  0.179376  0.323836  0.761710   \n",
       "7       bank  79.826528  59.492879  31.837754  0.138028  0.278784  0.434274   \n",
       "8  wisconsin   0.037515   1.889676   0.876523 -0.033453 -0.104536  0.028409   \n",
       "\n",
       "      mape0     mape1     mape2      cal0      cal1      cal2       pbl0  \\\n",
       "0  0.123362  0.118249  0.112115  0.053389  0.030908  0.130347   1.102795   \n",
       "1  0.152981  0.150017  0.167919  0.086470  0.070936  0.185014   1.248788   \n",
       "2  0.818732  0.953881  0.418976  0.103117  0.054998  0.088986   1.762397   \n",
       "3  0.105941  0.106188  0.104647  0.102485  0.031721  0.099573   0.249787   \n",
       "4  0.082763  0.083644  0.087648  0.057720  0.018498  0.067326   0.193667   \n",
       "5  0.059168  0.056606  0.057393  0.042529  0.034718  0.046603   0.252281   \n",
       "6  0.537719  0.644296  0.666911  0.274646  0.062901  0.282889  35.342486   \n",
       "7  0.676827  0.956758  1.101968  0.241424  0.072042  0.283555  43.961693   \n",
       "8  1.032966  0.652571  0.767421  0.270632  0.077789  0.126816   1.656004   \n",
       "\n",
       "        pbl1       pbl2  \n",
       "0   1.085243   0.996282  \n",
       "1   1.222206   1.328433  \n",
       "2   1.701047   1.256813  \n",
       "3   0.239068   0.246657  \n",
       "4   0.194426   0.194243  \n",
       "5   0.250943   0.243464  \n",
       "6  27.358981  25.522440  \n",
       "7  34.354989  34.950066  \n",
       "8   1.354894   1.437394  "
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_columns = ['dataset'] + [name + str(i) for name in metric_names + ['cal', 'pbl'] for i in range(3)]\n",
    "df_bayesian.loc[:,ordered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: mpg\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "918/918 [==============================] - 2s 2ms/step - loss: 3.1980 - mae: 6.6957\n",
      "Epoch 2/2\n",
      "918/918 [==============================] - 2s 2ms/step - loss: 2.6911 - mae: 6.6003\n",
      "Dataset: boston\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "1184/1184 [==============================] - 3s 2ms/step - loss: 2.9373 - mae: 6.4353\n",
      "Epoch 2/2\n",
      "1184/1184 [==============================] - 2s 2ms/step - loss: 2.6152 - mae: 6.5393\n",
      "Dataset: yacht\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "721/721 [==============================] - 2s 2ms/step - loss: 4.6479 - mae: 9.6369\n",
      "Epoch 2/2\n",
      "721/721 [==============================] - 1s 2ms/step - loss: 4.2425 - mae: 9.7249\n",
      "Dataset: wine\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "3746/3746 [==============================] - 8s 2ms/step - loss: 0.2434 - mae: 0.6954\n",
      "Epoch 2/2\n",
      "3746/3746 [==============================] - 7s 2ms/step - loss: 0.2305 - mae: 0.6759\n",
      "Dataset: crime\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "3746/3746 [==============================] - 7s 2ms/step - loss: 0.2409 - mae: 0.6885\n",
      "Epoch 2/2\n",
      "3746/3746 [==============================] - 6s 2ms/step - loss: 0.2315 - mae: 0.6918\n",
      "Dataset: auto\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "3746/3746 [==============================] - 7s 2ms/step - loss: 0.3289 - mae: 0.8877\n",
      "Epoch 2/2\n",
      "3746/3746 [==============================] - 6s 2ms/step - loss: 0.2912 - mae: 0.8451\n",
      "Dataset: cpu\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "487/487 [==============================] - 2s 2ms/step - loss: 39.1521 - mae: 77.3722\n",
      "Epoch 2/2\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 37.8327 - mae: 77.0851\n",
      "Dataset: bank\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "487/487 [==============================] - 2s 2ms/step - loss: 34.3466 - mae: 69.4588\n",
      "Epoch 2/2\n",
      "487/487 [==============================] - 1s 2ms/step - loss: 34.7115 - mae: 69.4525\n",
      "Dataset: wisconsin\n",
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/2\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 1.4945 - mae: 3.2430\n",
      "Epoch 2/2\n",
      "453/453 [==============================] - 1s 2ms/step - loss: 1.3457 - mae: 3.3252\n"
     ]
    }
   ],
   "source": [
    "from src.ensemble import EnsembleDNNForecaster\n",
    "from src.calibrated1 import CalibratedRegressor\n",
    "from src.calibrated2 import DistributionCalibratedRegressor\n",
    "l_out = []\n",
    "metric_names = ['rmse', 'r2', 'mape', 'mae']\n",
    "dataset_names = datasets.keys()\n",
    "# dataset_names.remove('wisconsin')\n",
    "# dataset_names.remove('yacht')\n",
    "alphas=np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "for dataset in dataset_names:\n",
    "    print('Dataset: %s' % dataset)\n",
    "    (X_tr, y_tr, X_ts, y_ts) = datasets[dataset]\n",
    "    \n",
    "    for i in range(3):\n",
    "        if i == 0:\n",
    "            clf = EnsembleDNNForecaster()\n",
    "            clf.fit(X_tr, y_tr)\n",
    "            y_ts_pred = clf.predict(X_ts)['y_pred']\n",
    "            l_out0 = [dataset] + [metrics[m](y_ts, y_ts_pred) for m in metric_names]\n",
    "            current_model = clf\n",
    "        elif i == 1:\n",
    "            print('Recal 1')\n",
    "            current_model = CalibratedRegressor(clf)\n",
    "            current_model.fit(X_tr, y_tr)\n",
    "            y_ts_pred = current_model.predict(pd.DataFrame(X_ts))['y_pred']\n",
    "            l_out0 += [metrics[m](y_ts, y_ts_pred) for m in metric_names]\n",
    "        elif i == 2:\n",
    "            print('Recal 2')    \n",
    "            current_model = DistributionCalibratedQuantileRegressor(clf)\n",
    "            current_model.fit(X_tr, y_tr)\n",
    "            y_ts_pred = current_model.predict(X_ts).flatten()\n",
    "            l_out0 += [metrics[m](y_ts, y_ts_pred) for m in metric_names]\n",
    "    \n",
    "        p_exp = np.arange(0,1,0.05)\n",
    "        p_obs = prep_calibration(current_model, X_ts, y_ts, p_exp)\n",
    "        cal_err = mean_calibration_error(p_exp, p_obs)\n",
    "\n",
    "        q_pred=np.empty([y_ts.shape[0], alphas.shape[0]])\n",
    "        for i, alpha in enumerate(alphas):\n",
    "            q_pred[:,i] = current_model.predict_quantile(X_ts, alpha).to_numpy().flatten()\n",
    "        pbl_err = pinball_loss(y_ts, q_pred, alphas)\n",
    "\n",
    "        l_out0 += [cal_err, pbl_err]\n",
    "    l_out += [l_out0]\n",
    "    \n",
    "columns = ['dataset'] + [name + str(i) for i in range(3) for name in metric_names + ['cal', 'pbl']]\n",
    "df_ensemble = pd.DataFrame(l_out, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>rmse0</th>\n",
       "      <th>rmse1</th>\n",
       "      <th>rmse2</th>\n",
       "      <th>r20</th>\n",
       "      <th>r21</th>\n",
       "      <th>r22</th>\n",
       "      <th>mape0</th>\n",
       "      <th>mape1</th>\n",
       "      <th>mape2</th>\n",
       "      <th>mae0</th>\n",
       "      <th>mae1</th>\n",
       "      <th>mae2</th>\n",
       "      <th>cal0</th>\n",
       "      <th>cal1</th>\n",
       "      <th>cal2</th>\n",
       "      <th>pbl0</th>\n",
       "      <th>pbl1</th>\n",
       "      <th>pbl2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mpg</td>\n",
       "      <td>6.908439</td>\n",
       "      <td>11.819784</td>\n",
       "      <td>9.412203</td>\n",
       "      <td>-0.882292</td>\n",
       "      <td>-2.582685</td>\n",
       "      <td>-1.637702</td>\n",
       "      <td>0.288183</td>\n",
       "      <td>0.471108</td>\n",
       "      <td>0.364452</td>\n",
       "      <td>7.667194</td>\n",
       "      <td>11.857822</td>\n",
       "      <td>9.573434</td>\n",
       "      <td>0.333734</td>\n",
       "      <td>0.470061</td>\n",
       "      <td>0.336346</td>\n",
       "      <td>3.555596</td>\n",
       "      <td>5.207469</td>\n",
       "      <td>3.639072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boston</td>\n",
       "      <td>6.553659</td>\n",
       "      <td>12.702200</td>\n",
       "      <td>5.629033</td>\n",
       "      <td>-0.494173</td>\n",
       "      <td>-1.856390</td>\n",
       "      <td>-0.364568</td>\n",
       "      <td>0.327888</td>\n",
       "      <td>0.507731</td>\n",
       "      <td>0.318090</td>\n",
       "      <td>8.427232</td>\n",
       "      <td>12.860030</td>\n",
       "      <td>8.003746</td>\n",
       "      <td>0.293636</td>\n",
       "      <td>0.433876</td>\n",
       "      <td>0.280373</td>\n",
       "      <td>3.820485</td>\n",
       "      <td>5.494944</td>\n",
       "      <td>3.622965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yacht</td>\n",
       "      <td>9.226756</td>\n",
       "      <td>10.836262</td>\n",
       "      <td>4.922452</td>\n",
       "      <td>-0.395747</td>\n",
       "      <td>-0.545856</td>\n",
       "      <td>-0.112637</td>\n",
       "      <td>0.773896</td>\n",
       "      <td>1.683818</td>\n",
       "      <td>3.211313</td>\n",
       "      <td>9.405702</td>\n",
       "      <td>10.836262</td>\n",
       "      <td>9.458529</td>\n",
       "      <td>0.309427</td>\n",
       "      <td>0.422117</td>\n",
       "      <td>0.208501</td>\n",
       "      <td>4.603608</td>\n",
       "      <td>4.932029</td>\n",
       "      <td>4.277691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wine</td>\n",
       "      <td>0.433328</td>\n",
       "      <td>0.053327</td>\n",
       "      <td>0.203310</td>\n",
       "      <td>-0.262182</td>\n",
       "      <td>-0.003971</td>\n",
       "      <td>-0.057715</td>\n",
       "      <td>0.131844</td>\n",
       "      <td>0.122339</td>\n",
       "      <td>0.123365</td>\n",
       "      <td>0.714859</td>\n",
       "      <td>0.700542</td>\n",
       "      <td>0.690043</td>\n",
       "      <td>0.205123</td>\n",
       "      <td>0.138893</td>\n",
       "      <td>0.164480</td>\n",
       "      <td>0.308350</td>\n",
       "      <td>0.276247</td>\n",
       "      <td>0.267734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crime</td>\n",
       "      <td>0.194694</td>\n",
       "      <td>0.693841</td>\n",
       "      <td>0.208260</td>\n",
       "      <td>-0.060771</td>\n",
       "      <td>-0.771815</td>\n",
       "      <td>-0.069535</td>\n",
       "      <td>0.115222</td>\n",
       "      <td>0.123983</td>\n",
       "      <td>0.115149</td>\n",
       "      <td>0.685040</td>\n",
       "      <td>0.779197</td>\n",
       "      <td>0.686057</td>\n",
       "      <td>0.191421</td>\n",
       "      <td>0.289496</td>\n",
       "      <td>0.143596</td>\n",
       "      <td>0.255264</td>\n",
       "      <td>0.344289</td>\n",
       "      <td>0.252493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>auto</td>\n",
       "      <td>0.089920</td>\n",
       "      <td>0.113200</td>\n",
       "      <td>0.541816</td>\n",
       "      <td>-0.007813</td>\n",
       "      <td>-0.012382</td>\n",
       "      <td>-0.283667</td>\n",
       "      <td>0.084165</td>\n",
       "      <td>0.083995</td>\n",
       "      <td>0.090603</td>\n",
       "      <td>0.862010</td>\n",
       "      <td>0.862127</td>\n",
       "      <td>0.877866</td>\n",
       "      <td>0.139593</td>\n",
       "      <td>0.127298</td>\n",
       "      <td>0.197753</td>\n",
       "      <td>0.308753</td>\n",
       "      <td>0.313738</td>\n",
       "      <td>0.351814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cpu</td>\n",
       "      <td>53.967371</td>\n",
       "      <td>79.863192</td>\n",
       "      <td>80.801158</td>\n",
       "      <td>-0.088498</td>\n",
       "      <td>-0.193805</td>\n",
       "      <td>-0.198384</td>\n",
       "      <td>2.970015</td>\n",
       "      <td>0.515923</td>\n",
       "      <td>0.516275</td>\n",
       "      <td>138.050172</td>\n",
       "      <td>85.882068</td>\n",
       "      <td>86.350465</td>\n",
       "      <td>0.396863</td>\n",
       "      <td>0.163034</td>\n",
       "      <td>0.336307</td>\n",
       "      <td>64.273224</td>\n",
       "      <td>39.050366</td>\n",
       "      <td>41.823550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bank</td>\n",
       "      <td>87.267024</td>\n",
       "      <td>111.017350</td>\n",
       "      <td>117.853916</td>\n",
       "      <td>-0.165095</td>\n",
       "      <td>-0.267188</td>\n",
       "      <td>-0.301108</td>\n",
       "      <td>5.062429</td>\n",
       "      <td>5.696315</td>\n",
       "      <td>5.880017</td>\n",
       "      <td>181.797910</td>\n",
       "      <td>198.288666</td>\n",
       "      <td>203.319347</td>\n",
       "      <td>0.395961</td>\n",
       "      <td>0.402128</td>\n",
       "      <td>0.485056</td>\n",
       "      <td>83.752613</td>\n",
       "      <td>89.909993</td>\n",
       "      <td>101.132340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wisconsin</td>\n",
       "      <td>4.007429</td>\n",
       "      <td>3.920993</td>\n",
       "      <td>4.103392</td>\n",
       "      <td>-0.551960</td>\n",
       "      <td>-0.528406</td>\n",
       "      <td>-0.578711</td>\n",
       "      <td>1.508150</td>\n",
       "      <td>3.044881</td>\n",
       "      <td>1.560828</td>\n",
       "      <td>4.007429</td>\n",
       "      <td>6.083615</td>\n",
       "      <td>4.103392</td>\n",
       "      <td>0.388776</td>\n",
       "      <td>0.356503</td>\n",
       "      <td>0.528612</td>\n",
       "      <td>1.723467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.938160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset      rmse0       rmse1       rmse2       r20       r21       r22  \\\n",
       "0        mpg   6.908439   11.819784    9.412203 -0.882292 -2.582685 -1.637702   \n",
       "1     boston   6.553659   12.702200    5.629033 -0.494173 -1.856390 -0.364568   \n",
       "2      yacht   9.226756   10.836262    4.922452 -0.395747 -0.545856 -0.112637   \n",
       "3       wine   0.433328    0.053327    0.203310 -0.262182 -0.003971 -0.057715   \n",
       "4      crime   0.194694    0.693841    0.208260 -0.060771 -0.771815 -0.069535   \n",
       "5       auto   0.089920    0.113200    0.541816 -0.007813 -0.012382 -0.283667   \n",
       "6        cpu  53.967371   79.863192   80.801158 -0.088498 -0.193805 -0.198384   \n",
       "7       bank  87.267024  111.017350  117.853916 -0.165095 -0.267188 -0.301108   \n",
       "8  wisconsin   4.007429    3.920993    4.103392 -0.551960 -0.528406 -0.578711   \n",
       "\n",
       "      mape0     mape1     mape2        mae0        mae1        mae2      cal0  \\\n",
       "0  0.288183  0.471108  0.364452    7.667194   11.857822    9.573434  0.333734   \n",
       "1  0.327888  0.507731  0.318090    8.427232   12.860030    8.003746  0.293636   \n",
       "2  0.773896  1.683818  3.211313    9.405702   10.836262    9.458529  0.309427   \n",
       "3  0.131844  0.122339  0.123365    0.714859    0.700542    0.690043  0.205123   \n",
       "4  0.115222  0.123983  0.115149    0.685040    0.779197    0.686057  0.191421   \n",
       "5  0.084165  0.083995  0.090603    0.862010    0.862127    0.877866  0.139593   \n",
       "6  2.970015  0.515923  0.516275  138.050172   85.882068   86.350465  0.396863   \n",
       "7  5.062429  5.696315  5.880017  181.797910  198.288666  203.319347  0.395961   \n",
       "8  1.508150  3.044881  1.560828    4.007429    6.083615    4.103392  0.388776   \n",
       "\n",
       "       cal1      cal2       pbl0       pbl1        pbl2  \n",
       "0  0.470061  0.336346   3.555596   5.207469    3.639072  \n",
       "1  0.433876  0.280373   3.820485   5.494944    3.622965  \n",
       "2  0.422117  0.208501   4.603608   4.932029    4.277691  \n",
       "3  0.138893  0.164480   0.308350   0.276247    0.267734  \n",
       "4  0.289496  0.143596   0.255264   0.344289    0.252493  \n",
       "5  0.127298  0.197753   0.308753   0.313738    0.351814  \n",
       "6  0.163034  0.336307  64.273224  39.050366   41.823550  \n",
       "7  0.402128  0.485056  83.752613  89.909993  101.132340  \n",
       "8  0.356503  0.528612   1.723467        NaN    1.938160  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_columns = ['dataset'] + [name + str(i) for name in metric_names + ['cal', 'pbl'] for i in range(3)]\n",
    "df_ensemble.loc[:,ordered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ensemble=df_lin.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"12\" halign=\"left\">Bayesian Linear Regression</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Uncalibrated</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Kuleshov et al.</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Song et al.</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Ours</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>CHK</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>CHK</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>CHK</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>CHK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mpg</th>\n",
       "      <td>2.456</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.921</td>\n",
       "      <td>2.465</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.916</td>\n",
       "      <td>2.522</td>\n",
       "      <td>0.115</td>\n",
       "      <td>1.019</td>\n",
       "      <td>2.522</td>\n",
       "      <td>0.115</td>\n",
       "      <td>1.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boston</th>\n",
       "      <td>3.459</td>\n",
       "      <td>0.171</td>\n",
       "      <td>1.392</td>\n",
       "      <td>3.399</td>\n",
       "      <td>0.165</td>\n",
       "      <td>1.365</td>\n",
       "      <td>3.349</td>\n",
       "      <td>0.169</td>\n",
       "      <td>1.362</td>\n",
       "      <td>3.349</td>\n",
       "      <td>0.169</td>\n",
       "      <td>1.362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yacht</th>\n",
       "      <td>6.171</td>\n",
       "      <td>3.800</td>\n",
       "      <td>2.438</td>\n",
       "      <td>5.964</td>\n",
       "      <td>4.802</td>\n",
       "      <td>2.379</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine</th>\n",
       "      <td>0.628</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crime</th>\n",
       "      <td>0.516</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auto</th>\n",
       "      <td>0.635</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu</th>\n",
       "      <td>39.896</td>\n",
       "      <td>0.647</td>\n",
       "      <td>15.871</td>\n",
       "      <td>39.086</td>\n",
       "      <td>0.636</td>\n",
       "      <td>15.463</td>\n",
       "      <td>28.160</td>\n",
       "      <td>0.324</td>\n",
       "      <td>13.615</td>\n",
       "      <td>28.160</td>\n",
       "      <td>0.324</td>\n",
       "      <td>13.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bank</th>\n",
       "      <td>39.508</td>\n",
       "      <td>0.590</td>\n",
       "      <td>17.478</td>\n",
       "      <td>39.148</td>\n",
       "      <td>0.568</td>\n",
       "      <td>16.639</td>\n",
       "      <td>29.314</td>\n",
       "      <td>0.387</td>\n",
       "      <td>14.274</td>\n",
       "      <td>29.314</td>\n",
       "      <td>0.387</td>\n",
       "      <td>14.274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Bayesian Linear Regression                                        \\\n",
       "                      Uncalibrated                Kuleshov et al.          \n",
       "                               MAE   MAPE     CHK             MAE   MAPE   \n",
       "dataset                                                                    \n",
       "mpg                          2.456  0.114   0.921           2.465  0.114   \n",
       "boston                       3.459  0.171   1.392           3.399  0.165   \n",
       "yacht                        6.171  3.800   2.438           5.964  4.802   \n",
       "wine                         0.628  0.106   0.240           0.627  0.105   \n",
       "crime                        0.516  0.087   0.202           0.514  0.086   \n",
       "auto                         0.635  0.058   0.251           0.629  0.057   \n",
       "cpu                         39.896  0.647  15.871          39.086  0.636   \n",
       "bank                        39.508  0.590  17.478          39.148  0.568   \n",
       "\n",
       "                                                                   \n",
       "                Song et al.                   Ours                 \n",
       "            CHK         MAE   MAPE     CHK     MAE   MAPE     CHK  \n",
       "dataset                                                            \n",
       "mpg       0.916       2.522  0.115   1.019   2.522  0.115   1.019  \n",
       "boston    1.365       3.349  0.169   1.362   3.349  0.169   1.362  \n",
       "yacht     2.379       0.908  0.336   0.352   0.908  0.336   0.352  \n",
       "wine      0.239       0.637  0.106   0.241   0.637  0.106   0.241  \n",
       "crime     0.202       0.517  0.086   0.204   0.517  0.086   0.204  \n",
       "auto      0.250       0.636  0.060   0.257   0.636  0.060   0.257  \n",
       "cpu      15.463      28.160  0.324  13.615  28.160  0.324  13.615  \n",
       "bank     16.639      29.314  0.387  14.274  29.314  0.387  14.274  "
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_latex = df_lin.copy()\n",
    "df_latex = df_latex.round(3)\n",
    "df_latex = df_latex.iloc[:-1].loc[:,[\n",
    "    'dataset', \n",
    "    'mae0', 'mape0', 'pbl0',\n",
    "    'mae1', 'mape1', 'pbl1',\n",
    "    'mae2', 'mape2', 'pbl2',\n",
    "    'mae2', 'mape2', 'pbl2',    \n",
    "]]\n",
    "df_latex = df_latex.set_index('dataset')\n",
    "\n",
    "header_names = (['Bayesian Linear Regression' for i in range(13)] ,\n",
    "                ['Uncalibrated', 'Uncalibrated', 'Uncalibrated', \n",
    "                 'Kuleshov et al.', 'Kuleshov et al.', 'Kuleshov et al.', \n",
    "                 'Song et al.', 'Song et al.', 'Song et al.', \n",
    "                 'Ours', 'Ours', 'Ours'],\n",
    "                ['MAE', 'MAPE', 'CHK', \n",
    "                 'MAE', 'MAPE', 'CHK',\n",
    "                 'MAE', 'MAPE', 'CHK',\n",
    "                 'MAE', 'MAPE', 'CHK',]\n",
    "               )\n",
    "tuples = list(zip(*header_names))\n",
    "header = pd.MultiIndex.from_tuples(tuples)\n",
    "df_latex.columns = header\n",
    "\n",
    "df_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{12}{l}{Bayesian Linear Regression} \\\\\n",
      "{} & \\multicolumn{3}{l}{Uncalibrated} & \\multicolumn{3}{l}{Kuleshov et al.} & \\multicolumn{3}{l}{Song et al.} & \\multicolumn{3}{l}{Ours} \\\\\n",
      "{} &                        MAE &   MAPE &     CHK &             MAE &   MAPE &     CHK &         MAE &   MAPE &     CHK &     MAE &   MAPE &     CHK \\\\\n",
      "dataset &                            &        &         &                 &        &         &             &        &         &         &        &         \\\\\n",
      "\\midrule\n",
      "mpg     &                      2.456 &  0.114 &   0.921 &           2.465 &  0.114 &   0.916 &       2.522 &  0.115 &   1.019 &   2.522 &  0.115 &   1.019 \\\\\n",
      "boston  &                      3.459 &  0.171 &   1.392 &           3.399 &  0.165 &   1.365 &       3.349 &  0.169 &   1.362 &   3.349 &  0.169 &   1.362 \\\\\n",
      "yacht   &                      6.171 &  3.800 &   2.438 &           5.964 &  4.802 &   2.379 &       0.908 &  0.336 &   0.352 &   0.908 &  0.336 &   0.352 \\\\\n",
      "wine    &                      0.628 &  0.106 &   0.240 &           0.627 &  0.105 &   0.239 &       0.637 &  0.106 &   0.241 &   0.637 &  0.106 &   0.241 \\\\\n",
      "crime   &                      0.516 &  0.087 &   0.202 &           0.514 &  0.086 &   0.202 &       0.517 &  0.086 &   0.204 &   0.517 &  0.086 &   0.204 \\\\\n",
      "auto    &                      0.635 &  0.058 &   0.251 &           0.629 &  0.057 &   0.250 &       0.636 &  0.060 &   0.257 &   0.636 &  0.060 &   0.257 \\\\\n",
      "cpu     &                     39.896 &  0.647 &  15.871 &          39.086 &  0.636 &  15.463 &      28.160 &  0.324 &  13.615 &  28.160 &  0.324 &  13.615 \\\\\n",
      "bank    &                     39.508 &  0.590 &  17.478 &          39.148 &  0.568 &  16.639 &      29.314 &  0.387 &  14.274 &  29.314 &  0.387 &  14.274 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_latex.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"12\" halign=\"left\">Bayesian Neural Network</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Uncalibrated</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Kuleshov et al.</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Song et al.</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Ours</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>CHK</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>CHK</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>CHK</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>CHK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mpg</th>\n",
       "      <td>2.736</td>\n",
       "      <td>0.122</td>\n",
       "      <td>1.198</td>\n",
       "      <td>2.973</td>\n",
       "      <td>0.127</td>\n",
       "      <td>1.176</td>\n",
       "      <td>2.601</td>\n",
       "      <td>0.119</td>\n",
       "      <td>1.083</td>\n",
       "      <td>2.601</td>\n",
       "      <td>0.119</td>\n",
       "      <td>1.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boston</th>\n",
       "      <td>2.966</td>\n",
       "      <td>0.147</td>\n",
       "      <td>1.237</td>\n",
       "      <td>3.003</td>\n",
       "      <td>0.141</td>\n",
       "      <td>1.206</td>\n",
       "      <td>3.507</td>\n",
       "      <td>0.184</td>\n",
       "      <td>1.404</td>\n",
       "      <td>3.507</td>\n",
       "      <td>0.184</td>\n",
       "      <td>1.404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yacht</th>\n",
       "      <td>3.539</td>\n",
       "      <td>0.592</td>\n",
       "      <td>1.535</td>\n",
       "      <td>3.772</td>\n",
       "      <td>0.516</td>\n",
       "      <td>1.519</td>\n",
       "      <td>3.175</td>\n",
       "      <td>0.470</td>\n",
       "      <td>1.510</td>\n",
       "      <td>3.175</td>\n",
       "      <td>0.470</td>\n",
       "      <td>1.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crime</th>\n",
       "      <td>0.498</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auto</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu</th>\n",
       "      <td>74.001</td>\n",
       "      <td>0.518</td>\n",
       "      <td>35.528</td>\n",
       "      <td>71.033</td>\n",
       "      <td>0.633</td>\n",
       "      <td>28.683</td>\n",
       "      <td>66.428</td>\n",
       "      <td>0.630</td>\n",
       "      <td>31.318</td>\n",
       "      <td>66.428</td>\n",
       "      <td>0.630</td>\n",
       "      <td>31.318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bank</th>\n",
       "      <td>96.088</td>\n",
       "      <td>0.722</td>\n",
       "      <td>46.022</td>\n",
       "      <td>90.887</td>\n",
       "      <td>1.105</td>\n",
       "      <td>inf</td>\n",
       "      <td>85.096</td>\n",
       "      <td>1.194</td>\n",
       "      <td>39.257</td>\n",
       "      <td>85.096</td>\n",
       "      <td>1.194</td>\n",
       "      <td>39.257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Bayesian Neural Network                                                \\\n",
       "                   Uncalibrated                Kuleshov et al.                  \n",
       "                            MAE   MAPE     CHK             MAE   MAPE     CHK   \n",
       "dataset                                                                         \n",
       "mpg                       2.736  0.122   1.198           2.973  0.127   1.176   \n",
       "boston                    2.966  0.147   1.237           3.003  0.141   1.206   \n",
       "yacht                     3.539  0.592   1.535           3.772  0.516   1.519   \n",
       "wine                      0.625  0.105   0.252           0.630  0.104   0.241   \n",
       "crime                     0.498  0.085   0.195           0.487  0.083   0.192   \n",
       "auto                      0.625  0.059   0.250           0.623  0.058   0.246   \n",
       "cpu                      74.001  0.518  35.528          71.033  0.633  28.683   \n",
       "bank                     96.088  0.722  46.022          90.887  1.105     inf   \n",
       "\n",
       "                                                           \n",
       "        Song et al.                   Ours                 \n",
       "                MAE   MAPE     CHK     MAE   MAPE     CHK  \n",
       "dataset                                                    \n",
       "mpg           2.601  0.119   1.083   2.601  0.119   1.083  \n",
       "boston        3.507  0.184   1.404   3.507  0.184   1.404  \n",
       "yacht         3.175  0.470   1.510   3.175  0.470   1.510  \n",
       "wine          0.621  0.103   0.238   0.621  0.103   0.238  \n",
       "crime         0.491  0.083   0.192   0.491  0.083   0.192  \n",
       "auto          0.644  0.061   0.248   0.644  0.061   0.248  \n",
       "cpu          66.428  0.630  31.318  66.428  0.630  31.318  \n",
       "bank         85.096  1.194  39.257  85.096  1.194  39.257  "
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_latex = df_bayesian.copy()\n",
    "df_latex = df_latex.round(3)\n",
    "df_latex = df_latex.iloc[:-1].loc[:,[\n",
    "    'dataset', \n",
    "    'mae0', 'mape0', 'pbl0',\n",
    "    'mae1', 'mape1', 'pbl1',\n",
    "    'mae2', 'mape2', 'pbl2',\n",
    "    'mae2', 'mape2', 'pbl2',    \n",
    "]]\n",
    "df_latex = df_latex.set_index('dataset')\n",
    "\n",
    "header_names = (['Bayesian Neural Network' for i in range(13)],\n",
    "                ['Uncalibrated', 'Uncalibrated', 'Uncalibrated', \n",
    "                 'Kuleshov et al.', 'Kuleshov et al.', 'Kuleshov et al.', \n",
    "                 'Song et al.', 'Song et al.', 'Song et al.', \n",
    "                 'Ours', 'Ours', 'Ours'],\n",
    "                ['MAE', 'MAPE', 'CHK', \n",
    "                 'MAE', 'MAPE', 'CHK',\n",
    "                 'MAE', 'MAPE', 'CHK',\n",
    "                 'MAE', 'MAPE', 'CHK',]\n",
    "               )\n",
    "tuples = list(zip(*header_names))\n",
    "header = pd.MultiIndex.from_tuples(tuples)\n",
    "df_latex.columns = header\n",
    "\n",
    "df_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{12}{l}{Bayesian Neural Network} \\\\\n",
      "{} & \\multicolumn{3}{l}{Uncalibrated} & \\multicolumn{3}{l}{Kuleshov et al.} & \\multicolumn{3}{l}{Song et al.} & \\multicolumn{3}{l}{Ours} \\\\\n",
      "{} &                     MAE &   MAPE &     CHK &             MAE &   MAPE &     CHK &         MAE &   MAPE &     CHK &     MAE &   MAPE &     CHK \\\\\n",
      "dataset &                         &        &         &                 &        &         &             &        &         &         &        &         \\\\\n",
      "\\midrule\n",
      "mpg     &                   2.736 &  0.122 &   1.198 &           2.973 &  0.127 &   1.176 &       2.601 &  0.119 &   1.083 &   2.601 &  0.119 &   1.083 \\\\\n",
      "boston  &                   2.966 &  0.147 &   1.237 &           3.003 &  0.141 &   1.206 &       3.507 &  0.184 &   1.404 &   3.507 &  0.184 &   1.404 \\\\\n",
      "yacht   &                   3.539 &  0.592 &   1.535 &           3.772 &  0.516 &   1.519 &       3.175 &  0.470 &   1.510 &   3.175 &  0.470 &   1.510 \\\\\n",
      "wine    &                   0.625 &  0.105 &   0.252 &           0.630 &  0.104 &   0.241 &       0.621 &  0.103 &   0.238 &   0.621 &  0.103 &   0.238 \\\\\n",
      "crime   &                   0.498 &  0.085 &   0.195 &           0.487 &  0.083 &   0.192 &       0.491 &  0.083 &   0.192 &   0.491 &  0.083 &   0.192 \\\\\n",
      "auto    &                   0.625 &  0.059 &   0.250 &           0.623 &  0.058 &   0.246 &       0.644 &  0.061 &   0.248 &   0.644 &  0.061 &   0.248 \\\\\n",
      "cpu     &                  74.001 &  0.518 &  35.528 &          71.033 &  0.633 &  28.683 &      66.428 &  0.630 &  31.318 &  66.428 &  0.630 &  31.318 \\\\\n",
      "bank    &                  96.088 &  0.722 &  46.022 &          90.887 &  1.105 &     inf &      85.096 &  1.194 &  39.257 &  85.096 &  1.194 &  39.257 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_latex.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"12\" halign=\"left\">Deep Ensemble</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Uncalibrated</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Kuleshov et al.</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Song et al.</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Ours</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>CHK</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>CHK</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>CHK</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>CHK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mpg</th>\n",
       "      <td>7.667</td>\n",
       "      <td>0.288</td>\n",
       "      <td>3.556</td>\n",
       "      <td>11.858</td>\n",
       "      <td>0.471</td>\n",
       "      <td>5.207</td>\n",
       "      <td>9.573</td>\n",
       "      <td>0.364</td>\n",
       "      <td>3.639</td>\n",
       "      <td>9.573</td>\n",
       "      <td>0.364</td>\n",
       "      <td>3.639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boston</th>\n",
       "      <td>8.427</td>\n",
       "      <td>0.328</td>\n",
       "      <td>3.820</td>\n",
       "      <td>12.860</td>\n",
       "      <td>0.508</td>\n",
       "      <td>5.495</td>\n",
       "      <td>8.004</td>\n",
       "      <td>0.318</td>\n",
       "      <td>3.623</td>\n",
       "      <td>8.004</td>\n",
       "      <td>0.318</td>\n",
       "      <td>3.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yacht</th>\n",
       "      <td>9.406</td>\n",
       "      <td>0.774</td>\n",
       "      <td>4.604</td>\n",
       "      <td>10.836</td>\n",
       "      <td>1.684</td>\n",
       "      <td>4.932</td>\n",
       "      <td>9.459</td>\n",
       "      <td>3.211</td>\n",
       "      <td>4.278</td>\n",
       "      <td>9.459</td>\n",
       "      <td>3.211</td>\n",
       "      <td>4.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine</th>\n",
       "      <td>0.715</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crime</th>\n",
       "      <td>0.685</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auto</th>\n",
       "      <td>0.862</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu</th>\n",
       "      <td>138.050</td>\n",
       "      <td>2.970</td>\n",
       "      <td>64.273</td>\n",
       "      <td>85.882</td>\n",
       "      <td>0.516</td>\n",
       "      <td>39.050</td>\n",
       "      <td>86.350</td>\n",
       "      <td>0.516</td>\n",
       "      <td>41.824</td>\n",
       "      <td>86.350</td>\n",
       "      <td>0.516</td>\n",
       "      <td>41.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bank</th>\n",
       "      <td>181.798</td>\n",
       "      <td>5.062</td>\n",
       "      <td>83.753</td>\n",
       "      <td>198.289</td>\n",
       "      <td>5.696</td>\n",
       "      <td>89.910</td>\n",
       "      <td>203.319</td>\n",
       "      <td>5.880</td>\n",
       "      <td>101.132</td>\n",
       "      <td>203.319</td>\n",
       "      <td>5.880</td>\n",
       "      <td>101.132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Deep Ensemble                                                \\\n",
       "         Uncalibrated                Kuleshov et al.                  \n",
       "                  MAE   MAPE     CHK             MAE   MAPE     CHK   \n",
       "dataset                                                               \n",
       "mpg             7.667  0.288   3.556          11.858  0.471   5.207   \n",
       "boston          8.427  0.328   3.820          12.860  0.508   5.495   \n",
       "yacht           9.406  0.774   4.604          10.836  1.684   4.932   \n",
       "wine            0.715  0.132   0.308           0.701  0.122   0.276   \n",
       "crime           0.685  0.115   0.255           0.779  0.124   0.344   \n",
       "auto            0.862  0.084   0.309           0.862  0.084   0.314   \n",
       "cpu           138.050  2.970  64.273          85.882  0.516  39.050   \n",
       "bank          181.798  5.062  83.753         198.289  5.696  89.910   \n",
       "\n",
       "                                                              \n",
       "        Song et al.                     Ours                  \n",
       "                MAE   MAPE      CHK      MAE   MAPE      CHK  \n",
       "dataset                                                       \n",
       "mpg           9.573  0.364    3.639    9.573  0.364    3.639  \n",
       "boston        8.004  0.318    3.623    8.004  0.318    3.623  \n",
       "yacht         9.459  3.211    4.278    9.459  3.211    4.278  \n",
       "wine          0.690  0.123    0.268    0.690  0.123    0.268  \n",
       "crime         0.686  0.115    0.252    0.686  0.115    0.252  \n",
       "auto          0.878  0.091    0.352    0.878  0.091    0.352  \n",
       "cpu          86.350  0.516   41.824   86.350  0.516   41.824  \n",
       "bank        203.319  5.880  101.132  203.319  5.880  101.132  "
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_latex = df_ensemble.copy()\n",
    "df_latex = df_latex.round(3)\n",
    "df_latex = df_latex.iloc[:-1].loc[:,[\n",
    "    'dataset', \n",
    "    'mae0', 'mape0', 'pbl0',\n",
    "    'mae1', 'mape1', 'pbl1',\n",
    "    'mae2', 'mape2', 'pbl2',\n",
    "    'mae2', 'mape2', 'pbl2',    \n",
    "]]\n",
    "df_latex = df_latex.set_index('dataset')\n",
    "\n",
    "header_names = (['Deep Ensemble' for i in range(13)],\n",
    "                ['Uncalibrated', 'Uncalibrated', 'Uncalibrated', \n",
    "                 'Kuleshov et al.', 'Kuleshov et al.', 'Kuleshov et al.', \n",
    "                 'Song et al.', 'Song et al.', 'Song et al.', \n",
    "                 'Ours', 'Ours', 'Ours'],\n",
    "                ['MAE', 'MAPE', 'CHK', \n",
    "                 'MAE', 'MAPE', 'CHK',\n",
    "                 'MAE', 'MAPE', 'CHK',\n",
    "                 'MAE', 'MAPE', 'CHK',]\n",
    "               )\n",
    "tuples = list(zip(*header_names))\n",
    "header = pd.MultiIndex.from_tuples(tuples)\n",
    "df_latex.columns = header\n",
    "\n",
    "df_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{12}{l}{Deep Ensemble} \\\\\n",
      "{} & \\multicolumn{3}{l}{Uncalibrated} & \\multicolumn{3}{l}{Kuleshov et al.} & \\multicolumn{3}{l}{Song et al.} & \\multicolumn{3}{l}{Ours} \\\\\n",
      "{} &           MAE &   MAPE &     CHK &             MAE &   MAPE &     CHK &         MAE &   MAPE &      CHK &      MAE &   MAPE &      CHK \\\\\n",
      "dataset &               &        &         &                 &        &         &             &        &          &          &        &          \\\\\n",
      "\\midrule\n",
      "mpg     &         7.667 &  0.288 &   3.556 &          11.858 &  0.471 &   5.207 &       9.573 &  0.364 &    3.639 &    9.573 &  0.364 &    3.639 \\\\\n",
      "boston  &         8.427 &  0.328 &   3.820 &          12.860 &  0.508 &   5.495 &       8.004 &  0.318 &    3.623 &    8.004 &  0.318 &    3.623 \\\\\n",
      "yacht   &         9.406 &  0.774 &   4.604 &          10.836 &  1.684 &   4.932 &       9.459 &  3.211 &    4.278 &    9.459 &  3.211 &    4.278 \\\\\n",
      "wine    &         0.715 &  0.132 &   0.308 &           0.701 &  0.122 &   0.276 &       0.690 &  0.123 &    0.268 &    0.690 &  0.123 &    0.268 \\\\\n",
      "crime   &         0.685 &  0.115 &   0.255 &           0.779 &  0.124 &   0.344 &       0.686 &  0.115 &    0.252 &    0.686 &  0.115 &    0.252 \\\\\n",
      "auto    &         0.862 &  0.084 &   0.309 &           0.862 &  0.084 &   0.314 &       0.878 &  0.091 &    0.352 &    0.878 &  0.091 &    0.352 \\\\\n",
      "cpu     &       138.050 &  2.970 &  64.273 &          85.882 &  0.516 &  39.050 &      86.350 &  0.516 &   41.824 &   86.350 &  0.516 &   41.824 \\\\\n",
      "bank    &       181.798 &  5.062 &  83.753 &         198.289 &  5.696 &  89.910 &     203.319 &  5.880 &  101.132 &  203.319 &  5.880 &  101.132 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_latex.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "      <th>mape</th>\n",
       "      <th>cal0</th>\n",
       "      <th>cal1</th>\n",
       "      <th>cal2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mpg</td>\n",
       "      <td>0.014372</td>\n",
       "      <td>0.678815</td>\n",
       "      <td>0.118794</td>\n",
       "      <td>0.053174</td>\n",
       "      <td>0.042289</td>\n",
       "      <td>0.052860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boston</td>\n",
       "      <td>0.659406</td>\n",
       "      <td>0.756587</td>\n",
       "      <td>0.142088</td>\n",
       "      <td>0.041694</td>\n",
       "      <td>0.048888</td>\n",
       "      <td>0.065183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yacht</td>\n",
       "      <td>1.442260</td>\n",
       "      <td>0.747825</td>\n",
       "      <td>0.760690</td>\n",
       "      <td>0.174772</td>\n",
       "      <td>0.062915</td>\n",
       "      <td>0.097397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wine</td>\n",
       "      <td>0.121620</td>\n",
       "      <td>0.171496</td>\n",
       "      <td>0.103378</td>\n",
       "      <td>0.113661</td>\n",
       "      <td>0.032735</td>\n",
       "      <td>0.200686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crime</td>\n",
       "      <td>0.008932</td>\n",
       "      <td>0.398673</td>\n",
       "      <td>0.084195</td>\n",
       "      <td>0.058347</td>\n",
       "      <td>0.024584</td>\n",
       "      <td>0.197563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>auto</td>\n",
       "      <td>0.173017</td>\n",
       "      <td>0.237374</td>\n",
       "      <td>0.056902</td>\n",
       "      <td>0.071787</td>\n",
       "      <td>0.012209</td>\n",
       "      <td>0.148758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cpu</td>\n",
       "      <td>83.986782</td>\n",
       "      <td>0.043592</td>\n",
       "      <td>0.600895</td>\n",
       "      <td>0.257012</td>\n",
       "      <td>0.051443</td>\n",
       "      <td>0.154128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bank</td>\n",
       "      <td>57.300161</td>\n",
       "      <td>-0.099929</td>\n",
       "      <td>0.518060</td>\n",
       "      <td>0.375168</td>\n",
       "      <td>0.087850</td>\n",
       "      <td>0.153775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wisconsin</td>\n",
       "      <td>0.370514</td>\n",
       "      <td>-0.158733</td>\n",
       "      <td>0.938386</td>\n",
       "      <td>0.271466</td>\n",
       "      <td>0.052245</td>\n",
       "      <td>0.189696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset       rmse        r2      mape      cal0      cal1      cal2\n",
       "0        mpg   0.014372  0.678815  0.118794  0.053174  0.042289  0.052860\n",
       "1     boston   0.659406  0.756587  0.142088  0.041694  0.048888  0.065183\n",
       "2      yacht   1.442260  0.747825  0.760690  0.174772  0.062915  0.097397\n",
       "3       wine   0.121620  0.171496  0.103378  0.113661  0.032735  0.200686\n",
       "4      crime   0.008932  0.398673  0.084195  0.058347  0.024584  0.197563\n",
       "5       auto   0.173017  0.237374  0.056902  0.071787  0.012209  0.148758\n",
       "6        cpu  83.986782  0.043592  0.600895  0.257012  0.051443  0.154128\n",
       "7       bank  57.300161 -0.099929  0.518060  0.375168  0.087850  0.153775\n",
       "8  wisconsin   0.370514 -0.158733  0.938386  0.271466  0.052245  0.189696"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recal 1\n",
      "Recal 2\n",
      "Epoch 1/3000\n",
      "379/379 [==============================] - 5s 14ms/step - loss: 4417.9883\n",
      "Epoch 2/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 115.6395\n",
      "Epoch 3/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 41.5719\n",
      "Epoch 4/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 23.5061\n",
      "Epoch 5/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 15.3930\n",
      "Epoch 6/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 11.2122\n",
      "Epoch 7/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 8.8733\n",
      "Epoch 8/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 7.4891\n",
      "Epoch 9/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 6.6419\n",
      "Epoch 10/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 6.1137\n",
      "Epoch 11/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 5.7847\n",
      "Epoch 12/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 5.5834\n",
      "Epoch 13/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 5.4663\n",
      "Epoch 14/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 5.4048\n",
      "Epoch 15/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 5.3799\n",
      "Epoch 16/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 5.3793\n",
      "Epoch 17/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 5.3947\n",
      "Epoch 18/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 5.4202\n",
      "Epoch 19/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 5.4520\n",
      "Epoch 20/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 5.4871\n",
      "Epoch 21/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 5.5231\n",
      "Epoch 22/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.5592\n",
      "Epoch 23/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 5.5941\n",
      "Epoch 24/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 5.6267\n",
      "Epoch 25/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 5.6574\n",
      "Epoch 26/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.6852\n",
      "Epoch 27/3000\n",
      "379/379 [==============================] - 0s 25us/step - loss: 5.7104\n",
      "Epoch 28/3000\n",
      "379/379 [==============================] - 0s 22us/step - loss: 5.7324\n",
      "Epoch 29/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 5.7519\n",
      "Epoch 30/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 5.7685\n",
      "Epoch 31/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 5.7822\n",
      "Epoch 32/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.7935\n",
      "Epoch 33/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 5.8022\n",
      "Epoch 34/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.8087\n",
      "Epoch 35/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 5.8129\n",
      "Epoch 36/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 5.8152\n",
      "Epoch 37/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 5.8156\n",
      "Epoch 38/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 5.8143\n",
      "Epoch 39/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 5.8115\n",
      "Epoch 40/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.8073\n",
      "Epoch 41/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.8018\n",
      "Epoch 42/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 5.7952\n",
      "Epoch 43/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.7876\n",
      "Epoch 44/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 5.7791\n",
      "Epoch 45/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 5.7698\n",
      "Epoch 46/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 5.7598\n",
      "Epoch 47/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 5.7493\n",
      "Epoch 48/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 5.7383\n",
      "Epoch 49/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 5.7268\n",
      "Epoch 50/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 5.7151\n",
      "Epoch 51/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.7031\n",
      "Epoch 52/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 5.6909\n",
      "Epoch 53/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.6786\n",
      "Epoch 54/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.6661\n",
      "Epoch 55/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.6535\n",
      "Epoch 56/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.6408\n",
      "Epoch 57/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.6281\n",
      "Epoch 58/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.6155\n",
      "Epoch 59/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 5.6030\n",
      "Epoch 60/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 5.5905\n",
      "Epoch 61/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 5.5781\n",
      "Epoch 62/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.5660\n",
      "Epoch 63/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 5.5540\n",
      "Epoch 64/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 5.5421\n",
      "Epoch 65/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 5.5299\n",
      "Epoch 66/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.5167\n",
      "Epoch 67/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 5.5019\n",
      "Epoch 68/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 5.4849\n",
      "Epoch 69/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 5.4651\n",
      "Epoch 70/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 5.4429\n",
      "Epoch 71/3000\n",
      "379/379 [==============================] - 0s 22us/step - loss: 5.4193\n",
      "Epoch 72/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.3949\n",
      "Epoch 73/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.3699\n",
      "Epoch 74/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.3443\n",
      "Epoch 75/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 5.3176\n",
      "Epoch 76/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 5.2900\n",
      "Epoch 77/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 5.2613\n",
      "Epoch 78/3000\n",
      "379/379 [==============================] - 0s 27us/step - loss: 5.2316\n",
      "Epoch 79/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 5.2011\n",
      "Epoch 80/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 5.1700\n",
      "Epoch 81/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 5.1373\n",
      "Epoch 82/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 5.1036\n",
      "Epoch 83/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 5.0689\n",
      "Epoch 84/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 5.0361\n",
      "Epoch 85/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 5.0084\n",
      "Epoch 86/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 4.9881\n",
      "Epoch 87/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 4.9735\n",
      "Epoch 88/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 4.9573\n",
      "Epoch 89/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 4.9347\n",
      "Epoch 90/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 4.9055\n",
      "Epoch 91/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 4.8711\n",
      "Epoch 92/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 4.8348\n",
      "Epoch 93/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 4.7982\n",
      "Epoch 94/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 4.7612\n",
      "Epoch 95/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 4.7222\n",
      "Epoch 96/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 4.6794\n",
      "Epoch 97/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 4.6319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 4.5791\n",
      "Epoch 99/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 4.5202\n",
      "Epoch 100/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 4.4572\n",
      "Epoch 101/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 4.3916\n",
      "Epoch 102/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 4.3280\n",
      "Epoch 103/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 4.2724\n",
      "Epoch 104/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 4.2260\n",
      "Epoch 105/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 4.1790\n",
      "Epoch 106/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 4.1268\n",
      "Epoch 107/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 4.0750\n",
      "Epoch 108/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 4.0392\n",
      "Epoch 109/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 4.0165\n",
      "Epoch 110/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.9975\n",
      "Epoch 111/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.9778\n",
      "Epoch 112/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 3.9607\n",
      "Epoch 113/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 3.9446\n",
      "Epoch 114/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 3.9218\n",
      "Epoch 115/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.9000\n",
      "Epoch 116/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.8780\n",
      "Epoch 117/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 3.8523\n",
      "Epoch 118/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.8234\n",
      "Epoch 119/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.7945\n",
      "Epoch 120/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 3.7649\n",
      "Epoch 121/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.7331\n",
      "Epoch 122/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.7010\n",
      "Epoch 123/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.6691\n",
      "Epoch 124/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.6370\n",
      "Epoch 125/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.6050\n",
      "Epoch 126/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.5740\n",
      "Epoch 127/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.5435\n",
      "Epoch 128/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.5134\n",
      "Epoch 129/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.4833\n",
      "Epoch 130/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.4552\n",
      "Epoch 131/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 3.4285\n",
      "Epoch 132/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.4033\n",
      "Epoch 133/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.3801\n",
      "Epoch 134/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.3591\n",
      "Epoch 135/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.3405\n",
      "Epoch 136/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.3237\n",
      "Epoch 137/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.3090\n",
      "Epoch 138/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.2963\n",
      "Epoch 139/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.2850\n",
      "Epoch 140/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.2747\n",
      "Epoch 141/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.2653\n",
      "Epoch 142/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 3.2568\n",
      "Epoch 143/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.2487\n",
      "Epoch 144/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.2409\n",
      "Epoch 145/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.2336\n",
      "Epoch 146/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.2266\n",
      "Epoch 147/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.2199\n",
      "Epoch 148/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.2136\n",
      "Epoch 149/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.2077\n",
      "Epoch 150/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.2021\n",
      "Epoch 151/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.1966\n",
      "Epoch 152/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.1909\n",
      "Epoch 153/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.1849\n",
      "Epoch 154/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.1786\n",
      "Epoch 155/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.1718\n",
      "Epoch 156/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 3.1648\n",
      "Epoch 157/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.1576\n",
      "Epoch 158/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.1503\n",
      "Epoch 159/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.1430\n",
      "Epoch 160/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 3.1357\n",
      "Epoch 161/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.1282\n",
      "Epoch 162/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.1207\n",
      "Epoch 163/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.1129\n",
      "Epoch 164/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.1050\n",
      "Epoch 165/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.0970\n",
      "Epoch 166/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.0891\n",
      "Epoch 167/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 3.0813\n",
      "Epoch 168/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.0735\n",
      "Epoch 169/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.0659\n",
      "Epoch 170/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.0583\n",
      "Epoch 171/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 3.0507\n",
      "Epoch 172/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.0430\n",
      "Epoch 173/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.0354\n",
      "Epoch 174/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 3.0278\n",
      "Epoch 175/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.0203\n",
      "Epoch 176/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 3.0129\n",
      "Epoch 177/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 3.0056\n",
      "Epoch 178/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.9983\n",
      "Epoch 179/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.9911\n",
      "Epoch 180/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.9841\n",
      "Epoch 181/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.9771\n",
      "Epoch 182/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.9704\n",
      "Epoch 183/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.9638\n",
      "Epoch 184/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.9575\n",
      "Epoch 185/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.9513\n",
      "Epoch 186/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.9454\n",
      "Epoch 187/3000\n",
      "379/379 [==============================] - 0s 26us/step - loss: 2.9398\n",
      "Epoch 188/3000\n",
      "379/379 [==============================] - 0s 27us/step - loss: 2.9344\n",
      "Epoch 189/3000\n",
      "379/379 [==============================] - 0s 19us/step - loss: 2.9295\n",
      "Epoch 190/3000\n",
      "379/379 [==============================] - 0s 28us/step - loss: 2.9249\n",
      "Epoch 191/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.9207\n",
      "Epoch 192/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.9169\n",
      "Epoch 193/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.9135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.9104\n",
      "Epoch 195/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.9078\n",
      "Epoch 196/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.9055\n",
      "Epoch 197/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.9035\n",
      "Epoch 198/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.9018\n",
      "Epoch 199/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.9004\n",
      "Epoch 200/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8992\n",
      "Epoch 201/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8983\n",
      "Epoch 202/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8975\n",
      "Epoch 203/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8969\n",
      "Epoch 204/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8964\n",
      "Epoch 205/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8960\n",
      "Epoch 206/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8956\n",
      "Epoch 207/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8953\n",
      "Epoch 208/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8950\n",
      "Epoch 209/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8946\n",
      "Epoch 210/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8943\n",
      "Epoch 211/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8939\n",
      "Epoch 212/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8935\n",
      "Epoch 213/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.8931\n",
      "Epoch 214/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8926\n",
      "Epoch 215/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8921\n",
      "Epoch 216/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.8917\n",
      "Epoch 217/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8912\n",
      "Epoch 218/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8908\n",
      "Epoch 219/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8904\n",
      "Epoch 220/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.8900\n",
      "Epoch 221/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8895\n",
      "Epoch 222/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8891\n",
      "Epoch 223/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8887\n",
      "Epoch 224/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8883\n",
      "Epoch 225/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8879\n",
      "Epoch 226/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8875\n",
      "Epoch 227/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.8872\n",
      "Epoch 228/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8868\n",
      "Epoch 229/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8865\n",
      "Epoch 230/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8861\n",
      "Epoch 231/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8857\n",
      "Epoch 232/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8853\n",
      "Epoch 233/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.8850\n",
      "Epoch 234/3000\n",
      "379/379 [==============================] - 0s 22us/step - loss: 2.8847\n",
      "Epoch 235/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8844\n",
      "Epoch 236/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8841\n",
      "Epoch 237/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.8838\n",
      "Epoch 238/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8835\n",
      "Epoch 239/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8831\n",
      "Epoch 240/3000\n",
      "379/379 [==============================] - 0s 25us/step - loss: 2.8828\n",
      "Epoch 241/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8825\n",
      "Epoch 242/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.8821\n",
      "Epoch 243/3000\n",
      "379/379 [==============================] - 0s 33us/step - loss: 2.8818\n",
      "Epoch 244/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8814\n",
      "Epoch 245/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8810\n",
      "Epoch 246/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8807\n",
      "Epoch 247/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8803\n",
      "Epoch 248/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8800\n",
      "Epoch 249/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8796\n",
      "Epoch 250/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8793\n",
      "Epoch 251/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.8790\n",
      "Epoch 252/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8787\n",
      "Epoch 253/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8784\n",
      "Epoch 254/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8780\n",
      "Epoch 255/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8777\n",
      "Epoch 256/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8773\n",
      "Epoch 257/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8769\n",
      "Epoch 258/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8765\n",
      "Epoch 259/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8761\n",
      "Epoch 260/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8758\n",
      "Epoch 261/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8754\n",
      "Epoch 262/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8750\n",
      "Epoch 263/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8747\n",
      "Epoch 264/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8744\n",
      "Epoch 265/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8741\n",
      "Epoch 266/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8737\n",
      "Epoch 267/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8735\n",
      "Epoch 268/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8732\n",
      "Epoch 269/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8728\n",
      "Epoch 270/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8725\n",
      "Epoch 271/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8722\n",
      "Epoch 272/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8719\n",
      "Epoch 273/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8716\n",
      "Epoch 274/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8713\n",
      "Epoch 275/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8710\n",
      "Epoch 276/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8707\n",
      "Epoch 277/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8704\n",
      "Epoch 278/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8701\n",
      "Epoch 279/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8698\n",
      "Epoch 280/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8694\n",
      "Epoch 281/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8691\n",
      "Epoch 282/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8688\n",
      "Epoch 283/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8684\n",
      "Epoch 284/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8681\n",
      "Epoch 285/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8678\n",
      "Epoch 286/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8674\n",
      "Epoch 287/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8671\n",
      "Epoch 288/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8667\n",
      "Epoch 289/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8660\n",
      "Epoch 291/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8657\n",
      "Epoch 292/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8653\n",
      "Epoch 293/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8650\n",
      "Epoch 294/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8646\n",
      "Epoch 295/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8643\n",
      "Epoch 296/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8639\n",
      "Epoch 297/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8636\n",
      "Epoch 298/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8632\n",
      "Epoch 299/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8629\n",
      "Epoch 300/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8625\n",
      "Epoch 301/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8621\n",
      "Epoch 302/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8617\n",
      "Epoch 303/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8614\n",
      "Epoch 304/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8610\n",
      "Epoch 305/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8606\n",
      "Epoch 306/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8602\n",
      "Epoch 307/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8599\n",
      "Epoch 308/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8595\n",
      "Epoch 309/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8591\n",
      "Epoch 310/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8587\n",
      "Epoch 311/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8583\n",
      "Epoch 312/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8579\n",
      "Epoch 313/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8575\n",
      "Epoch 314/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8571\n",
      "Epoch 315/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8567\n",
      "Epoch 316/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8563\n",
      "Epoch 317/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8559\n",
      "Epoch 318/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8554\n",
      "Epoch 319/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8550\n",
      "Epoch 320/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8546\n",
      "Epoch 321/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8542\n",
      "Epoch 322/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8537\n",
      "Epoch 323/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8533\n",
      "Epoch 324/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8529\n",
      "Epoch 325/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8524\n",
      "Epoch 326/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8520\n",
      "Epoch 327/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8515\n",
      "Epoch 328/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8511\n",
      "Epoch 329/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8506\n",
      "Epoch 330/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8502\n",
      "Epoch 331/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8497\n",
      "Epoch 332/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8493\n",
      "Epoch 333/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8488\n",
      "Epoch 334/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.8483\n",
      "Epoch 335/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8479\n",
      "Epoch 336/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8474\n",
      "Epoch 337/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8469\n",
      "Epoch 338/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8464\n",
      "Epoch 339/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8459\n",
      "Epoch 340/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8455\n",
      "Epoch 341/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8450\n",
      "Epoch 342/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8445\n",
      "Epoch 343/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.8440\n",
      "Epoch 344/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8435\n",
      "Epoch 345/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8430\n",
      "Epoch 346/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8425\n",
      "Epoch 347/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8420\n",
      "Epoch 348/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8415\n",
      "Epoch 349/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8410\n",
      "Epoch 350/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8405\n",
      "Epoch 351/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8400\n",
      "Epoch 352/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8394\n",
      "Epoch 353/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8389\n",
      "Epoch 354/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8384\n",
      "Epoch 355/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8379\n",
      "Epoch 356/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8374\n",
      "Epoch 357/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8368\n",
      "Epoch 358/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.8363\n",
      "Epoch 359/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8358\n",
      "Epoch 360/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8352\n",
      "Epoch 361/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8347\n",
      "Epoch 362/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8342\n",
      "Epoch 363/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8336\n",
      "Epoch 364/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8331\n",
      "Epoch 365/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8326\n",
      "Epoch 366/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8320\n",
      "Epoch 367/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8315\n",
      "Epoch 368/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8309\n",
      "Epoch 369/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8304\n",
      "Epoch 370/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8298\n",
      "Epoch 371/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8293\n",
      "Epoch 372/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8288\n",
      "Epoch 373/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8282\n",
      "Epoch 374/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8277\n",
      "Epoch 375/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8271\n",
      "Epoch 376/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.8266\n",
      "Epoch 377/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8261\n",
      "Epoch 378/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8255\n",
      "Epoch 379/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8250\n",
      "Epoch 380/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8245\n",
      "Epoch 381/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.8239\n",
      "Epoch 382/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8234\n",
      "Epoch 383/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8229\n",
      "Epoch 384/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8223\n",
      "Epoch 385/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 386/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8213\n",
      "Epoch 387/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8208\n",
      "Epoch 388/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.8202\n",
      "Epoch 389/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8197\n",
      "Epoch 390/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8192\n",
      "Epoch 391/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8187\n",
      "Epoch 392/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8181\n",
      "Epoch 393/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8176\n",
      "Epoch 394/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8171\n",
      "Epoch 395/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8166\n",
      "Epoch 396/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8161\n",
      "Epoch 397/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8155\n",
      "Epoch 398/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8150\n",
      "Epoch 399/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8145\n",
      "Epoch 400/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8140\n",
      "Epoch 401/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.8135\n",
      "Epoch 402/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8130\n",
      "Epoch 403/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8125\n",
      "Epoch 404/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8120\n",
      "Epoch 405/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8115\n",
      "Epoch 406/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8110\n",
      "Epoch 407/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8106\n",
      "Epoch 408/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8101\n",
      "Epoch 409/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8096\n",
      "Epoch 410/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8092\n",
      "Epoch 411/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8087\n",
      "Epoch 412/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8082\n",
      "Epoch 413/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8077\n",
      "Epoch 414/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8073\n",
      "Epoch 415/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8068\n",
      "Epoch 416/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8064\n",
      "Epoch 417/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8059\n",
      "Epoch 418/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8055\n",
      "Epoch 419/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.8050\n",
      "Epoch 420/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8046\n",
      "Epoch 421/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.8041\n",
      "Epoch 422/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.8037\n",
      "Epoch 423/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.8032\n",
      "Epoch 424/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.8028\n",
      "Epoch 425/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8024\n",
      "Epoch 426/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.8019\n",
      "Epoch 427/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8015\n",
      "Epoch 428/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.8011\n",
      "Epoch 429/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8007\n",
      "Epoch 430/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.8002\n",
      "Epoch 431/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7998\n",
      "Epoch 432/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7994\n",
      "Epoch 433/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7990\n",
      "Epoch 434/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7986\n",
      "Epoch 435/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7982\n",
      "Epoch 436/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7978\n",
      "Epoch 437/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7974\n",
      "Epoch 438/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7970\n",
      "Epoch 439/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7966\n",
      "Epoch 440/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7962\n",
      "Epoch 441/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7958\n",
      "Epoch 442/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7954\n",
      "Epoch 443/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7950\n",
      "Epoch 444/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7946\n",
      "Epoch 445/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7942\n",
      "Epoch 446/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7939\n",
      "Epoch 447/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7935\n",
      "Epoch 448/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7931\n",
      "Epoch 449/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7927\n",
      "Epoch 450/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7924\n",
      "Epoch 451/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7920\n",
      "Epoch 452/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7916\n",
      "Epoch 453/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7913\n",
      "Epoch 454/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7909\n",
      "Epoch 455/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7906\n",
      "Epoch 456/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7903\n",
      "Epoch 457/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7899\n",
      "Epoch 458/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7896\n",
      "Epoch 459/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7892\n",
      "Epoch 460/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7889\n",
      "Epoch 461/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7886\n",
      "Epoch 462/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7883\n",
      "Epoch 463/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7879\n",
      "Epoch 464/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7876\n",
      "Epoch 465/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7873\n",
      "Epoch 466/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7870\n",
      "Epoch 467/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7867\n",
      "Epoch 468/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7864\n",
      "Epoch 469/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7861\n",
      "Epoch 470/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7858\n",
      "Epoch 471/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7856\n",
      "Epoch 472/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7853\n",
      "Epoch 473/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7850\n",
      "Epoch 474/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7847\n",
      "Epoch 475/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7846\n",
      "Epoch 476/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7844\n",
      "Epoch 477/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7845\n",
      "Epoch 478/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7842\n",
      "Epoch 479/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7836\n",
      "Epoch 480/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7833\n",
      "Epoch 481/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 482/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7829\n",
      "Epoch 483/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7827\n",
      "Epoch 484/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7825\n",
      "Epoch 485/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7822\n",
      "Epoch 486/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7821\n",
      "Epoch 487/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7818\n",
      "Epoch 488/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7816\n",
      "Epoch 489/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7814\n",
      "Epoch 490/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7812\n",
      "Epoch 491/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7810\n",
      "Epoch 492/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7808\n",
      "Epoch 493/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7806\n",
      "Epoch 494/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7804\n",
      "Epoch 495/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7802\n",
      "Epoch 496/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7803\n",
      "Epoch 497/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7804\n",
      "Epoch 498/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7804\n",
      "Epoch 499/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7800\n",
      "Epoch 500/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7794\n",
      "Epoch 501/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7794\n",
      "Epoch 502/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7796\n",
      "Epoch 503/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7793\n",
      "Epoch 504/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7788\n",
      "Epoch 505/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7786\n",
      "Epoch 506/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7787\n",
      "Epoch 507/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7786\n",
      "Epoch 508/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7783\n",
      "Epoch 509/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7780\n",
      "Epoch 510/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7780\n",
      "Epoch 511/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7779\n",
      "Epoch 512/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7778\n",
      "Epoch 513/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7778\n",
      "Epoch 514/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7776\n",
      "Epoch 515/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7773\n",
      "Epoch 516/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7771\n",
      "Epoch 517/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7772\n",
      "Epoch 518/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7775\n",
      "Epoch 519/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7777\n",
      "Epoch 520/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7770\n",
      "Epoch 521/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7766\n",
      "Epoch 522/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7767\n",
      "Epoch 523/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7769\n",
      "Epoch 524/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7767\n",
      "Epoch 525/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7762\n",
      "Epoch 526/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7761\n",
      "Epoch 527/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7762\n",
      "Epoch 528/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7762\n",
      "Epoch 529/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7759\n",
      "Epoch 530/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7757\n",
      "Epoch 531/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7756\n",
      "Epoch 532/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7757\n",
      "Epoch 533/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7760\n",
      "Epoch 534/3000\n",
      "379/379 [==============================] - 0s 10us/step - loss: 2.7761\n",
      "Epoch 535/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7756\n",
      "Epoch 536/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7752\n",
      "Epoch 537/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7752\n",
      "Epoch 538/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7754\n",
      "Epoch 539/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7754\n",
      "Epoch 540/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7750\n",
      "Epoch 541/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7748\n",
      "Epoch 542/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7749\n",
      "Epoch 543/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7749\n",
      "Epoch 544/3000\n",
      "379/379 [==============================] - 0s 10us/step - loss: 2.7749\n",
      "Epoch 545/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7749\n",
      "Epoch 546/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7747\n",
      "Epoch 547/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7745\n",
      "Epoch 548/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7744\n",
      "Epoch 549/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7744\n",
      "Epoch 550/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7744\n",
      "Epoch 551/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7743\n",
      "Epoch 552/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7742\n",
      "Epoch 553/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7741\n",
      "Epoch 554/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7741\n",
      "Epoch 555/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7741\n",
      "Epoch 556/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7741\n",
      "Epoch 557/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7741\n",
      "Epoch 558/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7740\n",
      "Epoch 559/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7740\n",
      "Epoch 560/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7740\n",
      "Epoch 561/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7739\n",
      "Epoch 562/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7737\n",
      "Epoch 563/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7737\n",
      "Epoch 564/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7737\n",
      "Epoch 565/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7737\n",
      "Epoch 566/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7737\n",
      "Epoch 567/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7738\n",
      "Epoch 568/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7737\n",
      "Epoch 569/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7736\n",
      "Epoch 570/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7735\n",
      "Epoch 571/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7734\n",
      "Epoch 572/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7733\n",
      "Epoch 573/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7733\n",
      "Epoch 574/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7733\n",
      "Epoch 575/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7732\n",
      "Epoch 576/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7732\n",
      "Epoch 577/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 578/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7731\n",
      "Epoch 579/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7731\n",
      "Epoch 580/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7731\n",
      "Epoch 581/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7730\n",
      "Epoch 582/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7731\n",
      "Epoch 583/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7733\n",
      "Epoch 584/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7740\n",
      "Epoch 585/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7750\n",
      "Epoch 586/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7761\n",
      "Epoch 587/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7750\n",
      "Epoch 588/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7737\n",
      "Epoch 589/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7729\n",
      "Epoch 590/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7737\n",
      "Epoch 591/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7749\n",
      "Epoch 592/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7738\n",
      "Epoch 593/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7729\n",
      "Epoch 594/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7730\n",
      "Epoch 595/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7736\n",
      "Epoch 596/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7736\n",
      "Epoch 597/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7728\n",
      "Epoch 598/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7728\n",
      "Epoch 599/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7733\n",
      "Epoch 600/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7731\n",
      "Epoch 601/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7728\n",
      "Epoch 602/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7726\n",
      "Epoch 603/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7729\n",
      "Epoch 604/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7730\n",
      "Epoch 605/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7727\n",
      "Epoch 606/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7725\n",
      "Epoch 607/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7726\n",
      "Epoch 608/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7727\n",
      "Epoch 609/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7727\n",
      "Epoch 610/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7725\n",
      "Epoch 611/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7724\n",
      "Epoch 612/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7725\n",
      "Epoch 613/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7726\n",
      "Epoch 614/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7725\n",
      "Epoch 615/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7724\n",
      "Epoch 616/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7724\n",
      "Epoch 617/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7724\n",
      "Epoch 618/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7724\n",
      "Epoch 619/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7724\n",
      "Epoch 620/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7723\n",
      "Epoch 621/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7723\n",
      "Epoch 622/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7723\n",
      "Epoch 623/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7723\n",
      "Epoch 624/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7722\n",
      "Epoch 625/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7722\n",
      "Epoch 626/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7722\n",
      "Epoch 627/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7722\n",
      "Epoch 628/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7722\n",
      "Epoch 629/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7722\n",
      "Epoch 630/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7721\n",
      "Epoch 631/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7721\n",
      "Epoch 632/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7721\n",
      "Epoch 633/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7721\n",
      "Epoch 634/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7721\n",
      "Epoch 635/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7721\n",
      "Epoch 636/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7720\n",
      "Epoch 637/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7720\n",
      "Epoch 638/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7720\n",
      "Epoch 639/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7720\n",
      "Epoch 640/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7720\n",
      "Epoch 641/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7720\n",
      "Epoch 642/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7720\n",
      "Epoch 643/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7719\n",
      "Epoch 644/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7719\n",
      "Epoch 645/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7719\n",
      "Epoch 646/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7719\n",
      "Epoch 647/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7719\n",
      "Epoch 648/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7719\n",
      "Epoch 649/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7719\n",
      "Epoch 650/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7718\n",
      "Epoch 651/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7718\n",
      "Epoch 652/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7718\n",
      "Epoch 653/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7718\n",
      "Epoch 654/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7718\n",
      "Epoch 655/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7718\n",
      "Epoch 656/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7718\n",
      "Epoch 657/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7717\n",
      "Epoch 658/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7717\n",
      "Epoch 659/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7717\n",
      "Epoch 660/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7717\n",
      "Epoch 661/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7717\n",
      "Epoch 662/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7717\n",
      "Epoch 663/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7717\n",
      "Epoch 664/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7717\n",
      "Epoch 665/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7716\n",
      "Epoch 666/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7717\n",
      "Epoch 667/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7717\n",
      "Epoch 668/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7717\n",
      "Epoch 669/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7717\n",
      "Epoch 670/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7716\n",
      "Epoch 671/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7716\n",
      "Epoch 672/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7716\n",
      "Epoch 673/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 674/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7715\n",
      "Epoch 675/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7716\n",
      "Epoch 676/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7716\n",
      "Epoch 677/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7715\n",
      "Epoch 678/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7715\n",
      "Epoch 679/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7715\n",
      "Epoch 680/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7715\n",
      "Epoch 681/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7715\n",
      "Epoch 682/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7715\n",
      "Epoch 683/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7715\n",
      "Epoch 684/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7715\n",
      "Epoch 685/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7715\n",
      "Epoch 686/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7714\n",
      "Epoch 687/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7714\n",
      "Epoch 688/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7714\n",
      "Epoch 689/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7714\n",
      "Epoch 690/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7713\n",
      "Epoch 691/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7713\n",
      "Epoch 692/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7713\n",
      "Epoch 693/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7713\n",
      "Epoch 694/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7713\n",
      "Epoch 695/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7713\n",
      "Epoch 696/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7714\n",
      "Epoch 697/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7714\n",
      "Epoch 698/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7714\n",
      "Epoch 699/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7714\n",
      "Epoch 700/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7714\n",
      "Epoch 701/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7713\n",
      "Epoch 702/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7713\n",
      "Epoch 703/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7712\n",
      "Epoch 704/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7712\n",
      "Epoch 705/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7712\n",
      "Epoch 706/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7712\n",
      "Epoch 707/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7712\n",
      "Epoch 708/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7712\n",
      "Epoch 709/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7712\n",
      "Epoch 710/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7711\n",
      "Epoch 711/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7711\n",
      "Epoch 712/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7711\n",
      "Epoch 713/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7711\n",
      "Epoch 714/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7711\n",
      "Epoch 715/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7711\n",
      "Epoch 716/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7712\n",
      "Epoch 717/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7712\n",
      "Epoch 718/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7712\n",
      "Epoch 719/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7712\n",
      "Epoch 720/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7711\n",
      "Epoch 721/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7710\n",
      "Epoch 722/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7710\n",
      "Epoch 723/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7710\n",
      "Epoch 724/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7710\n",
      "Epoch 725/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7710\n",
      "Epoch 726/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7710\n",
      "Epoch 727/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7710\n",
      "Epoch 728/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7710\n",
      "Epoch 729/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7711\n",
      "Epoch 730/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7711\n",
      "Epoch 731/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7711\n",
      "Epoch 732/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7712\n",
      "Epoch 733/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7711\n",
      "Epoch 734/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7711\n",
      "Epoch 735/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7710\n",
      "Epoch 736/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7710\n",
      "Epoch 737/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7709\n",
      "Epoch 738/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7709\n",
      "Epoch 739/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7708\n",
      "Epoch 740/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7707\n",
      "Epoch 741/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7707\n",
      "Epoch 742/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7708\n",
      "Epoch 743/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7708\n",
      "Epoch 744/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7708\n",
      "Epoch 745/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7708\n",
      "Epoch 746/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7707\n",
      "Epoch 747/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7707\n",
      "Epoch 748/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7707\n",
      "Epoch 749/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7706\n",
      "Epoch 750/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7706\n",
      "Epoch 751/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7706\n",
      "Epoch 752/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7706\n",
      "Epoch 753/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7707\n",
      "Epoch 754/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7707\n",
      "Epoch 755/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7709\n",
      "Epoch 756/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7709\n",
      "Epoch 757/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7710\n",
      "Epoch 758/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7714\n",
      "Epoch 759/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7719\n",
      "Epoch 760/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7716\n",
      "Epoch 761/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7712\n",
      "Epoch 762/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7707\n",
      "Epoch 763/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7704\n",
      "Epoch 764/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7706\n",
      "Epoch 765/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7708\n",
      "Epoch 766/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7710\n",
      "Epoch 767/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7708\n",
      "Epoch 768/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7706\n",
      "Epoch 769/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 770/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7703\n",
      "Epoch 771/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7705\n",
      "Epoch 772/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7706\n",
      "Epoch 773/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7707\n",
      "Epoch 774/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7706\n",
      "Epoch 775/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7705\n",
      "Epoch 776/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7703\n",
      "Epoch 777/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7702\n",
      "Epoch 778/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7702\n",
      "Epoch 779/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7702\n",
      "Epoch 780/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7702\n",
      "Epoch 781/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7702\n",
      "Epoch 782/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7702\n",
      "Epoch 783/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7702\n",
      "Epoch 784/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7701\n",
      "Epoch 785/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7701\n",
      "Epoch 786/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7700\n",
      "Epoch 787/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7701\n",
      "Epoch 788/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7701\n",
      "Epoch 789/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7701\n",
      "Epoch 790/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7701\n",
      "Epoch 791/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7701\n",
      "Epoch 792/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7700\n",
      "Epoch 793/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7700\n",
      "Epoch 794/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7700\n",
      "Epoch 795/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7699\n",
      "Epoch 796/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7699\n",
      "Epoch 797/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7698\n",
      "Epoch 798/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7699\n",
      "Epoch 799/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7698\n",
      "Epoch 800/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7698\n",
      "Epoch 801/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7698\n",
      "Epoch 802/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7697\n",
      "Epoch 803/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7697\n",
      "Epoch 804/3000\n",
      "379/379 [==============================] - 0s 24us/step - loss: 2.7697\n",
      "Epoch 805/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7697\n",
      "Epoch 806/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7697\n",
      "Epoch 807/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7698\n",
      "Epoch 808/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7698\n",
      "Epoch 809/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7699\n",
      "Epoch 810/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7698\n",
      "Epoch 811/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7698\n",
      "Epoch 812/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7697\n",
      "Epoch 813/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7695\n",
      "Epoch 814/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7694\n",
      "Epoch 815/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7694\n",
      "Epoch 816/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7694\n",
      "Epoch 817/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7695\n",
      "Epoch 818/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7695\n",
      "Epoch 819/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7694\n",
      "Epoch 820/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7693\n",
      "Epoch 821/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7693\n",
      "Epoch 822/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7692\n",
      "Epoch 823/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7692\n",
      "Epoch 824/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7692\n",
      "Epoch 825/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7692\n",
      "Epoch 826/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7693\n",
      "Epoch 827/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7694\n",
      "Epoch 828/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7694\n",
      "Epoch 829/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7694\n",
      "Epoch 830/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7693\n",
      "Epoch 831/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7693\n",
      "Epoch 832/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7691\n",
      "Epoch 833/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7690\n",
      "Epoch 834/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7689\n",
      "Epoch 835/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7688\n",
      "Epoch 836/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7688\n",
      "Epoch 837/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7688\n",
      "Epoch 838/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7689\n",
      "Epoch 839/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7688\n",
      "Epoch 840/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7687\n",
      "Epoch 841/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7686\n",
      "Epoch 842/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7686\n",
      "Epoch 843/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7685\n",
      "Epoch 844/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7685\n",
      "Epoch 845/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7685\n",
      "Epoch 846/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7685\n",
      "Epoch 847/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7686\n",
      "Epoch 848/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7687\n",
      "Epoch 849/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7688\n",
      "Epoch 850/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7687\n",
      "Epoch 851/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7687\n",
      "Epoch 852/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7685\n",
      "Epoch 853/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7682\n",
      "Epoch 854/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7680\n",
      "Epoch 855/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7679\n",
      "Epoch 856/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7679\n",
      "Epoch 857/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7679\n",
      "Epoch 858/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7680\n",
      "Epoch 859/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7679\n",
      "Epoch 860/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7679\n",
      "Epoch 861/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7677\n",
      "Epoch 862/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7676\n",
      "Epoch 863/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7675\n",
      "Epoch 864/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7674\n",
      "Epoch 865/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 866/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7674\n",
      "Epoch 867/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7674\n",
      "Epoch 868/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7674\n",
      "Epoch 869/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7675\n",
      "Epoch 870/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7674\n",
      "Epoch 871/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7675\n",
      "Epoch 872/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7673\n",
      "Epoch 873/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7673\n",
      "Epoch 874/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7670\n",
      "Epoch 875/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7668\n",
      "Epoch 876/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7667\n",
      "Epoch 877/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7665\n",
      "Epoch 878/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7665\n",
      "Epoch 879/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7665\n",
      "Epoch 880/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7665\n",
      "Epoch 881/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7664\n",
      "Epoch 882/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7663\n",
      "Epoch 883/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7662\n",
      "Epoch 884/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7660\n",
      "Epoch 885/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7659\n",
      "Epoch 886/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7658\n",
      "Epoch 887/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7657\n",
      "Epoch 888/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7656\n",
      "Epoch 889/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7656\n",
      "Epoch 890/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7655\n",
      "Epoch 891/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7656\n",
      "Epoch 892/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7655\n",
      "Epoch 893/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7656\n",
      "Epoch 894/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7655\n",
      "Epoch 895/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7655\n",
      "Epoch 896/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7652\n",
      "Epoch 897/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7649\n",
      "Epoch 898/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7646\n",
      "Epoch 899/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7643\n",
      "Epoch 900/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7641\n",
      "Epoch 901/3000\n",
      "379/379 [==============================] - 0s 23us/step - loss: 2.7640\n",
      "Epoch 902/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7640\n",
      "Epoch 903/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7640\n",
      "Epoch 904/3000\n",
      "379/379 [==============================] - 0s 19us/step - loss: 2.7640\n",
      "Epoch 905/3000\n",
      "379/379 [==============================] - 0s 22us/step - loss: 2.7638\n",
      "Epoch 906/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7637\n",
      "Epoch 907/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7635\n",
      "Epoch 908/3000\n",
      "379/379 [==============================] - 0s 20us/step - loss: 2.7634\n",
      "Epoch 909/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7633\n",
      "Epoch 910/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7631\n",
      "Epoch 911/3000\n",
      "379/379 [==============================] - 0s 21us/step - loss: 2.7630\n",
      "Epoch 912/3000\n",
      "379/379 [==============================] - 0s 23us/step - loss: 2.7630\n",
      "Epoch 913/3000\n",
      "379/379 [==============================] - 0s 26us/step - loss: 2.7629\n",
      "Epoch 914/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7629\n",
      "Epoch 915/3000\n",
      "379/379 [==============================] - 0s 23us/step - loss: 2.7630\n",
      "Epoch 916/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7634\n",
      "Epoch 917/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7642\n",
      "Epoch 918/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7648\n",
      "Epoch 919/3000\n",
      "379/379 [==============================] - 0s 22us/step - loss: 2.7661\n",
      "Epoch 920/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7649\n",
      "Epoch 921/3000\n",
      "379/379 [==============================] - 0s 20us/step - loss: 2.7638\n",
      "Epoch 922/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7624\n",
      "Epoch 923/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7624\n",
      "Epoch 924/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7633\n",
      "Epoch 925/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7634\n",
      "Epoch 926/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7631\n",
      "Epoch 927/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7620\n",
      "Epoch 928/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7617\n",
      "Epoch 929/3000\n",
      "379/379 [==============================] - 0s 20us/step - loss: 2.7621\n",
      "Epoch 930/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7623\n",
      "Epoch 931/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7621\n",
      "Epoch 932/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7614\n",
      "Epoch 933/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7611\n",
      "Epoch 934/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7612\n",
      "Epoch 935/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7614\n",
      "Epoch 936/3000\n",
      "379/379 [==============================] - 0s 24us/step - loss: 2.7613\n",
      "Epoch 937/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7609\n",
      "Epoch 938/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7606\n",
      "Epoch 939/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7606\n",
      "Epoch 940/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7607\n",
      "Epoch 941/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7607\n",
      "Epoch 942/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7606\n",
      "Epoch 943/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7604\n",
      "Epoch 944/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7602\n",
      "Epoch 945/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7601\n",
      "Epoch 946/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7602\n",
      "Epoch 947/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7601\n",
      "Epoch 948/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7601\n",
      "Epoch 949/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7599\n",
      "Epoch 950/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7597\n",
      "Epoch 951/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7596\n",
      "Epoch 952/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7596\n",
      "Epoch 953/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7596\n",
      "Epoch 954/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7595\n",
      "Epoch 955/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7594\n",
      "Epoch 956/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7593\n",
      "Epoch 957/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7592\n",
      "Epoch 958/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7592\n",
      "Epoch 959/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7591\n",
      "Epoch 960/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7591\n",
      "Epoch 961/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 962/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7589\n",
      "Epoch 963/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7588\n",
      "Epoch 964/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7587\n",
      "Epoch 965/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7587\n",
      "Epoch 966/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7586\n",
      "Epoch 967/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7585\n",
      "Epoch 968/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7585\n",
      "Epoch 969/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7584\n",
      "Epoch 970/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7583\n",
      "Epoch 971/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7583\n",
      "Epoch 972/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7582\n",
      "Epoch 973/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7581\n",
      "Epoch 974/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7581\n",
      "Epoch 975/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7580\n",
      "Epoch 976/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7580\n",
      "Epoch 977/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7580\n",
      "Epoch 978/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7579\n",
      "Epoch 979/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7578\n",
      "Epoch 980/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7577\n",
      "Epoch 981/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7576\n",
      "Epoch 982/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7576\n",
      "Epoch 983/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7575\n",
      "Epoch 984/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7575\n",
      "Epoch 985/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7574\n",
      "Epoch 986/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7574\n",
      "Epoch 987/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7574\n",
      "Epoch 988/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7573\n",
      "Epoch 989/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7572\n",
      "Epoch 990/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7572\n",
      "Epoch 991/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7571\n",
      "Epoch 992/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7570\n",
      "Epoch 993/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7569\n",
      "Epoch 994/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7569\n",
      "Epoch 995/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7568\n",
      "Epoch 996/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7568\n",
      "Epoch 997/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7567\n",
      "Epoch 998/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7567\n",
      "Epoch 999/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7567\n",
      "Epoch 1000/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7567\n",
      "Epoch 1001/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7567\n",
      "Epoch 1002/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7568\n",
      "Epoch 1003/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7568\n",
      "Epoch 1004/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7568\n",
      "Epoch 1005/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7566\n",
      "Epoch 1006/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7566\n",
      "Epoch 1007/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7564\n",
      "Epoch 1008/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7563\n",
      "Epoch 1009/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7561\n",
      "Epoch 1010/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7560\n",
      "Epoch 1011/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7559\n",
      "Epoch 1012/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7558\n",
      "Epoch 1013/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7558\n",
      "Epoch 1014/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7558\n",
      "Epoch 1015/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7558\n",
      "Epoch 1016/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7557\n",
      "Epoch 1017/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7558\n",
      "Epoch 1018/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7558\n",
      "Epoch 1019/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7560\n",
      "Epoch 1020/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7561\n",
      "Epoch 1021/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7563\n",
      "Epoch 1022/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7562\n",
      "Epoch 1023/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7563\n",
      "Epoch 1024/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7560\n",
      "Epoch 1025/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7559\n",
      "Epoch 1026/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7555\n",
      "Epoch 1027/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7552\n",
      "Epoch 1028/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7550\n",
      "Epoch 1029/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7550\n",
      "Epoch 1030/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7549\n",
      "Epoch 1031/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7550\n",
      "Epoch 1032/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7551\n",
      "Epoch 1033/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7551\n",
      "Epoch 1034/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7551\n",
      "Epoch 1035/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7551\n",
      "Epoch 1036/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7551\n",
      "Epoch 1037/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7549\n",
      "Epoch 1038/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7548\n",
      "Epoch 1039/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7546\n",
      "Epoch 1040/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7545\n",
      "Epoch 1041/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7544\n",
      "Epoch 1042/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7543\n",
      "Epoch 1043/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7542\n",
      "Epoch 1044/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7542\n",
      "Epoch 1045/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7542\n",
      "Epoch 1046/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7542\n",
      "Epoch 1047/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7542\n",
      "Epoch 1048/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7542\n",
      "Epoch 1049/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7542\n",
      "Epoch 1050/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7541\n",
      "Epoch 1051/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7540\n",
      "Epoch 1052/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7540\n",
      "Epoch 1053/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7539\n",
      "Epoch 1054/3000\n",
      "379/379 [==============================] - 0s 21us/step - loss: 2.7538\n",
      "Epoch 1055/3000\n",
      "379/379 [==============================] - 0s 22us/step - loss: 2.7538\n",
      "Epoch 1056/3000\n",
      "379/379 [==============================] - 0s 44us/step - loss: 2.7537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1057/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7536\n",
      "Epoch 1058/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7536\n",
      "Epoch 1059/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7535\n",
      "Epoch 1060/3000\n",
      "379/379 [==============================] - 0s 27us/step - loss: 2.7534\n",
      "Epoch 1061/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7534\n",
      "Epoch 1062/3000\n",
      "379/379 [==============================] - 0s 20us/step - loss: 2.7533\n",
      "Epoch 1063/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7533\n",
      "Epoch 1064/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7532\n",
      "Epoch 1065/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7532\n",
      "Epoch 1066/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7531\n",
      "Epoch 1067/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7531\n",
      "Epoch 1068/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7531\n",
      "Epoch 1069/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7531\n",
      "Epoch 1070/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7531\n",
      "Epoch 1071/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7532\n",
      "Epoch 1072/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7533\n",
      "Epoch 1073/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7535\n",
      "Epoch 1074/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7537\n",
      "Epoch 1075/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7544\n",
      "Epoch 1076/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7547\n",
      "Epoch 1077/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7559\n",
      "Epoch 1078/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7553\n",
      "Epoch 1079/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7556\n",
      "Epoch 1080/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7539\n",
      "Epoch 1081/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7529\n",
      "Epoch 1082/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7524\n",
      "Epoch 1083/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7527\n",
      "Epoch 1084/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7534\n",
      "Epoch 1085/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7536\n",
      "Epoch 1086/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7537\n",
      "Epoch 1087/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7529\n",
      "Epoch 1088/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7523\n",
      "Epoch 1089/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7521\n",
      "Epoch 1090/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7523\n",
      "Epoch 1091/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7527\n",
      "Epoch 1092/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7527\n",
      "Epoch 1093/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7525\n",
      "Epoch 1094/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7521\n",
      "Epoch 1095/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7518\n",
      "Epoch 1096/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7518\n",
      "Epoch 1097/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7520\n",
      "Epoch 1098/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7521\n",
      "Epoch 1099/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7520\n",
      "Epoch 1100/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7518\n",
      "Epoch 1101/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7516\n",
      "Epoch 1102/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7515\n",
      "Epoch 1103/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7515\n",
      "Epoch 1104/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7516\n",
      "Epoch 1105/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7516\n",
      "Epoch 1106/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7515\n",
      "Epoch 1107/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7514\n",
      "Epoch 1108/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7513\n",
      "Epoch 1109/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7512\n",
      "Epoch 1110/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7512\n",
      "Epoch 1111/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7512\n",
      "Epoch 1112/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7512\n",
      "Epoch 1113/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7511\n",
      "Epoch 1114/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7511\n",
      "Epoch 1115/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7510\n",
      "Epoch 1116/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7509\n",
      "Epoch 1117/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7509\n",
      "Epoch 1118/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7508\n",
      "Epoch 1119/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7508\n",
      "Epoch 1120/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7507\n",
      "Epoch 1121/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7507\n",
      "Epoch 1122/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7506\n",
      "Epoch 1123/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7506\n",
      "Epoch 1124/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7505\n",
      "Epoch 1125/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7504\n",
      "Epoch 1126/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7504\n",
      "Epoch 1127/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7503\n",
      "Epoch 1128/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7503\n",
      "Epoch 1129/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7502\n",
      "Epoch 1130/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7502\n",
      "Epoch 1131/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7501\n",
      "Epoch 1132/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7501\n",
      "Epoch 1133/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7500\n",
      "Epoch 1134/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7500\n",
      "Epoch 1135/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7499\n",
      "Epoch 1136/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7499\n",
      "Epoch 1137/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7498\n",
      "Epoch 1138/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7498\n",
      "Epoch 1139/3000\n",
      "379/379 [==============================] - 0s 19us/step - loss: 2.7497\n",
      "Epoch 1140/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7497\n",
      "Epoch 1141/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7496\n",
      "Epoch 1142/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7496\n",
      "Epoch 1143/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7495\n",
      "Epoch 1144/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7495\n",
      "Epoch 1145/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7495\n",
      "Epoch 1146/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7494\n",
      "Epoch 1147/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7494\n",
      "Epoch 1148/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7493\n",
      "Epoch 1149/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7493\n",
      "Epoch 1150/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7493\n",
      "Epoch 1151/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s 14us/step - loss: 2.7493\n",
      "Epoch 1152/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7492\n",
      "Epoch 1153/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7492\n",
      "Epoch 1154/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7493\n",
      "Epoch 1155/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7494\n",
      "Epoch 1156/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7496\n",
      "Epoch 1157/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7498\n",
      "Epoch 1158/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7505\n",
      "Epoch 1159/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7508\n",
      "Epoch 1160/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7520\n",
      "Epoch 1161/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7515\n",
      "Epoch 1162/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7519\n",
      "Epoch 1163/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7503\n",
      "Epoch 1164/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7493\n",
      "Epoch 1165/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7486\n",
      "Epoch 1166/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7487\n",
      "Epoch 1167/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7493\n",
      "Epoch 1168/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7497\n",
      "Epoch 1169/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7501\n",
      "Epoch 1170/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7494\n",
      "Epoch 1171/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7488\n",
      "Epoch 1172/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7483\n",
      "Epoch 1173/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7483\n",
      "Epoch 1174/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7486\n",
      "Epoch 1175/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7488\n",
      "Epoch 1176/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7489\n",
      "Epoch 1177/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7486\n",
      "Epoch 1178/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7483\n",
      "Epoch 1179/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7480\n",
      "Epoch 1180/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7480\n",
      "Epoch 1181/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7481\n",
      "Epoch 1182/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7482\n",
      "Epoch 1183/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7482\n",
      "Epoch 1184/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7480\n",
      "Epoch 1185/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7478\n",
      "Epoch 1186/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7477\n",
      "Epoch 1187/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7476\n",
      "Epoch 1188/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7477\n",
      "Epoch 1189/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7477\n",
      "Epoch 1190/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7477\n",
      "Epoch 1191/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7476\n",
      "Epoch 1192/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7475\n",
      "Epoch 1193/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7474\n",
      "Epoch 1194/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7474\n",
      "Epoch 1195/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7473\n",
      "Epoch 1196/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7472\n",
      "Epoch 1197/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7472\n",
      "Epoch 1198/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7472\n",
      "Epoch 1199/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7472\n",
      "Epoch 1200/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7471\n",
      "Epoch 1201/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7471\n",
      "Epoch 1202/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7470\n",
      "Epoch 1203/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7470\n",
      "Epoch 1204/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7469\n",
      "Epoch 1205/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7468\n",
      "Epoch 1206/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7468\n",
      "Epoch 1207/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7468\n",
      "Epoch 1208/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7467\n",
      "Epoch 1209/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7467\n",
      "Epoch 1210/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7467\n",
      "Epoch 1211/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7466\n",
      "Epoch 1212/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7466\n",
      "Epoch 1213/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7465\n",
      "Epoch 1214/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7465\n",
      "Epoch 1215/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7464\n",
      "Epoch 1216/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7464\n",
      "Epoch 1217/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7463\n",
      "Epoch 1218/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7463\n",
      "Epoch 1219/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7462\n",
      "Epoch 1220/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7462\n",
      "Epoch 1221/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7462\n",
      "Epoch 1222/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7461\n",
      "Epoch 1223/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7461\n",
      "Epoch 1224/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7461\n",
      "Epoch 1225/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7461\n",
      "Epoch 1226/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7461\n",
      "Epoch 1227/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7462\n",
      "Epoch 1228/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7462\n",
      "Epoch 1229/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7464\n",
      "Epoch 1230/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7465\n",
      "Epoch 1231/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7468\n",
      "Epoch 1232/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7468\n",
      "Epoch 1233/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7474\n",
      "Epoch 1234/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7473\n",
      "Epoch 1235/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7478\n",
      "Epoch 1236/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7472\n",
      "Epoch 1237/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7469\n",
      "Epoch 1238/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7462\n",
      "Epoch 1239/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7457\n",
      "Epoch 1240/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7454\n",
      "Epoch 1241/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7453\n",
      "Epoch 1242/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7455\n",
      "Epoch 1243/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7456\n",
      "Epoch 1244/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7459\n",
      "Epoch 1245/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1246/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7458\n",
      "Epoch 1247/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7455\n",
      "Epoch 1248/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7452\n",
      "Epoch 1249/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7450\n",
      "Epoch 1250/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7449\n",
      "Epoch 1251/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7449\n",
      "Epoch 1252/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7450\n",
      "Epoch 1253/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7450\n",
      "Epoch 1254/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7450\n",
      "Epoch 1255/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7451\n",
      "Epoch 1256/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7450\n",
      "Epoch 1257/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7449\n",
      "Epoch 1258/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7447\n",
      "Epoch 1259/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7446\n",
      "Epoch 1260/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7446\n",
      "Epoch 1261/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7445\n",
      "Epoch 1262/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7444\n",
      "Epoch 1263/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7444\n",
      "Epoch 1264/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7444\n",
      "Epoch 1265/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7444\n",
      "Epoch 1266/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7444\n",
      "Epoch 1267/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7444\n",
      "Epoch 1268/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7445\n",
      "Epoch 1269/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7446\n",
      "Epoch 1270/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7447\n",
      "Epoch 1271/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7447\n",
      "Epoch 1272/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7447\n",
      "Epoch 1273/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7445\n",
      "Epoch 1274/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7445\n",
      "Epoch 1275/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7443\n",
      "Epoch 1276/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7441\n",
      "Epoch 1277/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7440\n",
      "Epoch 1278/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7439\n",
      "Epoch 1279/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7438\n",
      "Epoch 1280/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7437\n",
      "Epoch 1281/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7436\n",
      "Epoch 1282/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7436\n",
      "Epoch 1283/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7435\n",
      "Epoch 1284/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7435\n",
      "Epoch 1285/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7435\n",
      "Epoch 1286/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7435\n",
      "Epoch 1287/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7435\n",
      "Epoch 1288/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7435\n",
      "Epoch 1289/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7435\n",
      "Epoch 1290/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7435\n",
      "Epoch 1291/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7437\n",
      "Epoch 1292/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7438\n",
      "Epoch 1293/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7442\n",
      "Epoch 1294/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7443\n",
      "Epoch 1295/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7450\n",
      "Epoch 1296/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7449\n",
      "Epoch 1297/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7455\n",
      "Epoch 1298/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7447\n",
      "Epoch 1299/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7446\n",
      "Epoch 1300/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7437\n",
      "Epoch 1301/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7431\n",
      "Epoch 1302/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7427\n",
      "Epoch 1303/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7427\n",
      "Epoch 1304/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7429\n",
      "Epoch 1305/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7431\n",
      "Epoch 1306/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7433\n",
      "Epoch 1307/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7431\n",
      "Epoch 1308/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7431\n",
      "Epoch 1309/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7427\n",
      "Epoch 1310/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7424\n",
      "Epoch 1311/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7423\n",
      "Epoch 1312/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7422\n",
      "Epoch 1313/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7423\n",
      "Epoch 1314/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7424\n",
      "Epoch 1315/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7425\n",
      "Epoch 1316/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7425\n",
      "Epoch 1317/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7426\n",
      "Epoch 1318/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7425\n",
      "Epoch 1319/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7424\n",
      "Epoch 1320/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7422\n",
      "Epoch 1321/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7421\n",
      "Epoch 1322/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7420\n",
      "Epoch 1323/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7418\n",
      "Epoch 1324/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7417\n",
      "Epoch 1325/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7417\n",
      "Epoch 1326/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7416\n",
      "Epoch 1327/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7416\n",
      "Epoch 1328/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7416\n",
      "Epoch 1329/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7416\n",
      "Epoch 1330/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7415\n",
      "Epoch 1331/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7415\n",
      "Epoch 1332/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7415\n",
      "Epoch 1333/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7415\n",
      "Epoch 1334/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7414\n",
      "Epoch 1335/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7413\n",
      "Epoch 1336/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7413\n",
      "Epoch 1337/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7412\n",
      "Epoch 1338/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7412\n",
      "Epoch 1339/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7413\n",
      "Epoch 1340/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s 12us/step - loss: 2.7414\n",
      "Epoch 1341/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7415\n",
      "Epoch 1342/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7416\n",
      "Epoch 1343/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7417\n",
      "Epoch 1344/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7419\n",
      "Epoch 1345/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7419\n",
      "Epoch 1346/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7421\n",
      "Epoch 1347/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7417\n",
      "Epoch 1348/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7416\n",
      "Epoch 1349/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7411\n",
      "Epoch 1350/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7409\n",
      "Epoch 1351/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7406\n",
      "Epoch 1352/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7405\n",
      "Epoch 1353/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7404\n",
      "Epoch 1354/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7404\n",
      "Epoch 1355/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7405\n",
      "Epoch 1356/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7405\n",
      "Epoch 1357/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7406\n",
      "Epoch 1358/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7407\n",
      "Epoch 1359/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7409\n",
      "Epoch 1360/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7409\n",
      "Epoch 1361/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7410\n",
      "Epoch 1362/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7409\n",
      "Epoch 1363/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7408\n",
      "Epoch 1364/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7405\n",
      "Epoch 1365/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7403\n",
      "Epoch 1366/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7401\n",
      "Epoch 1367/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7399\n",
      "Epoch 1368/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7398\n",
      "Epoch 1369/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7397\n",
      "Epoch 1370/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7397\n",
      "Epoch 1371/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7397\n",
      "Epoch 1372/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7397\n",
      "Epoch 1373/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7397\n",
      "Epoch 1374/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7397\n",
      "Epoch 1375/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7397\n",
      "Epoch 1376/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7398\n",
      "Epoch 1377/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7398\n",
      "Epoch 1378/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7400\n",
      "Epoch 1379/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7399\n",
      "Epoch 1380/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7402\n",
      "Epoch 1381/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7401\n",
      "Epoch 1382/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7403\n",
      "Epoch 1383/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7401\n",
      "Epoch 1384/3000\n",
      "379/379 [==============================] - 0s 21us/step - loss: 2.7400\n",
      "Epoch 1385/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7397\n",
      "Epoch 1386/3000\n",
      "379/379 [==============================] - 0s 21us/step - loss: 2.7395\n",
      "Epoch 1387/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7393\n",
      "Epoch 1388/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7391\n",
      "Epoch 1389/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7389\n",
      "Epoch 1390/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7388\n",
      "Epoch 1391/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7387\n",
      "Epoch 1392/3000\n",
      "379/379 [==============================] - 0s 25us/step - loss: 2.7388\n",
      "Epoch 1393/3000\n",
      "379/379 [==============================] - 0s 30us/step - loss: 2.7388\n",
      "Epoch 1394/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7388\n",
      "Epoch 1395/3000\n",
      "379/379 [==============================] - 0s 26us/step - loss: 2.7388\n",
      "Epoch 1396/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7388\n",
      "Epoch 1397/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7389\n",
      "Epoch 1398/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7389\n",
      "Epoch 1399/3000\n",
      "379/379 [==============================] - 0s 19us/step - loss: 2.7390\n",
      "Epoch 1400/3000\n",
      "379/379 [==============================] - 0s 20us/step - loss: 2.7391\n",
      "Epoch 1401/3000\n",
      "379/379 [==============================] - 0s 20us/step - loss: 2.7394\n",
      "Epoch 1402/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7395\n",
      "Epoch 1403/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7401\n",
      "Epoch 1404/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7399\n",
      "Epoch 1405/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7401\n",
      "Epoch 1406/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7394\n",
      "Epoch 1407/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7391\n",
      "Epoch 1408/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7384\n",
      "Epoch 1409/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7380\n",
      "Epoch 1410/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7379\n",
      "Epoch 1411/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7380\n",
      "Epoch 1412/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7382\n",
      "Epoch 1413/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7385\n",
      "Epoch 1414/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7389\n",
      "Epoch 1415/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7387\n",
      "Epoch 1416/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7386\n",
      "Epoch 1417/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7381\n",
      "Epoch 1418/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7379\n",
      "Epoch 1419/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7376\n",
      "Epoch 1420/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7374\n",
      "Epoch 1421/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7374\n",
      "Epoch 1422/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7375\n",
      "Epoch 1423/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7377\n",
      "Epoch 1424/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7378\n",
      "Epoch 1425/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7381\n",
      "Epoch 1426/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7381\n",
      "Epoch 1427/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7385\n",
      "Epoch 1428/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7383\n",
      "Epoch 1429/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7383\n",
      "Epoch 1430/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7377\n",
      "Epoch 1431/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7373\n",
      "Epoch 1432/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7370\n",
      "Epoch 1433/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7368\n",
      "Epoch 1434/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1435/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7371\n",
      "Epoch 1436/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7374\n",
      "Epoch 1437/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7374\n",
      "Epoch 1438/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7375\n",
      "Epoch 1439/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7373\n",
      "Epoch 1440/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7371\n",
      "Epoch 1441/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7367\n",
      "Epoch 1442/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7365\n",
      "Epoch 1443/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7364\n",
      "Epoch 1444/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7364\n",
      "Epoch 1445/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7365\n",
      "Epoch 1446/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7366\n",
      "Epoch 1447/3000\n",
      "379/379 [==============================] - 0s 26us/step - loss: 2.7368\n",
      "Epoch 1448/3000\n",
      "379/379 [==============================] - 0s 50us/step - loss: 2.7369\n",
      "Epoch 1449/3000\n",
      "379/379 [==============================] - 0s 60us/step - loss: 2.7372\n",
      "Epoch 1450/3000\n",
      "379/379 [==============================] - 0s 31us/step - loss: 2.7372\n",
      "Epoch 1451/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7374\n",
      "Epoch 1452/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7370\n",
      "Epoch 1453/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7367\n",
      "Epoch 1454/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7362\n",
      "Epoch 1455/3000\n",
      "379/379 [==============================] - 0s 25us/step - loss: 2.7360\n",
      "Epoch 1456/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7358\n",
      "Epoch 1457/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7358\n",
      "Epoch 1458/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7359\n",
      "Epoch 1459/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7360\n",
      "Epoch 1460/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7361\n",
      "Epoch 1461/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7361\n",
      "Epoch 1462/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7361\n",
      "Epoch 1463/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7360\n",
      "Epoch 1464/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7360\n",
      "Epoch 1465/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7358\n",
      "Epoch 1466/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7357\n",
      "Epoch 1467/3000\n",
      "379/379 [==============================] - 0s 19us/step - loss: 2.7356\n",
      "Epoch 1468/3000\n",
      "379/379 [==============================] - 0s 19us/step - loss: 2.7354\n",
      "Epoch 1469/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7353\n",
      "Epoch 1470/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7352\n",
      "Epoch 1471/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7351\n",
      "Epoch 1472/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7351\n",
      "Epoch 1473/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7351\n",
      "Epoch 1474/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7351\n",
      "Epoch 1475/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7352\n",
      "Epoch 1476/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7352\n",
      "Epoch 1477/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7353\n",
      "Epoch 1478/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7353\n",
      "Epoch 1479/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7356\n",
      "Epoch 1480/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7356\n",
      "Epoch 1481/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7358\n",
      "Epoch 1482/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7357\n",
      "Epoch 1483/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7358\n",
      "Epoch 1484/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7355\n",
      "Epoch 1485/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7354\n",
      "Epoch 1486/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7349\n",
      "Epoch 1487/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7348\n",
      "Epoch 1488/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7345\n",
      "Epoch 1489/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7343\n",
      "Epoch 1490/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7342\n",
      "Epoch 1491/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7342\n",
      "Epoch 1492/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7342\n",
      "Epoch 1493/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7342\n",
      "Epoch 1494/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7342\n",
      "Epoch 1495/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7342\n",
      "Epoch 1496/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7343\n",
      "Epoch 1497/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7343\n",
      "Epoch 1498/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7346\n",
      "Epoch 1499/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7347\n",
      "Epoch 1500/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7350\n",
      "Epoch 1501/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7350\n",
      "Epoch 1502/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7354\n",
      "Epoch 1503/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7351\n",
      "Epoch 1504/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7352\n",
      "Epoch 1505/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7346\n",
      "Epoch 1506/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7343\n",
      "Epoch 1507/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7338\n",
      "Epoch 1508/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7335\n",
      "Epoch 1509/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7333\n",
      "Epoch 1510/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7333\n",
      "Epoch 1511/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7333\n",
      "Epoch 1512/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7333\n",
      "Epoch 1513/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7334\n",
      "Epoch 1514/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7334\n",
      "Epoch 1515/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7335\n",
      "Epoch 1516/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7334\n",
      "Epoch 1517/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7335\n",
      "Epoch 1518/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7335\n",
      "Epoch 1519/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7335\n",
      "Epoch 1520/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7334\n",
      "Epoch 1521/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7334\n",
      "Epoch 1522/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7332\n",
      "Epoch 1523/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7334\n",
      "Epoch 1524/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7333\n",
      "Epoch 1525/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7335\n",
      "Epoch 1526/3000\n",
      "379/379 [==============================] - 0s 10us/step - loss: 2.7334\n",
      "Epoch 1527/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7336\n",
      "Epoch 1528/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7334\n",
      "Epoch 1529/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1530/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7331\n",
      "Epoch 1531/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7330\n",
      "Epoch 1532/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7326\n",
      "Epoch 1533/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7324\n",
      "Epoch 1534/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7322\n",
      "Epoch 1535/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7321\n",
      "Epoch 1536/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7320\n",
      "Epoch 1537/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7320\n",
      "Epoch 1538/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7320\n",
      "Epoch 1539/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7320\n",
      "Epoch 1540/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7321\n",
      "Epoch 1541/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7322\n",
      "Epoch 1542/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7326\n",
      "Epoch 1543/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7329\n",
      "Epoch 1544/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7338\n",
      "Epoch 1545/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7339\n",
      "Epoch 1546/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7347\n",
      "Epoch 1547/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7338\n",
      "Epoch 1548/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7337\n",
      "Epoch 1549/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7327\n",
      "Epoch 1550/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7321\n",
      "Epoch 1551/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7315\n",
      "Epoch 1552/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7313\n",
      "Epoch 1553/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7313\n",
      "Epoch 1554/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7315\n",
      "Epoch 1555/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7317\n",
      "Epoch 1556/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7317\n",
      "Epoch 1557/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7317\n",
      "Epoch 1558/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7316\n",
      "Epoch 1559/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7315\n",
      "Epoch 1560/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7313\n",
      "Epoch 1561/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7311\n",
      "Epoch 1562/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7310\n",
      "Epoch 1563/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7310\n",
      "Epoch 1564/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7309\n",
      "Epoch 1565/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7308\n",
      "Epoch 1566/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7308\n",
      "Epoch 1567/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7307\n",
      "Epoch 1568/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7307\n",
      "Epoch 1569/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7306\n",
      "Epoch 1570/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7306\n",
      "Epoch 1571/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7305\n",
      "Epoch 1572/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7304\n",
      "Epoch 1573/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7305\n",
      "Epoch 1574/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7304\n",
      "Epoch 1575/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7305\n",
      "Epoch 1576/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7306\n",
      "Epoch 1577/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7308\n",
      "Epoch 1578/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7310\n",
      "Epoch 1579/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7316\n",
      "Epoch 1580/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7317\n",
      "Epoch 1581/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7322\n",
      "Epoch 1582/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7317\n",
      "Epoch 1583/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7318\n",
      "Epoch 1584/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7311\n",
      "Epoch 1585/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7308\n",
      "Epoch 1586/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7301\n",
      "Epoch 1587/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7298\n",
      "Epoch 1588/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7296\n",
      "Epoch 1589/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7296\n",
      "Epoch 1590/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7296\n",
      "Epoch 1591/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7297\n",
      "Epoch 1592/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7298\n",
      "Epoch 1593/3000\n",
      "379/379 [==============================] - 0s 44us/step - loss: 2.7299\n",
      "Epoch 1594/3000\n",
      "379/379 [==============================] - 0s 41us/step - loss: 2.7303\n",
      "Epoch 1595/3000\n",
      "379/379 [==============================] - 0s 26us/step - loss: 2.7304\n",
      "Epoch 1596/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7308\n",
      "Epoch 1597/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7308\n",
      "Epoch 1598/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7312\n",
      "Epoch 1599/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7306\n",
      "Epoch 1600/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7304\n",
      "Epoch 1601/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7298\n",
      "Epoch 1602/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7294\n",
      "Epoch 1603/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7290\n",
      "Epoch 1604/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7288\n",
      "Epoch 1605/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7289\n",
      "Epoch 1606/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7290\n",
      "Epoch 1607/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7293\n",
      "Epoch 1608/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7296\n",
      "Epoch 1609/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7301\n",
      "Epoch 1610/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7300\n",
      "Epoch 1611/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7301\n",
      "Epoch 1612/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7296\n",
      "Epoch 1613/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7294\n",
      "Epoch 1614/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7289\n",
      "Epoch 1615/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7285\n",
      "Epoch 1616/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7283\n",
      "Epoch 1617/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7282\n",
      "Epoch 1618/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7282\n",
      "Epoch 1619/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7282\n",
      "Epoch 1620/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7283\n",
      "Epoch 1621/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7284\n",
      "Epoch 1622/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7287\n",
      "Epoch 1623/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7288\n",
      "Epoch 1624/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s 13us/step - loss: 2.7289\n",
      "Epoch 1625/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7288\n",
      "Epoch 1626/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7288\n",
      "Epoch 1627/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7286\n",
      "Epoch 1628/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7286\n",
      "Epoch 1629/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7283\n",
      "Epoch 1630/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7282\n",
      "Epoch 1631/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7279\n",
      "Epoch 1632/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7277\n",
      "Epoch 1633/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7275\n",
      "Epoch 1634/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7275\n",
      "Epoch 1635/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7274\n",
      "Epoch 1636/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7274\n",
      "Epoch 1637/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7274\n",
      "Epoch 1638/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7274\n",
      "Epoch 1639/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7274\n",
      "Epoch 1640/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7275\n",
      "Epoch 1641/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7277\n",
      "Epoch 1642/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7280\n",
      "Epoch 1643/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7286\n",
      "Epoch 1644/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7288\n",
      "Epoch 1645/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7294\n",
      "Epoch 1646/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7290\n",
      "Epoch 1647/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7292\n",
      "Epoch 1648/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7286\n",
      "Epoch 1649/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7284\n",
      "Epoch 1650/3000\n",
      "379/379 [==============================] - 0s 25us/step - loss: 2.7276\n",
      "Epoch 1651/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7270\n",
      "Epoch 1652/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7267\n",
      "Epoch 1653/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7266\n",
      "Epoch 1654/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7268\n",
      "Epoch 1655/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7272\n",
      "Epoch 1656/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7277\n",
      "Epoch 1657/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7278\n",
      "Epoch 1658/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7280\n",
      "Epoch 1659/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7275\n",
      "Epoch 1660/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7273\n",
      "Epoch 1661/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7269\n",
      "Epoch 1662/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7267\n",
      "Epoch 1663/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7264\n",
      "Epoch 1664/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7262\n",
      "Epoch 1665/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7261\n",
      "Epoch 1666/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7260\n",
      "Epoch 1667/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7260\n",
      "Epoch 1668/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7260\n",
      "Epoch 1669/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7260\n",
      "Epoch 1670/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7262\n",
      "Epoch 1671/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7266\n",
      "Epoch 1672/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7267\n",
      "Epoch 1673/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7269\n",
      "Epoch 1674/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7268\n",
      "Epoch 1675/3000\n",
      "379/379 [==============================] - 0s 21us/step - loss: 2.7269\n",
      "Epoch 1676/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7266\n",
      "Epoch 1677/3000\n",
      "379/379 [==============================] - 0s 19us/step - loss: 2.7265\n",
      "Epoch 1678/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7261\n",
      "Epoch 1679/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7258\n",
      "Epoch 1680/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7256\n",
      "Epoch 1681/3000\n",
      "379/379 [==============================] - 0s 19us/step - loss: 2.7254\n",
      "Epoch 1682/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7253\n",
      "Epoch 1683/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7252\n",
      "Epoch 1684/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7252\n",
      "Epoch 1685/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7252\n",
      "Epoch 1686/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7253\n",
      "Epoch 1687/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7254\n",
      "Epoch 1688/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7256\n",
      "Epoch 1689/3000\n",
      "379/379 [==============================] - 0s 20us/step - loss: 2.7257\n",
      "Epoch 1690/3000\n",
      "379/379 [==============================] - 0s 25us/step - loss: 2.7260\n",
      "Epoch 1691/3000\n",
      "379/379 [==============================] - 0s 19us/step - loss: 2.7262\n",
      "Epoch 1692/3000\n",
      "379/379 [==============================] - 0s 20us/step - loss: 2.7267\n",
      "Epoch 1693/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7265\n",
      "Epoch 1694/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7268\n",
      "Epoch 1695/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7262\n",
      "Epoch 1696/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7260\n",
      "Epoch 1697/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7254\n",
      "Epoch 1698/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7251\n",
      "Epoch 1699/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7247\n",
      "Epoch 1700/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7245\n",
      "Epoch 1701/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7244\n",
      "Epoch 1702/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7244\n",
      "Epoch 1703/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7245\n",
      "Epoch 1704/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7245\n",
      "Epoch 1705/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7245\n",
      "Epoch 1706/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7245\n",
      "Epoch 1707/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7245\n",
      "Epoch 1708/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7244\n",
      "Epoch 1709/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7243\n",
      "Epoch 1710/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7242\n",
      "Epoch 1711/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7241\n",
      "Epoch 1712/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7240\n",
      "Epoch 1713/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7240\n",
      "Epoch 1714/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7240\n",
      "Epoch 1715/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7240\n",
      "Epoch 1716/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7241\n",
      "Epoch 1717/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7244\n",
      "Epoch 1718/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1719/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7252\n",
      "Epoch 1720/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7254\n",
      "Epoch 1721/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7262\n",
      "Epoch 1722/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7261\n",
      "Epoch 1723/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7269\n",
      "Epoch 1724/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7260\n",
      "Epoch 1725/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7258\n",
      "Epoch 1726/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7247\n",
      "Epoch 1727/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7241\n",
      "Epoch 1728/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7236\n",
      "Epoch 1729/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7233\n",
      "Epoch 1730/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7231\n",
      "Epoch 1731/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7231\n",
      "Epoch 1732/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7232\n",
      "Epoch 1733/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7234\n",
      "Epoch 1734/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7237\n",
      "Epoch 1735/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7238\n",
      "Epoch 1736/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7240\n",
      "Epoch 1737/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7238\n",
      "Epoch 1738/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7239\n",
      "Epoch 1739/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7236\n",
      "Epoch 1740/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7234\n",
      "Epoch 1741/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7230\n",
      "Epoch 1742/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7229\n",
      "Epoch 1743/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7228\n",
      "Epoch 1744/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7228\n",
      "Epoch 1745/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7227\n",
      "Epoch 1746/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7227\n",
      "Epoch 1747/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7226\n",
      "Epoch 1748/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7227\n",
      "Epoch 1749/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7226\n",
      "Epoch 1750/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7227\n",
      "Epoch 1751/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7227\n",
      "Epoch 1752/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7229\n",
      "Epoch 1753/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7229\n",
      "Epoch 1754/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7231\n",
      "Epoch 1755/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7230\n",
      "Epoch 1756/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7233\n",
      "Epoch 1757/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7232\n",
      "Epoch 1758/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7235\n",
      "Epoch 1759/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7233\n",
      "Epoch 1760/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7234\n",
      "Epoch 1761/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7229\n",
      "Epoch 1762/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7229\n",
      "Epoch 1763/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7225\n",
      "Epoch 1764/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7222\n",
      "Epoch 1765/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7219\n",
      "Epoch 1766/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7217\n",
      "Epoch 1767/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7216\n",
      "Epoch 1768/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7215\n",
      "Epoch 1769/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7216\n",
      "Epoch 1770/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7217\n",
      "Epoch 1771/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7219\n",
      "Epoch 1772/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7222\n",
      "Epoch 1773/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7227\n",
      "Epoch 1774/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7229\n",
      "Epoch 1775/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7234\n",
      "Epoch 1776/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7232\n",
      "Epoch 1777/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7235\n",
      "Epoch 1778/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7229\n",
      "Epoch 1779/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7227\n",
      "Epoch 1780/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7219\n",
      "Epoch 1781/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7214\n",
      "Epoch 1782/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7211\n",
      "Epoch 1783/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7209\n",
      "Epoch 1784/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7210\n",
      "Epoch 1785/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7212\n",
      "Epoch 1786/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7216\n",
      "Epoch 1787/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7218\n",
      "Epoch 1788/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7224\n",
      "Epoch 1789/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7223\n",
      "Epoch 1790/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7227\n",
      "Epoch 1791/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7221\n",
      "Epoch 1792/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7219\n",
      "Epoch 1793/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7212\n",
      "Epoch 1794/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7208\n",
      "Epoch 1795/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7205\n",
      "Epoch 1796/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7204\n",
      "Epoch 1797/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7205\n",
      "Epoch 1798/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7206\n",
      "Epoch 1799/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7209\n",
      "Epoch 1800/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7211\n",
      "Epoch 1801/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7215\n",
      "Epoch 1802/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7215\n",
      "Epoch 1803/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7216\n",
      "Epoch 1804/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7212\n",
      "Epoch 1805/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7209\n",
      "Epoch 1806/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7204\n",
      "Epoch 1807/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7201\n",
      "Epoch 1808/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7200\n",
      "Epoch 1809/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7199\n",
      "Epoch 1810/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7199\n",
      "Epoch 1811/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7200\n",
      "Epoch 1812/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7200\n",
      "Epoch 1813/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s 12us/step - loss: 2.7201\n",
      "Epoch 1814/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7202\n",
      "Epoch 1815/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7202\n",
      "Epoch 1816/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7204\n",
      "Epoch 1817/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7204\n",
      "Epoch 1818/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7206\n",
      "Epoch 1819/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7206\n",
      "Epoch 1820/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7207\n",
      "Epoch 1821/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7205\n",
      "Epoch 1822/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7206\n",
      "Epoch 1823/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7204\n",
      "Epoch 1824/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7204\n",
      "Epoch 1825/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7201\n",
      "Epoch 1826/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7200\n",
      "Epoch 1827/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7197\n",
      "Epoch 1828/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7197\n",
      "Epoch 1829/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7195\n",
      "Epoch 1830/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7193\n",
      "Epoch 1831/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7192\n",
      "Epoch 1832/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7191\n",
      "Epoch 1833/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7190\n",
      "Epoch 1834/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7190\n",
      "Epoch 1835/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7190\n",
      "Epoch 1836/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7190\n",
      "Epoch 1837/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7190\n",
      "Epoch 1838/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7191\n",
      "Epoch 1839/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7192\n",
      "Epoch 1840/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7194\n",
      "Epoch 1841/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7196\n",
      "Epoch 1842/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7201\n",
      "Epoch 1843/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7202\n",
      "Epoch 1844/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7209\n",
      "Epoch 1845/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7207\n",
      "Epoch 1846/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7212\n",
      "Epoch 1847/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7206\n",
      "Epoch 1848/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7205\n",
      "Epoch 1849/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7197\n",
      "Epoch 1850/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7193\n",
      "Epoch 1851/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7187\n",
      "Epoch 1852/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7184\n",
      "Epoch 1853/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7182\n",
      "Epoch 1854/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7181\n",
      "Epoch 1855/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7181\n",
      "Epoch 1856/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7181\n",
      "Epoch 1857/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7181\n",
      "Epoch 1858/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7182\n",
      "Epoch 1859/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7184\n",
      "Epoch 1860/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7185\n",
      "Epoch 1861/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7189\n",
      "Epoch 1862/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7190\n",
      "Epoch 1863/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7194\n",
      "Epoch 1864/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7193\n",
      "Epoch 1865/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7195\n",
      "Epoch 1866/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7191\n",
      "Epoch 1867/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7191\n",
      "Epoch 1868/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7185\n",
      "Epoch 1869/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7183\n",
      "Epoch 1870/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7179\n",
      "Epoch 1871/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7178\n",
      "Epoch 1872/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7176\n",
      "Epoch 1873/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7175\n",
      "Epoch 1874/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7174\n",
      "Epoch 1875/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7173\n",
      "Epoch 1876/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7172\n",
      "Epoch 1877/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7172\n",
      "Epoch 1878/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7171\n",
      "Epoch 1879/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7171\n",
      "Epoch 1880/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7171\n",
      "Epoch 1881/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7170\n",
      "Epoch 1882/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7170\n",
      "Epoch 1883/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7171\n",
      "Epoch 1884/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7171\n",
      "Epoch 1885/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7173\n",
      "Epoch 1886/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7175\n",
      "Epoch 1887/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7180\n",
      "Epoch 1888/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7183\n",
      "Epoch 1889/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7192\n",
      "Epoch 1890/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7193\n",
      "Epoch 1891/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7205\n",
      "Epoch 1892/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7198\n",
      "Epoch 1893/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7203\n",
      "Epoch 1894/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7189\n",
      "Epoch 1895/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7183\n",
      "Epoch 1896/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7172\n",
      "Epoch 1897/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7167\n",
      "Epoch 1898/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7163\n",
      "Epoch 1899/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7163\n",
      "Epoch 1900/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7164\n",
      "Epoch 1901/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7166\n",
      "Epoch 1902/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7170\n",
      "Epoch 1903/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7172\n",
      "Epoch 1904/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7176\n",
      "Epoch 1905/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7175\n",
      "Epoch 1906/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7177\n",
      "Epoch 1907/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1908/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7169\n",
      "Epoch 1909/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7164\n",
      "Epoch 1910/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7160\n",
      "Epoch 1911/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7158\n",
      "Epoch 1912/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7157\n",
      "Epoch 1913/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7159\n",
      "Epoch 1914/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7161\n",
      "Epoch 1915/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7164\n",
      "Epoch 1916/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7166\n",
      "Epoch 1917/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7169\n",
      "Epoch 1918/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7167\n",
      "Epoch 1919/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7168\n",
      "Epoch 1920/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7164\n",
      "Epoch 1921/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7164\n",
      "Epoch 1922/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7160\n",
      "Epoch 1923/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7157\n",
      "Epoch 1924/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7154\n",
      "Epoch 1925/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7153\n",
      "Epoch 1926/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7152\n",
      "Epoch 1927/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7151\n",
      "Epoch 1928/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7151\n",
      "Epoch 1929/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7150\n",
      "Epoch 1930/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7150\n",
      "Epoch 1931/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7150\n",
      "Epoch 1932/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7151\n",
      "Epoch 1933/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7152\n",
      "Epoch 1934/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7154\n",
      "Epoch 1935/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7155\n",
      "Epoch 1936/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7159\n",
      "Epoch 1937/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7160\n",
      "Epoch 1938/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7166\n",
      "Epoch 1939/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7166\n",
      "Epoch 1940/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7171\n",
      "Epoch 1941/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7166\n",
      "Epoch 1942/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7168\n",
      "Epoch 1943/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7160\n",
      "Epoch 1944/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7157\n",
      "Epoch 1945/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7151\n",
      "Epoch 1946/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7148\n",
      "Epoch 1947/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7145\n",
      "Epoch 1948/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7144\n",
      "Epoch 1949/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7143\n",
      "Epoch 1950/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7142\n",
      "Epoch 1951/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7142\n",
      "Epoch 1952/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7141\n",
      "Epoch 1953/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7141\n",
      "Epoch 1954/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7141\n",
      "Epoch 1955/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7141\n",
      "Epoch 1956/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7141\n",
      "Epoch 1957/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7143\n",
      "Epoch 1958/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7144\n",
      "Epoch 1959/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7148\n",
      "Epoch 1960/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7150\n",
      "Epoch 1961/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7157\n",
      "Epoch 1962/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7159\n",
      "Epoch 1963/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7170\n",
      "Epoch 1964/3000\n",
      "379/379 [==============================] - 0s 19us/step - loss: 2.7167\n",
      "Epoch 1965/3000\n",
      "379/379 [==============================] - 0s 19us/step - loss: 2.7175\n",
      "Epoch 1966/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7164\n",
      "Epoch 1967/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7163\n",
      "Epoch 1968/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7150\n",
      "Epoch 1969/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7144\n",
      "Epoch 1970/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7138\n",
      "Epoch 1971/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7135\n",
      "Epoch 1972/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7134\n",
      "Epoch 1973/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7134\n",
      "Epoch 1974/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7134\n",
      "Epoch 1975/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7136\n",
      "Epoch 1976/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7138\n",
      "Epoch 1977/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7141\n",
      "Epoch 1978/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7146\n",
      "Epoch 1979/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7148\n",
      "Epoch 1980/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7155\n",
      "Epoch 1981/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7151\n",
      "Epoch 1982/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7152\n",
      "Epoch 1983/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7144\n",
      "Epoch 1984/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7140\n",
      "Epoch 1985/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7135\n",
      "Epoch 1986/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7131\n",
      "Epoch 1987/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7129\n",
      "Epoch 1988/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7128\n",
      "Epoch 1989/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7128\n",
      "Epoch 1990/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7128\n",
      "Epoch 1991/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7128\n",
      "Epoch 1992/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7128\n",
      "Epoch 1993/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7128\n",
      "Epoch 1994/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7129\n",
      "Epoch 1995/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7130\n",
      "Epoch 1996/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7131\n",
      "Epoch 1997/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7135\n",
      "Epoch 1998/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7137\n",
      "Epoch 1999/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7143\n",
      "Epoch 2000/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7143\n",
      "Epoch 2001/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7150\n",
      "Epoch 2002/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s 11us/step - loss: 2.7146\n",
      "Epoch 2003/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7148\n",
      "Epoch 2004/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7140\n",
      "Epoch 2005/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7136\n",
      "Epoch 2006/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7129\n",
      "Epoch 2007/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7125\n",
      "Epoch 2008/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7123\n",
      "Epoch 2009/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7121\n",
      "Epoch 2010/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7121\n",
      "Epoch 2011/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7121\n",
      "Epoch 2012/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7121\n",
      "Epoch 2013/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7120\n",
      "Epoch 2014/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7121\n",
      "Epoch 2015/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7121\n",
      "Epoch 2016/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7122\n",
      "Epoch 2017/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7123\n",
      "Epoch 2018/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7126\n",
      "Epoch 2019/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7129\n",
      "Epoch 2020/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7136\n",
      "Epoch 2021/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7137\n",
      "Epoch 2022/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7147\n",
      "Epoch 2023/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7144\n",
      "Epoch 2024/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7148\n",
      "Epoch 2025/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7138\n",
      "Epoch 2026/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7135\n",
      "Epoch 2027/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7126\n",
      "Epoch 2028/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7121\n",
      "Epoch 2029/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7117\n",
      "Epoch 2030/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7115\n",
      "Epoch 2031/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7116\n",
      "Epoch 2032/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7120\n",
      "Epoch 2033/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7126\n",
      "Epoch 2034/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7132\n",
      "Epoch 2035/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7145\n",
      "Epoch 2036/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7144\n",
      "Epoch 2037/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7156\n",
      "Epoch 2038/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7142\n",
      "Epoch 2039/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7135\n",
      "Epoch 2040/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7121\n",
      "Epoch 2041/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7114\n",
      "Epoch 2042/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7113\n",
      "Epoch 2043/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7116\n",
      "Epoch 2044/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7125\n",
      "Epoch 2045/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7132\n",
      "Epoch 2046/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7148\n",
      "Epoch 2047/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7142\n",
      "Epoch 2048/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7146\n",
      "Epoch 2049/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7130\n",
      "Epoch 2050/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7119\n",
      "Epoch 2051/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7112\n",
      "Epoch 2052/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7111\n",
      "Epoch 2053/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7116\n",
      "Epoch 2054/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7122\n",
      "Epoch 2055/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7132\n",
      "Epoch 2056/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7132\n",
      "Epoch 2057/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7136\n",
      "Epoch 2058/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7127\n",
      "Epoch 2059/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7122\n",
      "Epoch 2060/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7114\n",
      "Epoch 2061/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7109\n",
      "Epoch 2062/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7109\n",
      "Epoch 2063/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7111\n",
      "Epoch 2064/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7115\n",
      "Epoch 2065/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7117\n",
      "Epoch 2066/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7120\n",
      "Epoch 2067/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7118\n",
      "Epoch 2068/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7119\n",
      "Epoch 2069/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7114\n",
      "Epoch 2070/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7111\n",
      "Epoch 2071/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7107\n",
      "Epoch 2072/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7105\n",
      "Epoch 2073/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7105\n",
      "Epoch 2074/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7107\n",
      "Epoch 2075/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7109\n",
      "Epoch 2076/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7112\n",
      "Epoch 2077/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7115\n",
      "Epoch 2078/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7115\n",
      "Epoch 2079/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7117\n",
      "Epoch 2080/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7114\n",
      "Epoch 2081/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7112\n",
      "Epoch 2082/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7107\n",
      "Epoch 2083/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7103\n",
      "Epoch 2084/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7102\n",
      "Epoch 2085/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7104\n",
      "Epoch 2086/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7108\n",
      "Epoch 2087/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7113\n",
      "Epoch 2088/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7121\n",
      "Epoch 2089/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7121\n",
      "Epoch 2090/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7126\n",
      "Epoch 2091/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7118\n",
      "Epoch 2092/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7112\n",
      "Epoch 2093/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7104\n",
      "Epoch 2094/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7100\n",
      "Epoch 2095/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7100\n",
      "Epoch 2096/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2097/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7109\n",
      "Epoch 2098/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7114\n",
      "Epoch 2099/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7123\n",
      "Epoch 2100/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7121\n",
      "Epoch 2101/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7124\n",
      "Epoch 2102/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7113\n",
      "Epoch 2103/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7106\n",
      "Epoch 2104/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7100\n",
      "Epoch 2105/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7098\n",
      "Epoch 2106/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7100\n",
      "Epoch 2107/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7105\n",
      "Epoch 2108/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7113\n",
      "Epoch 2109/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7117\n",
      "Epoch 2110/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7125\n",
      "Epoch 2111/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7119\n",
      "Epoch 2112/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7118\n",
      "Epoch 2113/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7107\n",
      "Epoch 2114/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7100\n",
      "Epoch 2115/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7096\n",
      "Epoch 2116/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7096\n",
      "Epoch 2117/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7100\n",
      "Epoch 2118/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7104\n",
      "Epoch 2119/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7110\n",
      "Epoch 2120/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7112\n",
      "Epoch 2121/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7116\n",
      "Epoch 2122/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7110\n",
      "Epoch 2123/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7108\n",
      "Epoch 2124/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7100\n",
      "Epoch 2125/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7096\n",
      "Epoch 2126/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7093\n",
      "Epoch 2127/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7094\n",
      "Epoch 2128/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7096\n",
      "Epoch 2129/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7099\n",
      "Epoch 2130/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7103\n",
      "Epoch 2131/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7105\n",
      "Epoch 2132/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7110\n",
      "Epoch 2133/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7106\n",
      "Epoch 2134/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7107\n",
      "Epoch 2135/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7100\n",
      "Epoch 2136/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7095\n",
      "Epoch 2137/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7091\n",
      "Epoch 2138/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7090\n",
      "Epoch 2139/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7091\n",
      "Epoch 2140/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7094\n",
      "Epoch 2141/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7098\n",
      "Epoch 2142/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7102\n",
      "Epoch 2143/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7108\n",
      "Epoch 2144/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7107\n",
      "Epoch 2145/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7109\n",
      "Epoch 2146/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7102\n",
      "Epoch 2147/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7096\n",
      "Epoch 2148/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7090\n",
      "Epoch 2149/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7088\n",
      "Epoch 2150/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7089\n",
      "Epoch 2151/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7092\n",
      "Epoch 2152/3000\n",
      "379/379 [==============================] - 0s 19us/step - loss: 2.7098\n",
      "Epoch 2153/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7103\n",
      "Epoch 2154/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7110\n",
      "Epoch 2155/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7108\n",
      "Epoch 2156/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7110\n",
      "Epoch 2157/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7101\n",
      "Epoch 2158/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7094\n",
      "Epoch 2159/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7087\n",
      "Epoch 2160/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7086\n",
      "Epoch 2161/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7088\n",
      "Epoch 2162/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7093\n",
      "Epoch 2163/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7101\n",
      "Epoch 2164/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7104\n",
      "Epoch 2165/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7111\n",
      "Epoch 2166/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7105\n",
      "Epoch 2167/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7104\n",
      "Epoch 2168/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7094\n",
      "Epoch 2169/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7088\n",
      "Epoch 2170/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7084\n",
      "Epoch 2171/3000\n",
      "379/379 [==============================] - 0s 21us/step - loss: 2.7085\n",
      "Epoch 2172/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7090\n",
      "Epoch 2173/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7094\n",
      "Epoch 2174/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7101\n",
      "Epoch 2175/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7101\n",
      "Epoch 2176/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7104\n",
      "Epoch 2177/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7097\n",
      "Epoch 2178/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7095\n",
      "Epoch 2179/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7088\n",
      "Epoch 2180/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7084\n",
      "Epoch 2181/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7082\n",
      "Epoch 2182/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7082\n",
      "Epoch 2183/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7085\n",
      "Epoch 2184/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7089\n",
      "Epoch 2185/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7094\n",
      "Epoch 2186/3000\n",
      "379/379 [==============================] - 0s 19us/step - loss: 2.7096\n",
      "Epoch 2187/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7100\n",
      "Epoch 2188/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7095\n",
      "Epoch 2189/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7094\n",
      "Epoch 2190/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7088\n",
      "Epoch 2191/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s 15us/step - loss: 2.7083\n",
      "Epoch 2192/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7080\n",
      "Epoch 2193/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7079\n",
      "Epoch 2194/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7081\n",
      "Epoch 2195/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7085\n",
      "Epoch 2196/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7090\n",
      "Epoch 2197/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7093\n",
      "Epoch 2198/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7099\n",
      "Epoch 2199/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7096\n",
      "Epoch 2200/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7097\n",
      "Epoch 2201/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7089\n",
      "Epoch 2202/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7083\n",
      "Epoch 2203/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7078\n",
      "Epoch 2204/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7077\n",
      "Epoch 2205/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7079\n",
      "Epoch 2206/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7083\n",
      "Epoch 2207/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7091\n",
      "Epoch 2208/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7094\n",
      "Epoch 2209/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7102\n",
      "Epoch 2210/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7096\n",
      "Epoch 2211/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7096\n",
      "Epoch 2212/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7086\n",
      "Epoch 2213/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7079\n",
      "Epoch 2214/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7075\n",
      "Epoch 2215/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7076\n",
      "Epoch 2216/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7080\n",
      "Epoch 2217/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7086\n",
      "Epoch 2218/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7094\n",
      "Epoch 2219/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7094\n",
      "Epoch 2220/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7098\n",
      "Epoch 2221/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7089\n",
      "Epoch 2222/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7087\n",
      "Epoch 2223/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7079\n",
      "Epoch 2224/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7074\n",
      "Epoch 2225/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7073\n",
      "Epoch 2226/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7074\n",
      "Epoch 2227/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7076\n",
      "Epoch 2228/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7079\n",
      "Epoch 2229/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7084\n",
      "Epoch 2230/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7085\n",
      "Epoch 2231/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7088\n",
      "Epoch 2232/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7085\n",
      "Epoch 2233/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7085\n",
      "Epoch 2234/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7079\n",
      "Epoch 2235/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7075\n",
      "Epoch 2236/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7071\n",
      "Epoch 2237/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7071\n",
      "Epoch 2238/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7073\n",
      "Epoch 2239/3000\n",
      "379/379 [==============================] - 0s 19us/step - loss: 2.7077\n",
      "Epoch 2240/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7084\n",
      "Epoch 2241/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7088\n",
      "Epoch 2242/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7095\n",
      "Epoch 2243/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7090\n",
      "Epoch 2244/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7090\n",
      "Epoch 2245/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7080\n",
      "Epoch 2246/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7073\n",
      "Epoch 2247/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7069\n",
      "Epoch 2248/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7069\n",
      "Epoch 2249/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7072\n",
      "Epoch 2250/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7077\n",
      "Epoch 2251/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7084\n",
      "Epoch 2252/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7086\n",
      "Epoch 2253/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7092\n",
      "Epoch 2254/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7086\n",
      "Epoch 2255/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7085\n",
      "Epoch 2256/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7076\n",
      "Epoch 2257/3000\n",
      "379/379 [==============================] - 0s 20us/step - loss: 2.7070\n",
      "Epoch 2258/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7067\n",
      "Epoch 2259/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7067\n",
      "Epoch 2260/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7072\n",
      "Epoch 2261/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7077\n",
      "Epoch 2262/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7084\n",
      "Epoch 2263/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7085\n",
      "Epoch 2264/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7089\n",
      "Epoch 2265/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7082\n",
      "Epoch 2266/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7080\n",
      "Epoch 2267/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7072\n",
      "Epoch 2268/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7067\n",
      "Epoch 2269/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7065\n",
      "Epoch 2270/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7065\n",
      "Epoch 2271/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7068\n",
      "Epoch 2272/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7071\n",
      "Epoch 2273/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7076\n",
      "Epoch 2274/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7078\n",
      "Epoch 2275/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7082\n",
      "Epoch 2276/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7078\n",
      "Epoch 2277/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7078\n",
      "Epoch 2278/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7071\n",
      "Epoch 2279/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7067\n",
      "Epoch 2280/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7063\n",
      "Epoch 2281/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7062\n",
      "Epoch 2282/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7064\n",
      "Epoch 2283/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7068\n",
      "Epoch 2284/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7073\n",
      "Epoch 2285/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2286/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7081\n",
      "Epoch 2287/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7078\n",
      "Epoch 2288/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7079\n",
      "Epoch 2289/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7072\n",
      "Epoch 2290/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7066\n",
      "Epoch 2291/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7061\n",
      "Epoch 2292/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7061\n",
      "Epoch 2293/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7064\n",
      "Epoch 2294/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7069\n",
      "Epoch 2295/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7077\n",
      "Epoch 2296/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7079\n",
      "Epoch 2297/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7086\n",
      "Epoch 2298/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7079\n",
      "Epoch 2299/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7078\n",
      "Epoch 2300/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7068\n",
      "Epoch 2301/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7062\n",
      "Epoch 2302/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7059\n",
      "Epoch 2303/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7060\n",
      "Epoch 2304/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7063\n",
      "Epoch 2305/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7068\n",
      "Epoch 2306/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7074\n",
      "Epoch 2307/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7075\n",
      "Epoch 2308/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7079\n",
      "Epoch 2309/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7073\n",
      "Epoch 2310/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7072\n",
      "Epoch 2311/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7065\n",
      "Epoch 2312/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7060\n",
      "Epoch 2313/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7057\n",
      "Epoch 2314/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7058\n",
      "Epoch 2315/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7061\n",
      "Epoch 2316/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7066\n",
      "Epoch 2317/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7072\n",
      "Epoch 2318/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7073\n",
      "Epoch 2319/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7078\n",
      "Epoch 2320/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7072\n",
      "Epoch 2321/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7071\n",
      "Epoch 2322/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7063\n",
      "Epoch 2323/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7057\n",
      "Epoch 2324/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7055\n",
      "Epoch 2325/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7056\n",
      "Epoch 2326/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7062\n",
      "Epoch 2327/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7066\n",
      "Epoch 2328/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7074\n",
      "Epoch 2329/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7073\n",
      "Epoch 2330/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7077\n",
      "Epoch 2331/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7069\n",
      "Epoch 2332/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7066\n",
      "Epoch 2333/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7059\n",
      "Epoch 2334/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7054\n",
      "Epoch 2335/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7053\n",
      "Epoch 2336/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7054\n",
      "Epoch 2337/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7058\n",
      "Epoch 2338/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7061\n",
      "Epoch 2339/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7067\n",
      "Epoch 2340/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7067\n",
      "Epoch 2341/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7069\n",
      "Epoch 2342/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7063\n",
      "Epoch 2343/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7061\n",
      "Epoch 2344/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7056\n",
      "Epoch 2345/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7052\n",
      "Epoch 2346/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7051\n",
      "Epoch 2347/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7051\n",
      "Epoch 2348/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7053\n",
      "Epoch 2349/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7056\n",
      "Epoch 2350/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7059\n",
      "Epoch 2351/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7061\n",
      "Epoch 2352/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7064\n",
      "Epoch 2353/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7063\n",
      "Epoch 2354/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7065\n",
      "Epoch 2355/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7060\n",
      "Epoch 2356/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7056\n",
      "Epoch 2357/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7051\n",
      "Epoch 2358/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7048\n",
      "Epoch 2359/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7049\n",
      "Epoch 2360/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7054\n",
      "Epoch 2361/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7062\n",
      "Epoch 2362/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7068\n",
      "Epoch 2363/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7078\n",
      "Epoch 2364/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7073\n",
      "Epoch 2365/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7074\n",
      "Epoch 2366/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7061\n",
      "Epoch 2367/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7053\n",
      "Epoch 2368/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7047\n",
      "Epoch 2369/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7047\n",
      "Epoch 2370/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7052\n",
      "Epoch 2371/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7057\n",
      "Epoch 2372/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7065\n",
      "Epoch 2373/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7066\n",
      "Epoch 2374/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7070\n",
      "Epoch 2375/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7063\n",
      "Epoch 2376/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7062\n",
      "Epoch 2377/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7054\n",
      "Epoch 2378/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7048\n",
      "Epoch 2379/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7045\n",
      "Epoch 2380/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s 11us/step - loss: 2.7045\n",
      "Epoch 2381/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7047\n",
      "Epoch 2382/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7051\n",
      "Epoch 2383/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7057\n",
      "Epoch 2384/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7059\n",
      "Epoch 2385/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7064\n",
      "Epoch 2386/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7060\n",
      "Epoch 2387/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7060\n",
      "Epoch 2388/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7053\n",
      "Epoch 2389/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7048\n",
      "Epoch 2390/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7044\n",
      "Epoch 2391/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7043\n",
      "Epoch 2392/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7045\n",
      "Epoch 2393/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7048\n",
      "Epoch 2394/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7054\n",
      "Epoch 2395/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7057\n",
      "Epoch 2396/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7063\n",
      "Epoch 2397/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7061\n",
      "Epoch 2398/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7061\n",
      "Epoch 2399/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7054\n",
      "Epoch 2400/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7048\n",
      "Epoch 2401/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7042\n",
      "Epoch 2402/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7041\n",
      "Epoch 2403/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7044\n",
      "Epoch 2404/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7049\n",
      "Epoch 2405/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7057\n",
      "Epoch 2406/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7060\n",
      "Epoch 2407/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7068\n",
      "Epoch 2408/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7061\n",
      "Epoch 2409/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7060\n",
      "Epoch 2410/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7050\n",
      "Epoch 2411/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7043\n",
      "Epoch 2412/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7040\n",
      "Epoch 2413/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7041\n",
      "Epoch 2414/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7047\n",
      "Epoch 2415/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7054\n",
      "Epoch 2416/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7064\n",
      "Epoch 2417/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7063\n",
      "Epoch 2418/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7066\n",
      "Epoch 2419/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7055\n",
      "Epoch 2420/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7050\n",
      "Epoch 2421/3000\n",
      "379/379 [==============================] - 0s 19us/step - loss: 2.7043\n",
      "Epoch 2422/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7039\n",
      "Epoch 2423/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7038\n",
      "Epoch 2424/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7041\n",
      "Epoch 2425/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7047\n",
      "Epoch 2426/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7051\n",
      "Epoch 2427/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7058\n",
      "Epoch 2428/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7055\n",
      "Epoch 2429/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7057\n",
      "Epoch 2430/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7049\n",
      "Epoch 2431/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7045\n",
      "Epoch 2432/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7039\n",
      "Epoch 2433/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7037\n",
      "Epoch 2434/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7037\n",
      "Epoch 2435/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7040\n",
      "Epoch 2436/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7046\n",
      "Epoch 2437/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7050\n",
      "Epoch 2438/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7058\n",
      "Epoch 2439/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7055\n",
      "Epoch 2440/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7057\n",
      "Epoch 2441/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7048\n",
      "Epoch 2442/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7045\n",
      "Epoch 2443/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7039\n",
      "Epoch 2444/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7036\n",
      "Epoch 2445/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7035\n",
      "Epoch 2446/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7036\n",
      "Epoch 2447/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7039\n",
      "Epoch 2448/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7042\n",
      "Epoch 2449/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7047\n",
      "Epoch 2450/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7049\n",
      "Epoch 2451/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7053\n",
      "Epoch 2452/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7049\n",
      "Epoch 2453/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7048\n",
      "Epoch 2454/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7041\n",
      "Epoch 2455/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7037\n",
      "Epoch 2456/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7034\n",
      "Epoch 2457/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7033\n",
      "Epoch 2458/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7036\n",
      "Epoch 2459/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7038\n",
      "Epoch 2460/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7043\n",
      "Epoch 2461/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7044\n",
      "Epoch 2462/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7048\n",
      "Epoch 2463/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7045\n",
      "Epoch 2464/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7044\n",
      "Epoch 2465/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7039\n",
      "Epoch 2466/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7035\n",
      "Epoch 2467/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7032\n",
      "Epoch 2468/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7031\n",
      "Epoch 2469/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7031\n",
      "Epoch 2470/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7032\n",
      "Epoch 2471/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7033\n",
      "Epoch 2472/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7035\n",
      "Epoch 2473/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7038\n",
      "Epoch 2474/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2475/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7043\n",
      "Epoch 2476/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7042\n",
      "Epoch 2477/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7043\n",
      "Epoch 2478/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7040\n",
      "Epoch 2479/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7039\n",
      "Epoch 2480/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7036\n",
      "Epoch 2481/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7035\n",
      "Epoch 2482/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7034\n",
      "Epoch 2483/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7033\n",
      "Epoch 2484/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7032\n",
      "Epoch 2485/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7033\n",
      "Epoch 2486/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7032\n",
      "Epoch 2487/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7033\n",
      "Epoch 2488/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7033\n",
      "Epoch 2489/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7034\n",
      "Epoch 2490/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7035\n",
      "Epoch 2491/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7037\n",
      "Epoch 2492/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7037\n",
      "Epoch 2493/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7041\n",
      "Epoch 2494/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7040\n",
      "Epoch 2495/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7043\n",
      "Epoch 2496/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7040\n",
      "Epoch 2497/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7041\n",
      "Epoch 2498/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7036\n",
      "Epoch 2499/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7035\n",
      "Epoch 2500/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7031\n",
      "Epoch 2501/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7029\n",
      "Epoch 2502/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7028\n",
      "Epoch 2503/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7027\n",
      "Epoch 2504/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7027\n",
      "Epoch 2505/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7027\n",
      "Epoch 2506/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7027\n",
      "Epoch 2507/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7028\n",
      "Epoch 2508/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7029\n",
      "Epoch 2509/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7031\n",
      "Epoch 2510/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7033\n",
      "Epoch 2511/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7037\n",
      "Epoch 2512/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7038\n",
      "Epoch 2513/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7045\n",
      "Epoch 2514/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7045\n",
      "Epoch 2515/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7051\n",
      "Epoch 2516/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7047\n",
      "Epoch 2517/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7050\n",
      "Epoch 2518/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7043\n",
      "Epoch 2519/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7042\n",
      "Epoch 2520/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7035\n",
      "Epoch 2521/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7033\n",
      "Epoch 2522/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7028\n",
      "Epoch 2523/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7027\n",
      "Epoch 2524/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7025\n",
      "Epoch 2525/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7025\n",
      "Epoch 2526/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7024\n",
      "Epoch 2527/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7024\n",
      "Epoch 2528/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7024\n",
      "Epoch 2529/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7024\n",
      "Epoch 2530/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7025\n",
      "Epoch 2531/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7026\n",
      "Epoch 2532/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7030\n",
      "Epoch 2533/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7036\n",
      "Epoch 2534/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7042\n",
      "Epoch 2535/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7055\n",
      "Epoch 2536/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7056\n",
      "Epoch 2537/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7070\n",
      "Epoch 2538/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7058\n",
      "Epoch 2539/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7060\n",
      "Epoch 2540/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7043\n",
      "Epoch 2541/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7035\n",
      "Epoch 2542/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7026\n",
      "Epoch 2543/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7022\n",
      "Epoch 2544/3000\n",
      "379/379 [==============================] - 0s 20us/step - loss: 2.7022\n",
      "Epoch 2545/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7024\n",
      "Epoch 2546/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7028\n",
      "Epoch 2547/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7034\n",
      "Epoch 2548/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7046\n",
      "Epoch 2549/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7048\n",
      "Epoch 2550/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7058\n",
      "Epoch 2551/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7048\n",
      "Epoch 2552/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7045\n",
      "Epoch 2553/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7032\n",
      "Epoch 2554/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7026\n",
      "Epoch 2555/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7021\n",
      "Epoch 2556/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7020\n",
      "Epoch 2557/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7024\n",
      "Epoch 2558/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7029\n",
      "Epoch 2559/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7037\n",
      "Epoch 2560/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7041\n",
      "Epoch 2561/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7051\n",
      "Epoch 2562/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7044\n",
      "Epoch 2563/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7044\n",
      "Epoch 2564/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7032\n",
      "Epoch 2565/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7026\n",
      "Epoch 2566/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7020\n",
      "Epoch 2567/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7018\n",
      "Epoch 2568/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7021\n",
      "Epoch 2569/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s 12us/step - loss: 2.7025\n",
      "Epoch 2570/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7032\n",
      "Epoch 2571/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7036\n",
      "Epoch 2572/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7044\n",
      "Epoch 2573/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7041\n",
      "Epoch 2574/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7043\n",
      "Epoch 2575/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7032\n",
      "Epoch 2576/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7027\n",
      "Epoch 2577/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7020\n",
      "Epoch 2578/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7018\n",
      "Epoch 2579/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7017\n",
      "Epoch 2580/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7017\n",
      "Epoch 2581/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7018\n",
      "Epoch 2582/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7018\n",
      "Epoch 2583/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7020\n",
      "Epoch 2584/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7021\n",
      "Epoch 2585/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7023\n",
      "Epoch 2586/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7025\n",
      "Epoch 2587/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7028\n",
      "Epoch 2588/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7028\n",
      "Epoch 2589/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7031\n",
      "Epoch 2590/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7029\n",
      "Epoch 2591/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7030\n",
      "Epoch 2592/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7028\n",
      "Epoch 2593/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7028\n",
      "Epoch 2594/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7025\n",
      "Epoch 2595/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7024\n",
      "Epoch 2596/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7022\n",
      "Epoch 2597/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7021\n",
      "Epoch 2598/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7020\n",
      "Epoch 2599/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7019\n",
      "Epoch 2600/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7018\n",
      "Epoch 2601/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7018\n",
      "Epoch 2602/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7019\n",
      "Epoch 2603/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7021\n",
      "Epoch 2604/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7022\n",
      "Epoch 2605/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7026\n",
      "Epoch 2606/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7027\n",
      "Epoch 2607/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7030\n",
      "Epoch 2608/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7029\n",
      "Epoch 2609/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7032\n",
      "Epoch 2610/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7028\n",
      "Epoch 2611/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7028\n",
      "Epoch 2612/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7024\n",
      "Epoch 2613/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7023\n",
      "Epoch 2614/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7019\n",
      "Epoch 2615/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7018\n",
      "Epoch 2616/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7016\n",
      "Epoch 2617/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7014\n",
      "Epoch 2618/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7013\n",
      "Epoch 2619/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7012\n",
      "Epoch 2620/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7011\n",
      "Epoch 2621/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7011\n",
      "Epoch 2622/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7011\n",
      "Epoch 2623/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7011\n",
      "Epoch 2624/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7011\n",
      "Epoch 2625/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7011\n",
      "Epoch 2626/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7012\n",
      "Epoch 2627/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7012\n",
      "Epoch 2628/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7014\n",
      "Epoch 2629/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7016\n",
      "Epoch 2630/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7020\n",
      "Epoch 2631/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7026\n",
      "Epoch 2632/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7030\n",
      "Epoch 2633/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7040\n",
      "Epoch 2634/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7038\n",
      "Epoch 2635/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7047\n",
      "Epoch 2636/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7037\n",
      "Epoch 2637/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7036\n",
      "Epoch 2638/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7025\n",
      "Epoch 2639/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7021\n",
      "Epoch 2640/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7015\n",
      "Epoch 2641/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7012\n",
      "Epoch 2642/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7010\n",
      "Epoch 2643/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7009\n",
      "Epoch 2644/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7008\n",
      "Epoch 2645/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7009\n",
      "Epoch 2646/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7010\n",
      "Epoch 2647/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7013\n",
      "Epoch 2648/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7017\n",
      "Epoch 2649/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7021\n",
      "Epoch 2650/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7029\n",
      "Epoch 2651/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7030\n",
      "Epoch 2652/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7038\n",
      "Epoch 2653/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7033\n",
      "Epoch 2654/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7036\n",
      "Epoch 2655/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7027\n",
      "Epoch 2656/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7024\n",
      "Epoch 2657/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7017\n",
      "Epoch 2658/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7013\n",
      "Epoch 2659/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7009\n",
      "Epoch 2660/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7008\n",
      "Epoch 2661/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7006\n",
      "Epoch 2662/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7006\n",
      "Epoch 2663/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2664/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7008\n",
      "Epoch 2665/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7011\n",
      "Epoch 2666/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7016\n",
      "Epoch 2667/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7023\n",
      "Epoch 2668/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7027\n",
      "Epoch 2669/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7036\n",
      "Epoch 2670/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7031\n",
      "Epoch 2671/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7034\n",
      "Epoch 2672/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7024\n",
      "Epoch 2673/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7020\n",
      "Epoch 2674/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7013\n",
      "Epoch 2675/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7009\n",
      "Epoch 2676/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7006\n",
      "Epoch 2677/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7005\n",
      "Epoch 2678/3000\n",
      "379/379 [==============================] - 0s 19us/step - loss: 2.7004\n",
      "Epoch 2679/3000\n",
      "379/379 [==============================] - 0s 23us/step - loss: 2.7004\n",
      "Epoch 2680/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.7006\n",
      "Epoch 2681/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7009\n",
      "Epoch 2682/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7014\n",
      "Epoch 2683/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.7018\n",
      "Epoch 2684/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7025\n",
      "Epoch 2685/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7024\n",
      "Epoch 2686/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7029\n",
      "Epoch 2687/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7022\n",
      "Epoch 2688/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7022\n",
      "Epoch 2689/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7015\n",
      "Epoch 2690/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7012\n",
      "Epoch 2691/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7007\n",
      "Epoch 2692/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7006\n",
      "Epoch 2693/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7004\n",
      "Epoch 2694/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7003\n",
      "Epoch 2695/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7002\n",
      "Epoch 2696/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7002\n",
      "Epoch 2697/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7002\n",
      "Epoch 2698/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7003\n",
      "Epoch 2699/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7006\n",
      "Epoch 2700/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7010\n",
      "Epoch 2701/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7016\n",
      "Epoch 2702/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7019\n",
      "Epoch 2703/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7027\n",
      "Epoch 2704/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7025\n",
      "Epoch 2705/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7029\n",
      "Epoch 2706/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7021\n",
      "Epoch 2707/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7019\n",
      "Epoch 2708/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7012\n",
      "Epoch 2709/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7008\n",
      "Epoch 2710/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7004\n",
      "Epoch 2711/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7002\n",
      "Epoch 2712/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7001\n",
      "Epoch 2713/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7000\n",
      "Epoch 2714/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7000\n",
      "Epoch 2715/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6999\n",
      "Epoch 2716/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6999\n",
      "Epoch 2717/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7000\n",
      "Epoch 2718/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7003\n",
      "Epoch 2719/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7007\n",
      "Epoch 2720/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7012\n",
      "Epoch 2721/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7016\n",
      "Epoch 2722/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7024\n",
      "Epoch 2723/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7023\n",
      "Epoch 2724/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7028\n",
      "Epoch 2725/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7021\n",
      "Epoch 2726/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7020\n",
      "Epoch 2727/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7012\n",
      "Epoch 2728/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7008\n",
      "Epoch 2729/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7003\n",
      "Epoch 2730/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7001\n",
      "Epoch 2731/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6999\n",
      "Epoch 2732/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6999\n",
      "Epoch 2733/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6998\n",
      "Epoch 2734/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6997\n",
      "Epoch 2735/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6997\n",
      "Epoch 2736/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6997\n",
      "Epoch 2737/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6999\n",
      "Epoch 2738/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7002\n",
      "Epoch 2739/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7007\n",
      "Epoch 2740/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7011\n",
      "Epoch 2741/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7019\n",
      "Epoch 2742/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7020\n",
      "Epoch 2743/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7027\n",
      "Epoch 2744/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7021\n",
      "Epoch 2745/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7022\n",
      "Epoch 2746/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7012\n",
      "Epoch 2747/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7009\n",
      "Epoch 2748/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7002\n",
      "Epoch 2749/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6999\n",
      "Epoch 2750/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6997\n",
      "Epoch 2751/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.6996\n",
      "Epoch 2752/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6995\n",
      "Epoch 2753/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6995\n",
      "Epoch 2754/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6995\n",
      "Epoch 2755/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6995\n",
      "Epoch 2756/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6998\n",
      "Epoch 2757/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7003\n",
      "Epoch 2758/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s 11us/step - loss: 2.7010\n",
      "Epoch 2759/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7013\n",
      "Epoch 2760/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7023\n",
      "Epoch 2761/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7020\n",
      "Epoch 2762/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7024\n",
      "Epoch 2763/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7015\n",
      "Epoch 2764/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7012\n",
      "Epoch 2765/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7003\n",
      "Epoch 2766/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6999\n",
      "Epoch 2767/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.6996\n",
      "Epoch 2768/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.6994\n",
      "Epoch 2769/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.6993\n",
      "Epoch 2770/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.6993\n",
      "Epoch 2771/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.6994\n",
      "Epoch 2772/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.6997\n",
      "Epoch 2773/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.7001\n",
      "Epoch 2774/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7004\n",
      "Epoch 2775/3000\n",
      "379/379 [==============================] - 0s 28us/step - loss: 2.7011\n",
      "Epoch 2776/3000\n",
      "379/379 [==============================] - 0s 30us/step - loss: 2.7012\n",
      "Epoch 2777/3000\n",
      "379/379 [==============================] - 0s 27us/step - loss: 2.7017\n",
      "Epoch 2778/3000\n",
      "379/379 [==============================] - 0s 44us/step - loss: 2.7012\n",
      "Epoch 2779/3000\n",
      "379/379 [==============================] - 0s 60us/step - loss: 2.7013\n",
      "Epoch 2780/3000\n",
      "379/379 [==============================] - 0s 24us/step - loss: 2.7006\n",
      "Epoch 2781/3000\n",
      "379/379 [==============================] - 0s 22us/step - loss: 2.7003\n",
      "Epoch 2782/3000\n",
      "379/379 [==============================] - 0s 16us/step - loss: 2.6998\n",
      "Epoch 2783/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.6996\n",
      "Epoch 2784/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6994\n",
      "Epoch 2785/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6992\n",
      "Epoch 2786/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6991\n",
      "Epoch 2787/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6990\n",
      "Epoch 2788/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.6991\n",
      "Epoch 2789/3000\n",
      "379/379 [==============================] - 0s 44us/step - loss: 2.6992\n",
      "Epoch 2790/3000\n",
      "379/379 [==============================] - 0s 51us/step - loss: 2.6996\n",
      "Epoch 2791/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7000\n",
      "Epoch 2792/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7007\n",
      "Epoch 2793/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7009\n",
      "Epoch 2794/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7017\n",
      "Epoch 2795/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7013\n",
      "Epoch 2796/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7016\n",
      "Epoch 2797/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7008\n",
      "Epoch 2798/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7006\n",
      "Epoch 2799/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6999\n",
      "Epoch 2800/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6995\n",
      "Epoch 2801/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6992\n",
      "Epoch 2802/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6990\n",
      "Epoch 2803/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6989\n",
      "Epoch 2804/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6988\n",
      "Epoch 2805/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6989\n",
      "Epoch 2806/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6990\n",
      "Epoch 2807/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6994\n",
      "Epoch 2808/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6998\n",
      "Epoch 2809/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7006\n",
      "Epoch 2810/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7008\n",
      "Epoch 2811/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7017\n",
      "Epoch 2812/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7012\n",
      "Epoch 2813/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7014\n",
      "Epoch 2814/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7005\n",
      "Epoch 2815/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7001\n",
      "Epoch 2816/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6994\n",
      "Epoch 2817/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.6990\n",
      "Epoch 2818/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.6988\n",
      "Epoch 2819/3000\n",
      "379/379 [==============================] - 0s 17us/step - loss: 2.6987\n",
      "Epoch 2820/3000\n",
      "379/379 [==============================] - 0s 18us/step - loss: 2.6987\n",
      "Epoch 2821/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.6987\n",
      "Epoch 2822/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.6989\n",
      "Epoch 2823/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.6992\n",
      "Epoch 2824/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.6997\n",
      "Epoch 2825/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7000\n",
      "Epoch 2826/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.7006\n",
      "Epoch 2827/3000\n",
      "379/379 [==============================] - 0s 25us/step - loss: 2.7005\n",
      "Epoch 2828/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7009\n",
      "Epoch 2829/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7004\n",
      "Epoch 2830/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7003\n",
      "Epoch 2831/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6997\n",
      "Epoch 2832/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6995\n",
      "Epoch 2833/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6991\n",
      "Epoch 2834/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6989\n",
      "Epoch 2835/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6988\n",
      "Epoch 2836/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6987\n",
      "Epoch 2837/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6985\n",
      "Epoch 2838/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6984\n",
      "Epoch 2839/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6984\n",
      "Epoch 2840/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6985\n",
      "Epoch 2841/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6988\n",
      "Epoch 2842/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6993\n",
      "Epoch 2843/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.7000\n",
      "Epoch 2844/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7003\n",
      "Epoch 2845/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7012\n",
      "Epoch 2846/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7009\n",
      "Epoch 2847/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7013\n",
      "Epoch 2848/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7004\n",
      "Epoch 2849/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7002\n",
      "Epoch 2850/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6994\n",
      "Epoch 2851/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6990\n",
      "Epoch 2852/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2853/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6983\n",
      "Epoch 2854/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6982\n",
      "Epoch 2855/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6983\n",
      "Epoch 2856/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.6984\n",
      "Epoch 2857/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6987\n",
      "Epoch 2858/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6991\n",
      "Epoch 2859/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6995\n",
      "Epoch 2860/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7002\n",
      "Epoch 2861/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7002\n",
      "Epoch 2862/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7008\n",
      "Epoch 2863/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7002\n",
      "Epoch 2864/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7002\n",
      "Epoch 2865/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6995\n",
      "Epoch 2866/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6992\n",
      "Epoch 2867/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6987\n",
      "Epoch 2868/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6984\n",
      "Epoch 2869/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6983\n",
      "Epoch 2870/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6982\n",
      "Epoch 2871/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6981\n",
      "Epoch 2872/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6980\n",
      "Epoch 2873/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6981\n",
      "Epoch 2874/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6982\n",
      "Epoch 2875/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6985\n",
      "Epoch 2876/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6990\n",
      "Epoch 2877/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6997\n",
      "Epoch 2878/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.7000\n",
      "Epoch 2879/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7008\n",
      "Epoch 2880/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7004\n",
      "Epoch 2881/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7007\n",
      "Epoch 2882/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6998\n",
      "Epoch 2883/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6994\n",
      "Epoch 2884/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6987\n",
      "Epoch 2885/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6984\n",
      "Epoch 2886/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6981\n",
      "Epoch 2887/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6979\n",
      "Epoch 2888/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6978\n",
      "Epoch 2889/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6979\n",
      "Epoch 2890/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6979\n",
      "Epoch 2891/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6982\n",
      "Epoch 2892/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6985\n",
      "Epoch 2893/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6989\n",
      "Epoch 2894/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6995\n",
      "Epoch 2895/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6996\n",
      "Epoch 2896/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7001\n",
      "Epoch 2897/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6998\n",
      "Epoch 2898/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6999\n",
      "Epoch 2899/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6993\n",
      "Epoch 2900/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6991\n",
      "Epoch 2901/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6986\n",
      "Epoch 2902/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6983\n",
      "Epoch 2903/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6980\n",
      "Epoch 2904/3000\n",
      "379/379 [==============================] - 0s 15us/step - loss: 2.6978\n",
      "Epoch 2905/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6977\n",
      "Epoch 2906/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6976\n",
      "Epoch 2907/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6977\n",
      "Epoch 2908/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6979\n",
      "Epoch 2909/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6983\n",
      "Epoch 2910/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6988\n",
      "Epoch 2911/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6996\n",
      "Epoch 2912/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6998\n",
      "Epoch 2913/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7005\n",
      "Epoch 2914/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.7000\n",
      "Epoch 2915/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.7000\n",
      "Epoch 2916/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6991\n",
      "Epoch 2917/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6987\n",
      "Epoch 2918/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6981\n",
      "Epoch 2919/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6978\n",
      "Epoch 2920/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6977\n",
      "Epoch 2921/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6975\n",
      "Epoch 2922/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6974\n",
      "Epoch 2923/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6974\n",
      "Epoch 2924/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6975\n",
      "Epoch 2925/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6977\n",
      "Epoch 2926/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6980\n",
      "Epoch 2927/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6984\n",
      "Epoch 2928/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6990\n",
      "Epoch 2929/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6992\n",
      "Epoch 2930/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6998\n",
      "Epoch 2931/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6995\n",
      "Epoch 2932/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6997\n",
      "Epoch 2933/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6990\n",
      "Epoch 2934/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6988\n",
      "Epoch 2935/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6982\n",
      "Epoch 2936/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6979\n",
      "Epoch 2937/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6976\n",
      "Epoch 2938/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6974\n",
      "Epoch 2939/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6973\n",
      "Epoch 2940/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6972\n",
      "Epoch 2941/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6973\n",
      "Epoch 2942/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6975\n",
      "Epoch 2943/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6979\n",
      "Epoch 2944/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6984\n",
      "Epoch 2945/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6992\n",
      "Epoch 2946/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6993\n",
      "Epoch 2947/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 0s 12us/step - loss: 2.7001\n",
      "Epoch 2948/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6995\n",
      "Epoch 2949/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6996\n",
      "Epoch 2950/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6987\n",
      "Epoch 2951/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6983\n",
      "Epoch 2952/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6977\n",
      "Epoch 2953/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6973\n",
      "Epoch 2954/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6971\n",
      "Epoch 2955/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6971\n",
      "Epoch 2956/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6972\n",
      "Epoch 2957/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6976\n",
      "Epoch 2958/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6982\n",
      "Epoch 2959/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6986\n",
      "Epoch 2960/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6994\n",
      "Epoch 2961/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6992\n",
      "Epoch 2962/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6996\n",
      "Epoch 2963/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6988\n",
      "Epoch 2964/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6986\n",
      "Epoch 2965/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6979\n",
      "Epoch 2966/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6975\n",
      "Epoch 2967/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6972\n",
      "Epoch 2968/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6970\n",
      "Epoch 2969/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6969\n",
      "Epoch 2970/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6969\n",
      "Epoch 2971/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6970\n",
      "Epoch 2972/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6972\n",
      "Epoch 2973/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6976\n",
      "Epoch 2974/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6980\n",
      "Epoch 2975/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6985\n",
      "Epoch 2976/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6986\n",
      "Epoch 2977/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6990\n",
      "Epoch 2978/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6987\n",
      "Epoch 2979/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6987\n",
      "Epoch 2980/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6981\n",
      "Epoch 2981/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6979\n",
      "Epoch 2982/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6975\n",
      "Epoch 2983/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6972\n",
      "Epoch 2984/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6970\n",
      "Epoch 2985/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6969\n",
      "Epoch 2986/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6967\n",
      "Epoch 2987/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6967\n",
      "Epoch 2988/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6967\n",
      "Epoch 2989/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6968\n",
      "Epoch 2990/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6971\n",
      "Epoch 2991/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6974\n",
      "Epoch 2992/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6979\n",
      "Epoch 2993/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6982\n",
      "Epoch 2994/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6988\n",
      "Epoch 2995/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6987\n",
      "Epoch 2996/3000\n",
      "379/379 [==============================] - 0s 12us/step - loss: 2.6990\n",
      "Epoch 2997/3000\n",
      "379/379 [==============================] - 0s 11us/step - loss: 2.6985\n",
      "Epoch 2998/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.6983\n",
      "Epoch 2999/3000\n",
      "379/379 [==============================] - 0s 13us/step - loss: 2.6977\n",
      "Epoch 3000/3000\n",
      "379/379 [==============================] - 0s 14us/step - loss: 2.6973\n",
      "379/379 [==============================] - 2s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "from src.calibrated2 import DistributionCalibratedMDNRegressor\n",
    "dataset='boston'\n",
    "(X_tr, y_tr, X_ts, y_ts) = datasets[dataset]\n",
    "clf = BayesianDNNForecaster()\n",
    "clf.fit(X_tr, y_tr)\n",
    "y_ts_pred = clf.predict(X_ts)['y_pred']\n",
    "l_out0 = [dataset] + [metrics[m](y_ts, y_ts_pred) for m in metric_names]\n",
    "\n",
    "print('Recal 1')\n",
    "calibrated_model = CalibratedRegressor(clf)\n",
    "calibrated_model.fit(X_tr, y_tr)\n",
    "\n",
    "print('Recal 2')    \n",
    "calibrated_model2 = DistributionCalibratedMDNRegressor(clf)\n",
    "calibrated_model2.fit(X_tr, y_tr)\n",
    "\n",
    "p_exp = np.arange(0,1,0.05)\n",
    "p_obs = prep_calibration(calibrated_model, X_ts, y_ts, p_exp)\n",
    "p_obs2 = prep_calibration(calibrated_model2, X_ts, y_ts, p_exp)    \n",
    "p_raw = prep_calibration(clf, X_ts, y_ts, p_exp)\n",
    "\n",
    "cal_err0 = mean_calibration_error(p_exp, p_raw)\n",
    "cal_err1 = mean_calibration_error(p_exp, p_obs)\n",
    "cal_err2 = mean_calibration_error(p_exp, p_obs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_err0 = mean_calibration_error(p_exp, p_raw)\n",
    "cal_err1 = mean_calibration_error(p_exp, p_obs)\n",
    "cal_err2 = mean_calibration_error(p_exp, p_obs2)\n",
    "print(cal_err0, cal_err1, cal_err2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpg\n",
      "0.0156367855007\n",
      "0.0274433131121\n",
      "auto\n",
      "0.0381225229099\n",
      "0.0293291973678\n",
      "crime\n",
      "0.0240599406471\n",
      "0.0394288899302\n",
      "kinematics\n",
      "0.00260515105612\n",
      "0.00182319706635\n",
      "stocks\n",
      "0.00353525582265\n",
      "0.00152457254658\n",
      "cpu\n",
      "3.28440222129\n",
      "2.89678779368\n",
      "bank\n",
      "2.02503113196\n",
      "11.9446613856\n",
      "wine\n",
      "0.0491815233549\n",
      "0.0635707336068\n"
     ]
    }
   ],
   "source": [
    "from src.dropout import BayesianDNNForecaster\n",
    "from src.calibrated1 import CalibratedRegressor\n",
    "l_out = []\n",
    "metric_names = ['rmse', 'r2', 'mape']\n",
    "dataset_names = datasets.keys()\n",
    "dataset_names.remove('wisconsin')\n",
    "dataset_names.remove('yacht')\n",
    "\n",
    "for dataset in dataset_names:\n",
    "    print dataset\n",
    "    (X_tr, y_tr, X_ts, y_ts) = datasets[dataset]\n",
    "    clf = BayesianDNNForecaster()\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    y_ts_pred = clf.predict(X_ts)['y_pred']\n",
    "    y_tr_pred = clf.predict(X_tr)['y_pred']    \n",
    "    l_out0 = [dataset] + [metrics[m](y_ts, y_ts_pred) for m in metric_names]\n",
    "    \n",
    "    print root_mean_squared_error(y_tr, y_tr_pred)\n",
    "    print root_mean_squared_error(y_ts, y_ts_pred)\n",
    "    \n",
    "#     calibrated_model = CalibratedRegressor(clf)\n",
    "#     calibrated_model.fit(X_tr, y_tr)\n",
    "    \n",
    "    p_exp = np.arange(0,1,0.05)\n",
    "#     p_obs = prep_calibration(calibrated_model, X_ts, y_ts, p_exp)\n",
    "    p_raw = prep_calibration(clf, X_ts, y_ts, p_exp)\n",
    "    \n",
    "    cal_err0 = mean_calibration_error(p_exp, p_raw)\n",
    "#     cal_err1 = mean_calibration_error(p_exp, p_obs)\n",
    "    l_out0 += [cal_err0, 0]\n",
    "    \n",
    "    l_out += [l_out0]\n",
    "    \n",
    "columns = ['dataset'] + metric_names + ['cal0', 'cal1']\n",
    "df_out = pd.DataFrame(l_out, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "      <th>mape</th>\n",
       "      <th>cal0</th>\n",
       "      <th>cal1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mpg</td>\n",
       "      <td>0.027443</td>\n",
       "      <td>0.844944</td>\n",
       "      <td>0.096247</td>\n",
       "      <td>0.098661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>auto</td>\n",
       "      <td>0.029329</td>\n",
       "      <td>0.987630</td>\n",
       "      <td>0.009092</td>\n",
       "      <td>0.088999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>crime</td>\n",
       "      <td>0.039429</td>\n",
       "      <td>0.371946</td>\n",
       "      <td>0.083240</td>\n",
       "      <td>0.190236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kinematics</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.911160</td>\n",
       "      <td>0.096698</td>\n",
       "      <td>0.229269</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stocks</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.992993</td>\n",
       "      <td>0.008781</td>\n",
       "      <td>0.073548</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cpu</td>\n",
       "      <td>2.896788</td>\n",
       "      <td>0.938827</td>\n",
       "      <td>0.441374</td>\n",
       "      <td>0.164475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bank</td>\n",
       "      <td>11.944661</td>\n",
       "      <td>0.766270</td>\n",
       "      <td>0.343123</td>\n",
       "      <td>0.156215</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wine</td>\n",
       "      <td>0.063571</td>\n",
       "      <td>0.263155</td>\n",
       "      <td>0.102753</td>\n",
       "      <td>0.220382</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset       rmse        r2      mape      cal0  cal1\n",
       "0         mpg   0.027443  0.844944  0.096247  0.098661     0\n",
       "1        auto   0.029329  0.987630  0.009092  0.088999     0\n",
       "2       crime   0.039429  0.371946  0.083240  0.190236     0\n",
       "3  kinematics   0.001823  0.911160  0.096698  0.229269     0\n",
       "4      stocks   0.001525  0.992993  0.008781  0.073548     0\n",
       "5         cpu   2.896788  0.938827  0.441374  0.164475     0\n",
       "6        bank  11.944661  0.766270  0.343123  0.156215     0\n",
       "7        wine   0.063571  0.263155  0.102753  0.220382     0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dropout = df_out\n",
    "df_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpg\n",
      "(98, 5) (98, 5)\n",
      "(98,) (98,)\n",
      "(294, 5) (294, 5)\n",
      "(294,) (294,)\n",
      "0.485383016966\n",
      "0.580803554496\n",
      "auto\n",
      "(454, 5) (454, 5)\n",
      "(454,) (454,)\n",
      "(1359, 5) (1359, 5)\n",
      "(1359,) (1359,)\n",
      "0.11191593406\n",
      "0.0973018242684\n",
      "crime\n",
      "(400, 5) (400, 5)\n",
      "(400,) (400,)\n",
      "(1199, 5) (1199, 5)\n",
      "(1199,) (1199,)\n",
      "0.02904776225\n",
      "0.0249240226746\n",
      "kinematics\n",
      "(2048, 5) (2048, 5)\n",
      "(2048,) (2048,)\n",
      "(6144, 5) (6144, 5)\n",
      "(6144,) (6144,)\n",
      "0.00447829616352\n",
      "0.00408793862216\n",
      "stocks\n",
      "(454, 5) (454, 5)\n",
      "(454,) (454,)\n",
      "(1359, 5) (1359, 5)\n",
      "(1359,) (1359,)\n",
      "0.0222618409831\n",
      "0.027185645703\n",
      "cpu\n",
      "(53, 5) (53, 5)\n",
      "(53,) (53,)\n",
      "(156, 5) (156, 5)\n",
      "(156,) (156,)\n",
      "1.12376362301\n",
      "3.56245241008\n",
      "bank\n",
      "(53, 5) (53, 5)\n",
      "(53,) (53,)\n",
      "(156, 5) (156, 5)\n",
      "(156,) (156,)\n",
      "0.0126116714436\n",
      "9.30052697083\n",
      "wine\n",
      "(400, 5) (400, 5)\n",
      "(400,) (400,)\n",
      "(1199, 5) (1199, 5)\n",
      "(1199,) (1199,)\n",
      "0.0314493702709\n",
      "0.0268638891697\n"
     ]
    }
   ],
   "source": [
    "from src.ensemble import EnsembleDNNForecaster\n",
    "from src.calibrated1 import CalibratedRegressor\n",
    "l_out = []\n",
    "metric_names = ['rmse', 'r2', 'mape']\n",
    "dataset_names = datasets.keys()\n",
    "dataset_names.remove('wisconsin')\n",
    "dataset_names.remove('yacht')\n",
    "\n",
    "for dataset in dataset_names:\n",
    "    print dataset\n",
    "    (X_tr, y_tr, X_ts, y_ts) = datasets[dataset]\n",
    "    clf = EnsembleDNNForecaster()\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    y_ts_pred = clf.predict(X_ts)['y_pred']\n",
    "    y_tr_pred = clf.predict(X_tr)['y_pred']    \n",
    "    l_out0 = [dataset] + [metrics[m](y_ts, y_ts_pred) for m in metric_names]\n",
    "    \n",
    "    print root_mean_squared_error(y_tr, y_tr_pred)\n",
    "    print root_mean_squared_error(y_ts, y_ts_pred)\n",
    "    \n",
    "#     calibrated_model = CalibratedRegressor(clf)\n",
    "#     calibrated_model.fit(X_tr, y_tr)\n",
    "    \n",
    "    p_exp = np.arange(0,1,0.05)\n",
    "#     p_obs = prep_calibration(calibrated_model, X_ts, y_ts, p_exp)\n",
    "    p_raw = prep_calibration(clf, X_ts, y_ts, p_exp)\n",
    "    \n",
    "    cal_err0 = mean_calibration_error(p_exp, p_raw)\n",
    "#     cal_err1 = mean_calibration_error(p_exp, p_obs)\n",
    "    l_out0 += [cal_err0, 0]\n",
    "    \n",
    "    l_out += [l_out0]\n",
    "    \n",
    "columns = ['dataset'] + metric_names + ['cal0', 'cal1']\n",
    "df_ensemble = pd.DataFrame(l_out, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "      <th>mape</th>\n",
       "      <th>cal0</th>\n",
       "      <th>cal1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mpg</td>\n",
       "      <td>0.580804</td>\n",
       "      <td>0.839086</td>\n",
       "      <td>0.093003</td>\n",
       "      <td>0.063105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>auto</td>\n",
       "      <td>0.097302</td>\n",
       "      <td>0.973269</td>\n",
       "      <td>0.012672</td>\n",
       "      <td>0.237200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>crime</td>\n",
       "      <td>0.024924</td>\n",
       "      <td>0.405143</td>\n",
       "      <td>0.081365</td>\n",
       "      <td>0.059311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kinematics</td>\n",
       "      <td>0.004088</td>\n",
       "      <td>0.896072</td>\n",
       "      <td>0.109945</td>\n",
       "      <td>0.029155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stocks</td>\n",
       "      <td>0.027186</td>\n",
       "      <td>0.992562</td>\n",
       "      <td>0.010643</td>\n",
       "      <td>0.223648</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cpu</td>\n",
       "      <td>3.562452</td>\n",
       "      <td>0.935643</td>\n",
       "      <td>0.531536</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bank</td>\n",
       "      <td>9.300527</td>\n",
       "      <td>0.790729</td>\n",
       "      <td>0.320983</td>\n",
       "      <td>0.092316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wine</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.254406</td>\n",
       "      <td>0.102001</td>\n",
       "      <td>0.101407</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset      rmse        r2      mape      cal0  cal1\n",
       "0         mpg  0.580804  0.839086  0.093003  0.063105     0\n",
       "1        auto  0.097302  0.973269  0.012672  0.237200     0\n",
       "2       crime  0.024924  0.405143  0.081365  0.059311     0\n",
       "3  kinematics  0.004088  0.896072  0.109945  0.029155     0\n",
       "4      stocks  0.027186  0.992562  0.010643  0.223648     0\n",
       "5         cpu  3.562452  0.935643  0.531536  0.142236     0\n",
       "6        bank  9.300527  0.790729  0.320983  0.092316     0\n",
       "7        wine  0.026864  0.254406  0.102001  0.101407     0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Bayesian Linear Regression</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Bayesian Neural Network</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Concrete Dropout</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Deep Ensemble</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>MAPE</th>\n",
       "      <th>Calibr.</th>\n",
       "      <th>Recal.</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>Calibr.</th>\n",
       "      <th>Recal.</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>Calibr.</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>Calibr.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mpg</th>\n",
       "      <td>0.099</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auto</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crime</th>\n",
       "      <td>0.087</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kinematics</th>\n",
       "      <td>0.264</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stocks</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu</th>\n",
       "      <td>0.656</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bank</th>\n",
       "      <td>0.576</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine</th>\n",
       "      <td>0.106</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Bayesian Linear Regression                Bayesian Neural Network  \\\n",
       "                                 MAPE Calibr. Recal.                    MAPE   \n",
       "dataset                                                                        \n",
       "mpg                             0.099   0.042  0.037                   0.095   \n",
       "auto                            0.003   0.162  0.067                   0.024   \n",
       "crime                           0.087   0.036  0.022                   0.084   \n",
       "kinematics                      0.264   0.037  0.013                   0.110   \n",
       "stocks                          0.005   0.163  0.016                   0.018   \n",
       "cpu                             0.656   0.120  0.093                   0.471   \n",
       "bank                            0.576   0.104  0.070                   0.330   \n",
       "wine                            0.106   0.031  0.019                   0.103   \n",
       "\n",
       "                          Concrete Dropout         Deep Ensemble          \n",
       "           Calibr. Recal.             MAPE Calibr.          MAPE Calibr.  \n",
       "dataset                                                                   \n",
       "mpg          0.075  0.041            0.096   0.099         0.093   0.063  \n",
       "auto         0.197  0.023            0.009   0.089         0.013   0.237  \n",
       "crime        0.064  0.028            0.083   0.190         0.081   0.059  \n",
       "kinematics   0.051  0.014            0.097   0.229         0.110   0.029  \n",
       "stocks       0.172  0.015            0.009   0.074         0.011   0.224  \n",
       "cpu          0.141  0.064            0.441   0.164         0.532   0.142  \n",
       "bank         0.101  0.040            0.343   0.156         0.321   0.092  \n",
       "wine         0.109  0.023            0.103   0.220         0.102   0.101  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.concat([\n",
    "        df_lin[['dataset', 'mape', 'cal0', 'cal1']], \n",
    "        df_bay[['mape', 'cal0', 'cal1']], \n",
    "        df_dropout[['mape', 'cal0']],\n",
    "        df_ensemble[['mape', 'cal0']]],\n",
    "        axis=1\n",
    ")\n",
    "df_all = df_all.set_index('dataset')\n",
    "df_all = df_all.round(3)\n",
    "\n",
    "header_names = (['Bayesian Linear Regression', 'Bayesian Linear Regression', 'Bayesian Linear Regression', \n",
    "                 'Bayesian Neural Network', 'Bayesian Neural Network', 'Bayesian Neural Network', \n",
    "                 'Concrete Dropout', 'Concrete Dropout', \n",
    "                 'Deep Ensemble', 'Deep Ensemble',],\n",
    "                ['MAPE', 'Calibr.', 'Recal.', \n",
    "                 'MAPE', 'Calibr.', 'Recal.', \n",
    "                 'MAPE', 'Calibr.',\n",
    "                 'MAPE', 'Calibr.',]\n",
    "               )\n",
    "tuples = list(zip(*header_names))\n",
    "header = pd.MultiIndex.from_tuples(tuples)\n",
    "df_all.columns = header\n",
    "\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{3}{l}{Bayesian Linear Regression} & \\multicolumn{3}{l}{Bayesian Neural Network} & \\multicolumn{2}{l}{Concrete Dropout} & \\multicolumn{2}{l}{Deep Ensemble} \\\\\n",
      "{} &                       MAPE & Calibr. & Recal. &                    MAPE & Calibr. & Recal. &             MAPE & Calibr. &          MAPE & Calibr. \\\\\n",
      "dataset    &                            &         &        &                         &         &        &                  &         &               &         \\\\\n",
      "\\midrule\n",
      "mpg        &                      0.107 &   0.053 &  0.057 &                   0.091 &   0.102 &  0.021 &            0.081 &   0.068 &         0.079 &   0.087 \\\\\n",
      "auto       &                      0.002 &   0.175 &  0.029 &                   0.027 &   0.205 &  0.017 &            0.009 &   0.103 &         0.014 &   0.242 \\\\\n",
      "crime      &                      0.086 &   0.030 &  0.016 &                   0.086 &   0.070 &  0.015 &            0.084 &   0.182 &         0.082 &   0.050 \\\\\n",
      "kinematics &                      0.267 &   0.024 &  0.006 &                   0.110 &   0.043 &  0.016 &            0.097 &   0.228 &         0.107 &   0.027 \\\\\n",
      "stocks     &                      0.005 &   0.163 &  0.052 &                   0.020 &   0.183 &  0.024 &            0.011 &   0.039 &         0.013 &   0.229 \\\\\n",
      "cpu        &                      0.395 &   0.074 &  0.025 &                   0.351 &   0.163 &  0.065 &            0.294 &   0.166 &         0.319 &   0.147 \\\\\n",
      "bank       &                      0.572 &   0.073 &  0.083 &                   0.395 &   0.134 &  0.057 &            0.410 &   0.177 &         0.390 &   0.126 \\\\\n",
      "wine       &                      0.101 &   0.024 &  0.022 &                   0.097 &   0.096 &  0.028 &            0.099 &   0.207 &         0.099 &   0.096 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print df_all.to_latex(bold_rows=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afresh-core-4",
   "language": "python",
   "name": "afresh-core-4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
