{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we demonstrate distribution calibration of probabilistic uncertainty over continuous output. \n",
    "\n",
    "At first, we import the necessary files. For this demo, we use the  [California Housing Dataset](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from torchuq.transform.distcal_continuous import *\n",
    "from torchuq.transform.calibrate import *\n",
    "from torchuq.evaluate.distribution_cal import *\n",
    "from torchuq.dataset.regression import *\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torchuq.evaluate import quantile as q_eval\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "uci_dataset = [\"cal_housing\", \"protein\", \"superconductivity\"]\n",
    "\n",
    "subset_uci = [\"cal_housing\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use Bayesian Ridge Regression as the base model to predict probabilistic outcome, represented by the mean and standard deviation as parameters of a Gaussian outcome distribution.\n",
    "\n",
    "We use object of the **DistCalibrator** class to train a recalibrator that takes the probabilistic outcome from the base model and outputs the recalibrated distribution parameterized by a fixed number of equispaced quantiles. In this example, we use 20 equispaced quantiles to featurize the outcome distribution. \n",
    "\n",
    "We use an independent calibration dataset to train the DistCalibrator. We evaluate the quality of probabilistic uncertainty with the check score and calibration score as defined [here](https://arxiv.org/pdf/2112.07184). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset cal_housing....\n",
      "Splitting into train/val/test with 12384/4128/4128 samples\n",
      "Done loading dataset cal_housing\n",
      "Coeff of determination (R^2) on Train: 0.61\n",
      "Coeff of determination (R^2) on Test: 0.6\n",
      "=========================\n",
      "[Calibration Split] Check score before calibration=3.1939690113067627, Check score after quantile calibration=3.1262903213500977, Check score after dist calibration=3.054821014404297\n",
      "=========================\n",
      "[Test Split] Check score before calibration=3.199695587158203, Check score after quantile calibration=3.1359283924102783, Check score after dist calibration=3.0525572299957275\n",
      "=========================\n",
      "Loading dataset protein....\n",
      "Splitting into train/val/test with 27438/9146/9146 samples\n",
      "Done loading dataset protein\n",
      "Coeff of determination (R^2) on Train: 0.28\n",
      "Coeff of determination (R^2) on Test: 0.28\n",
      "=========================\n",
      "[Calibration Split] Check score before calibration=4.603669166564941, Check score after quantile calibration=4.563582420349121, Check score after dist calibration=4.459808826446533\n",
      "=========================\n",
      "[Test Split] Check score before calibration=4.569855213165283, Check score after quantile calibration=4.53015661239624, Check score after dist calibration=4.427648544311523\n",
      "=========================\n",
      "Loading dataset superconductivity....\n",
      "Splitting into train/val/test with 12757/4253/4253 samples\n",
      "Done loading dataset superconductivity\n",
      "Coeff of determination (R^2) on Train: 0.74\n",
      "Coeff of determination (R^2) on Test: 0.73\n",
      "=========================\n",
      "[Calibration Split] Check score before calibration=2.6868534088134766, Check score after quantile calibration=2.6781692504882812, Check score after dist calibration=2.4005584716796875\n",
      "=========================\n",
      "[Test Split] Check score before calibration=2.708859920501709, Check score after quantile calibration=2.7016067504882812, Check score after dist calibration=2.4348721504211426\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "# Number of evaluation buckets\n",
    "num_buckets=20\n",
    "name = \"cal_housing\"\n",
    "for name in uci_dataset:\n",
    "    # 60% Train, 20% Calibration, 20% Test dataset\n",
    "    dataset = get_regression_datasets(name, val_fraction=0.2, test_fraction=0.2, split_seed=0, normalize=True, verbose=True)\n",
    "\n",
    "    train_dataset, cal_dataset, test_dataset = dataset\n",
    "    X_train, y_train = train_dataset[:][0], train_dataset[:][1]\n",
    "    X_cal, y_cal = cal_dataset[:][0], cal_dataset[:][1]\n",
    "    X_test, y_test = test_dataset[:][0], test_dataset[:][1]\n",
    "\n",
    "    # Bayesian Ridge Regression to obtain probabilistic outcomes parameterized by the mean and std deviation of Gaussian outcome for each data-point\n",
    "    reg = BayesianRidge().fit(X_train, y_train)\n",
    "    print(f\"Coeff of determination (R^2) on Train: {reg.score(X_train, y_train):.2}\")\n",
    "    print(f\"Coeff of determination (R^2) on Test: {reg.score(X_test, y_test):.2}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Predict mean and std deviation of the outcome distribution on the calibration and test datasets \n",
    "    mean_cal, std_dev_cal = reg.predict(X_cal.numpy(), return_std=True)\n",
    "    mean_cal, std_dev_cal = torch.Tensor(mean_cal), torch.Tensor(std_dev_cal)\n",
    "\n",
    "    mean_test, std_dev_test = reg.predict(X_test.numpy(), return_std=True)\n",
    "    mean_test, std_dev_test = torch.Tensor(mean_test), torch.Tensor(std_dev_test)\n",
    "\n",
    "    params_cal = torch.cat((mean_cal.reshape(-1, 1), std_dev_cal.reshape(-1, 1)), axis=1)\n",
    "    params_test = torch.cat((mean_test.reshape(-1, 1), std_dev_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "    # Convert probabilistic predictions to quantiles\n",
    "    quantiles_cal = convert_normal_to_quantiles(mean_cal, std_dev_cal, num_buckets)\n",
    "    quantiles_test = convert_normal_to_quantiles(mean_test, std_dev_test, num_buckets)\n",
    "\n",
    "\n",
    "\n",
    "    # Use the DistCalibrator class and train it on the calibration dataset\n",
    "    # Here, the recalibrator uses a fixed number of equispaced quantiles as featurization of the probabilistic outcome\n",
    "    calibrator = DistCalibrator(num_buckets = num_buckets, quantile_input=True, verbose=True)\n",
    "    calibrator.train(quantiles_cal, torch.Tensor(y_cal), num_epochs=10)\n",
    "\n",
    "\n",
    "    quantile_calibrator = RegressionCalibrator()\n",
    "    input_cdf = torch.distributions.Normal(mean_cal, std_dev_cal).cdf(y_cal)\n",
    "    empirical_cdf = compute_empirical_cdf(input_cdf)\n",
    "    quantile_calibrator.train(input_cdf, empirical_cdf)\n",
    "\n",
    "\n",
    "    calibrated_quantiles_cal = convert_normal_cdf_to_quantiles(mean_cal, std_dev_cal, torch.Tensor(quantile_calibrator.inverse_calibrator(num_buckets=num_buckets)))\n",
    "    calibrated_quantiles_test = convert_normal_cdf_to_quantiles(mean_test, std_dev_test, torch.Tensor(quantile_calibrator.inverse_calibrator(num_buckets=num_buckets)))\n",
    "\n",
    "    # Below code is needed if you featurized the Gaussian probabilistic outcome using their parameters mean and std deviation\n",
    "    # calibrator = DistCalibrator(quantile_input=False, verbose=True)\n",
    "    # calibrator.train(params_cal, torch.Tensor(y_cal))\n",
    "\n",
    "    # Evaluation\n",
    "    # \n",
    "\n",
    "    # Compare check scores and weighted calibrations cores \n",
    "    print(\"=\"*25)\n",
    "    check_score_before, check_score_after = comparison_quantile_check_score(quantiles_cal, torch.Tensor(y_cal), np.linspace(0, 1, num_buckets), model=calibrator)\n",
    "\n",
    "    _ , check_score_baseline = comparison_quantile_check_score(quantiles_cal, torch.Tensor(y_cal), np.linspace(0, 1, num_buckets), quant_calibrated_outcome=calibrated_quantiles_cal)\n",
    "\n",
    "    print(f\"[Calibration Split] Check score before calibration={check_score_before}, Check score after quantile calibration={check_score_baseline}, Check score after dist calibration={check_score_after}\")\n",
    "\n",
    "\n",
    "    print(\"=\"*25)\n",
    "\n",
    "    check_score_before, check_score_after = comparison_quantile_check_score(quantiles_test, torch.Tensor(y_test), np.linspace(0, 1, num_buckets), model=calibrator)\n",
    "\n",
    "    _, check_score_baseline = comparison_quantile_check_score(quantiles_test, torch.Tensor(y_test), np.linspace(0, 1, num_buckets), quant_calibrated_outcome=calibrated_quantiles_test)\n",
    "\n",
    "    print(f\"[Test Split] Check score before calibration={check_score_before}, Check score after quantile calibration={check_score_baseline}, Check score after dist calibration={check_score_after}\")\n",
    "\n",
    "    print(\"=\"*25)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icml22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
