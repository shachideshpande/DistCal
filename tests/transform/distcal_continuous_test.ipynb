{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we demonstrate distribution calibration of probabilistic uncertainty over continuous output. \n",
    "\n",
    "At first, we import the necessary files. For this demo, we use the  [California Housing Dataset](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from torchuq.transform.distcal_continuous import *\n",
    "from torchuq.transform.calibrate import *\n",
    "from torchuq.evaluate.distribution_cal import *\n",
    "from torchuq.dataset.regression import *\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torchuq.evaluate import quantile as q_eval\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#uci_dataset = [\"wine\", \"crime\", \"naval\", \"protein\", \"superconductivity\"]\n",
    "\n",
    "subset_uci = [\"cal_housing\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use Bayesian Ridge Regression as the base model to predict probabilistic outcome, represented by the mean and standard deviation as parameters of a Gaussian outcome distribution.\n",
    "\n",
    "We use object of the **DistCalibrator** class to train a recalibrator that takes the probabilistic outcome from the base model and outputs the recalibrated distribution parameterized by a fixed number of equispaced quantiles. In this example, we use 20 equispaced quantiles to featurize the outcome distribution. \n",
    "\n",
    "We use an independent calibration dataset to train the DistCalibrator. We evaluate the quality of probabilistic uncertainty with the check score and calibration score as defined [here](https://arxiv.org/pdf/2112.07184). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of evaluation buckets\n",
    "# num_buckets=20\n",
    "\n",
    "# for name in subset_uci:\n",
    "# \t# 60% Train, 20% Calibration, 20% Test dataset\n",
    "# \tdataset = get_regression_datasets(name, val_fraction=0.2, test_fraction=0.2, split_seed=0, normalize=True, verbose=True)\n",
    "\t\n",
    "# \ttrain_dataset, cal_dataset, test_dataset = dataset\n",
    "# \tX_train, y_train = train_dataset[:][0], train_dataset[:][1]\n",
    "# \tX_cal, y_cal = cal_dataset[:][0], cal_dataset[:][1]\n",
    "# \tX_test, y_test = test_dataset[:][0], test_dataset[:][1]\n",
    "\t\n",
    "# \t# Bayesian Ridge Regression to obtain probabilistic outcomes parameterized by the mean and std deviation of Gaussian outcome for each data-point\n",
    "# \treg = BayesianRidge().fit(X_train, y_train)\n",
    "# \tprint(f\"Coeff of determination (R^2) on Train: {reg.score(X_train, y_train):.2}\")\n",
    "# \tprint(f\"Coeff of determination (R^2) on Test: {reg.score(X_test, y_test):.2}\")\n",
    "\t\n",
    "\n",
    "\n",
    "# \t# Predict mean and std deviation of the outcome distribution on the calibration and test datasets \n",
    "# \tmean_cal, std_dev_cal = reg.predict(X_cal.numpy(), return_std=True)\n",
    "# \tmean_cal, std_dev_cal = torch.Tensor(mean_cal), torch.Tensor(std_dev_cal)\n",
    "\n",
    "# \tmean_test, std_dev_test = reg.predict(X_test.numpy(), return_std=True)\n",
    "# \tmean_test, std_dev_test = torch.Tensor(mean_test), torch.Tensor(std_dev_test)\n",
    "\n",
    "# \tparams_cal = torch.cat((mean_cal.reshape(-1, 1), std_dev_cal.reshape(-1, 1)), axis=1)\n",
    "# \tparams_test = torch.cat((mean_test.reshape(-1, 1), std_dev_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "# \t# Convert probabilistic predictions to quantiles\n",
    "# \tquantiles_cal = convert_normal_to_quantiles(mean_cal, std_dev_cal, num_buckets)\n",
    "# \tquantiles_test = convert_normal_to_quantiles(mean_test, std_dev_test, num_buckets)\n",
    "\n",
    "\t\n",
    "\n",
    "# \t# Use the DistCalibrator class and train it on the calibration dataset\n",
    "# \t# Here, the recalibrator uses a fixed number of equispaced quantiles as featurization of the probabilistic outcome\n",
    "# \tcalibrator = DistCalibrator(num_buckets = num_buckets, quantile_input=True, verbose=True)\n",
    "# \tcalibrator.train(quantiles_cal, torch.Tensor(y_cal))\n",
    "\n",
    "# \tquantile_calibrator = RegressionCalibrator()\n",
    "# \tinput_cdf = torch.distributions.Normal(mean_cal, std_dev_cal).cdf(y_cal)\n",
    "# \tempirical_cdf = compute_empirical_cdf(input_cdf)\n",
    "# \tquantile_calibrator.fit(input_cdf, empirical_cdf)\n",
    "\n",
    "\n",
    "# \tcalibrated_quantiles_cal = convert_normal_cdf_to_quantiles(mean_cal, std_dev_cal, quantile_calibrator.inverse_calibrator(num_buckets=num_buckets))\n",
    "# \tcalibrated_quantiles_test = convert_normal_cdf_to_quantiles(mean_test, std_dev_test, quantile_calibrator.inverse_calibrator(num_buckets=num_buckets))\n",
    "\n",
    "# \t# Below code is needed if you featurized the Gaussian probabilistic outcome using their parameters mean and std deviation\n",
    "# \t# calibrator = DistCalibrator(quantile_input=False, verbose=True)\n",
    "# \t# calibrator.train(params_cal, torch.Tensor(y_cal))\n",
    "\n",
    "# \t# Evaluation\n",
    "# \t# \n",
    "\t\n",
    "# \t# Compare check scores and weighted calibrations cores \n",
    "# \tprint(\"=\"*25)\n",
    "# \tcheck_score_before, check_score_after = comparison_quantile_check_score(quantiles_cal, torch.Tensor(y_cal), np.linspace(0, 1, num_buckets), model=calibrator)\n",
    "\n",
    "# \t_ , check_score_baseline = comparison_quantile_check_score(quantiles_cal, torch.Tensor(y_cal), np.linspace(0, 1, num_buckets), quant_calibrated_outcome=calibrated_quantiles_cal)\n",
    "\n",
    "# \tprint(f\"[Calibration Split] Check score before calibration={check_score_before}, Check score after quantile calibration={check_score_baseline}, Check score after dist calibration={check_score_after}\")\n",
    "\n",
    "\n",
    "# \tcal_score_before, cal_score_after = comparison_quantile_calibration_scores(quantiles_cal, torch.Tensor(y_cal), np.linspace(0, 1, num_buckets), model=calibrator)\n",
    "# \tcal_score_baseline, _ = comparison_quantile_calibration_scores(quantiles_cal, torch.Tensor(y_cal), np.linspace(0, 1, num_buckets), model=None, old_quantiles=calibrated_quantiles_cal)\n",
    "\n",
    "# \tprint(f\"[Calibration Split] Calibration score before calibration={cal_score_before}, Calibration score after quantile calibration={cal_score_baseline} , Calibration score after dist calibration={cal_score_after}\")\n",
    "\n",
    "# \tprint(\"=\"*25)\n",
    "\n",
    "# \tcheck_score_before, check_score_after = comparison_quantile_check_score(quantiles_test, torch.Tensor(y_test), np.linspace(0, 1, num_buckets), model=calibrator)\n",
    "\n",
    "# \t_, check_score_baseline = comparison_quantile_check_score(quantiles_test, torch.Tensor(y_test), np.linspace(0, 1, num_buckets), quant_calibrated_outcome=calibrated_quantiles_test)\n",
    "\n",
    "# \tprint(f\"[Test Split] Check score before calibration={check_score_before}, Check score after quantile calibration={cal_score_baseline}, Check score after dist calibration={check_score_after}\")\n",
    "\n",
    "# \tcal_score_before, cal_score_after = comparison_quantile_calibration_scores(quantiles_test, torch.Tensor(y_test), np.linspace(0, 1, num_buckets), model=calibrator)\n",
    "\n",
    "# \tcal_score_baseline, _ = comparison_quantile_calibration_scores(quantiles_test, torch.Tensor(y_test), np.linspace(0, 1, num_buckets), model=None, old_quantiles=calibrated_quantiles_test)\n",
    "\t\n",
    "# \tprint(f\"[Test Split] Calibration score before calibration={cal_score_before}, Calibration score after quantile calibration={cal_score_baseline}, Calibration score after dist calibration={cal_score_after}\")\n",
    "# \tprint(\"=\"*25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset cal_housing....\n",
      "Splitting into train/val/test with 12384/4128/4128 samples\n",
      "Done loading dataset cal_housing\n",
      "Coeff of determination (R^2) on Train: 0.61\n",
      "Coeff of determination (R^2) on Test: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:22<00:00, 66.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Number of evaluation buckets\n",
    "num_buckets=20\n",
    "name = \"cal_housing\"\n",
    "\n",
    "# 60% Train, 20% Calibration, 20% Test dataset\n",
    "dataset = get_regression_datasets(name, val_fraction=0.2, test_fraction=0.2, split_seed=0, normalize=True, verbose=True)\n",
    "\n",
    "train_dataset, cal_dataset, test_dataset = dataset\n",
    "X_train, y_train = train_dataset[:][0], train_dataset[:][1]\n",
    "X_cal, y_cal = cal_dataset[:][0], cal_dataset[:][1]\n",
    "X_test, y_test = test_dataset[:][0], test_dataset[:][1]\n",
    "\n",
    "# Bayesian Ridge Regression to obtain probabilistic outcomes parameterized by the mean and std deviation of Gaussian outcome for each data-point\n",
    "reg = BayesianRidge().fit(X_train, y_train)\n",
    "print(f\"Coeff of determination (R^2) on Train: {reg.score(X_train, y_train):.2}\")\n",
    "print(f\"Coeff of determination (R^2) on Test: {reg.score(X_test, y_test):.2}\")\n",
    "\n",
    "\n",
    "\n",
    "# Predict mean and std deviation of the outcome distribution on the calibration and test datasets \n",
    "mean_cal, std_dev_cal = reg.predict(X_cal.numpy(), return_std=True)\n",
    "mean_cal, std_dev_cal = torch.Tensor(mean_cal), torch.Tensor(std_dev_cal)\n",
    "\n",
    "mean_test, std_dev_test = reg.predict(X_test.numpy(), return_std=True)\n",
    "mean_test, std_dev_test = torch.Tensor(mean_test), torch.Tensor(std_dev_test)\n",
    "\n",
    "params_cal = torch.cat((mean_cal.reshape(-1, 1), std_dev_cal.reshape(-1, 1)), axis=1)\n",
    "params_test = torch.cat((mean_test.reshape(-1, 1), std_dev_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "# Convert probabilistic predictions to quantiles\n",
    "quantiles_cal = convert_normal_to_quantiles(mean_cal, std_dev_cal, num_buckets)\n",
    "quantiles_test = convert_normal_to_quantiles(mean_test, std_dev_test, num_buckets)\n",
    "\n",
    "\n",
    "\n",
    "# Use the DistCalibrator class and train it on the calibration dataset\n",
    "# Here, the recalibrator uses a fixed number of equispaced quantiles as featurization of the probabilistic outcome\n",
    "calibrator = DistCalibrator(num_buckets = num_buckets, quantile_input=True, verbose=True)\n",
    "calibrator.train(quantiles_cal, torch.Tensor(y_cal), num_epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchuq.transform.distcal_continuous import convert_normal_cdf_to_quantiles\n",
    "# from torchuq.transform.distcal_continuous import convert_normal_to_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "tensor(3.1940) tensor(2.9693)\n",
      "19 1.0 tensor([3.9249, 4.3757, 3.6061]) tensor([-0.6072,  0.2980, -0.0540]) tensor(0.)\n",
      "tensor(3.1940) tensor(3.1263)\n",
      "[Calibration Split] Check score before calibration=3.1939690113067627, Check score after quantile calibration=3.1262903213500977, Check score after dist calibration=2.9693214893341064\n",
      "[Calibration Split] Calibration score before calibration=0.04242164668846845, Calibration score after quantile calibration=1.8836053354855378e-07 , Calibration score after dist calibration=0.009871298769362314\n",
      "=========================\n",
      "tensor(3.1997) tensor(2.9658)\n",
      "19 1.0 tensor([3.4195, 4.4616, 3.8689]) tensor([ 2.5410,  0.5468, -0.0939]) tensor(5.7109e-05)\n",
      "tensor(3.1997) tensor(3.1359)\n",
      "[Test Split] Check score before calibration=3.199695587158203, Check score after quantile calibration=3.1359283924102783, Check score after dist calibration=2.9658091068267822\n",
      "[Test Split] Calibration score before calibration=0.03905351046967815, Calibration score after quantile calibration=0.00028768447603074295, Calibration score after dist calibration=0.009768624085810081\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "def compute_empirical_cdf(cdf):\n",
    "    \"\"\"\n",
    "    This function takes the output CDF as produced by probabilistic model corresponding to each observed outcome in the dataset to produce empirical CDF \n",
    "    This will be used to perform quantile calibration in Algorithm 1 of Kuleshov 2018\n",
    "    \"\"\"\n",
    "\n",
    "    empirical_cdf = (torch.Tensor([torch.sum(cdf<=p) for p in cdf]))/len(cdf)\n",
    "\n",
    "    return empirical_cdf\n",
    "def convert_normal_cdf_to_quantiles(mean, std_dev, cdf_values):\n",
    "    \"\"\" Converts prediction in the form of Normal distribution over outcomes to equispaced quantiles equal to num_buckets\n",
    "\n",
    "        Args:\n",
    "            mean (tensor): a batch of scalar means\n",
    "            std_dev (tensor): a batch of scalar outcomes\n",
    "            \n",
    "        Returns:\n",
    "            tensor: batch of cdf predictions represented as equispaced quantiles with the shape [batch_size, num_buckets]  \n",
    "        \"\"\"\n",
    "    normal_dist = Normal(mean, std_dev)\n",
    "    # print(cdf_values)\n",
    "    # print(mean.shape, cdf_values.shape)\n",
    "    quantiles=torch.zeros((mean.shape[0], cdf_values.shape[0]))\n",
    "\n",
    "    for i, cdf_value in enumerate(cdf_values):\n",
    "\n",
    "        quantiles[:, i] = normal_dist.icdf(cdf_value).T\n",
    "    quantiles[:, -1] = mean+6*std_dev \n",
    "    return quantiles\n",
    "def comparison_quantile_check_score(x_data, y_data, taus=np.linspace(0, 1, num=11), model=None, quant_calibrated_outcome=None):\n",
    "    \"\"\" \n",
    "    Computes check scores before and after applying recalibrator in the continuous outcome setting where distribution is featurized using equispaced quantiles. \n",
    "    \"\"\"\n",
    "    if x_data is None and y_data is None:\n",
    "        return 0, 0\n",
    "    check_score_before, check_score_after = 0, 0\n",
    "    for i, tau in enumerate(taus):\n",
    "        temp_before = check_score(tau, x_data[:, i], y_data).mean()\n",
    "        if model:\n",
    "          calibrated_outcome = model.model.predict((tau, x_data))[0]\n",
    "        else:\n",
    "          calibrated_outcome = quant_calibrated_outcome[:, i]\n",
    "          # calibrated_outcome = x_data[:, i]\n",
    "        temp_after = check_score(tau, calibrated_outcome, y_data).mean()\n",
    "        if not model and i>18:\n",
    "            print(i, tau, calibrated_outcome.detach()[:3], y_data.detach()[:3], temp_after.detach())\n",
    "        check_score_before += temp_before\n",
    "        check_score_after += temp_after\n",
    "    print(check_score_before, check_score_after.detach())\n",
    "    return check_score_before, check_score_after.detach()\n",
    "\n",
    "def comparison_quantile_calibration_scores(x_data, y_data, taus=np.linspace(0, 1, num=11), model=None, old_quantiles=None):\n",
    "    \"\"\" \n",
    "    Computes weighted calibration scores before and after applying recalibrator in the continuous outcome setting where distribution is featurized using equispaced quantiles. \n",
    "    \"\"\"\n",
    "    # Todo: remove redundant parts\n",
    "    if x_data is None and y_data is None:\n",
    "        return (0, 0)\n",
    "    \n",
    "    if old_quantiles is None:\n",
    "      old_quantiles = x_data\n",
    "    \n",
    "    num_buckets = len(taus)\n",
    "    expected_tau = taus\n",
    "    empirical_tau = np.zeros(num_buckets)\n",
    "    empirical_tau_old = np.zeros(num_buckets)\n",
    "    for tau_i, tau in enumerate(expected_tau):\n",
    "        q_old = old_quantiles[:, tau_i]\n",
    "        if model:\n",
    "          q_new = model.model.predict((tau, x_data))[0]\n",
    "        else:\n",
    "          q_new = q_old\n",
    "       \n",
    "\n",
    "        # Todo: write efficient code\n",
    "        for outcome_i, outcome in enumerate(y_data):\n",
    "\n",
    "            if(outcome<=q_new[outcome_i]):\n",
    "                empirical_tau[tau_i]+=1\n",
    "            if(outcome<=q_old[outcome_i]):\n",
    "                empirical_tau_old[tau_i]+=1\n",
    "        empirical_tau[tau_i]/=len(y_data)\n",
    "        empirical_tau_old[tau_i]/=len(y_data)\n",
    "\n",
    "        empirical_tau[len(empirical_tau)-1]=1\n",
    "        empirical_tau_old[len(empirical_tau)-1]=1\n",
    "        cal_score_before = (((empirical_tau_old-expected_tau)**2)*empirical_tau_old).sum()\n",
    "        cal_score_after = (((empirical_tau-expected_tau)**2)*empirical_tau).sum()\n",
    "\n",
    "    return (cal_score_before, cal_score_after)\n",
    "\n",
    "quantile_calibrator = RegressionCalibrator()\n",
    "input_cdf = torch.distributions.Normal(mean_cal, std_dev_cal).cdf(y_cal)\n",
    "empirical_cdf = compute_empirical_cdf(input_cdf)\n",
    "quantile_calibrator.train(input_cdf, empirical_cdf)\n",
    "\n",
    "\n",
    "calibrated_quantiles_cal = convert_normal_cdf_to_quantiles(mean_cal, std_dev_cal, torch.Tensor(quantile_calibrator.inverse_calibrator(num_buckets=num_buckets)))\n",
    "calibrated_quantiles_test = convert_normal_cdf_to_quantiles(mean_test, std_dev_test, torch.Tensor(quantile_calibrator.inverse_calibrator(num_buckets=num_buckets)))\n",
    "\n",
    "# Below code is needed if you featurized the Gaussian probabilistic outcome using their parameters mean and std deviation\n",
    "# calibrator = DistCalibrator(quantile_input=False, verbose=True)\n",
    "# calibrator.train(params_cal, torch.Tensor(y_cal))\n",
    "\n",
    "# Evaluation\n",
    "# \n",
    "\n",
    "# Compare check scores and weighted calibrations cores \n",
    "print(\"=\"*25)\n",
    "check_score_before, check_score_after = comparison_quantile_check_score(quantiles_cal, torch.Tensor(y_cal), np.linspace(0, 1, num_buckets), model=calibrator)\n",
    "\n",
    "_ , check_score_baseline = comparison_quantile_check_score(quantiles_cal, torch.Tensor(y_cal), np.linspace(0, 1, num_buckets), quant_calibrated_outcome=calibrated_quantiles_cal)\n",
    "\n",
    "print(f\"[Calibration Split] Check score before calibration={check_score_before}, Check score after quantile calibration={check_score_baseline}, Check score after dist calibration={check_score_after}\")\n",
    "\n",
    "\n",
    "cal_score_before, cal_score_after = comparison_quantile_calibration_scores(quantiles_cal, torch.Tensor(y_cal), np.linspace(0, 1, num_buckets), model=calibrator)\n",
    "cal_score_baseline, _ = comparison_quantile_calibration_scores(quantiles_cal, torch.Tensor(y_cal), np.linspace(0, 1, num_buckets), model=None, old_quantiles=calibrated_quantiles_cal)\n",
    "\n",
    "print(f\"[Calibration Split] Calibration score before calibration={cal_score_before}, Calibration score after quantile calibration={cal_score_baseline} , Calibration score after dist calibration={cal_score_after}\")\n",
    "\n",
    "print(\"=\"*25)\n",
    "\n",
    "check_score_before, check_score_after = comparison_quantile_check_score(quantiles_test, torch.Tensor(y_test), np.linspace(0, 1, num_buckets), model=calibrator)\n",
    "\n",
    "_, check_score_baseline = comparison_quantile_check_score(quantiles_test, torch.Tensor(y_test), np.linspace(0, 1, num_buckets), quant_calibrated_outcome=calibrated_quantiles_test)\n",
    "\n",
    "print(f\"[Test Split] Check score before calibration={check_score_before}, Check score after quantile calibration={check_score_baseline}, Check score after dist calibration={check_score_after}\")\n",
    "\n",
    "cal_score_before, cal_score_after = comparison_quantile_calibration_scores(quantiles_test, torch.Tensor(y_test), np.linspace(0, 1, num_buckets), model=calibrator)\n",
    "\n",
    "cal_score_baseline, _ = comparison_quantile_calibration_scores(quantiles_test, torch.Tensor(y_test), np.linspace(0, 1, num_buckets), model=None, old_quantiles=calibrated_quantiles_test)\n",
    "\n",
    "print(f\"[Test Split] Calibration score before calibration={cal_score_before}, Calibration score after quantile calibration={cal_score_baseline}, Calibration score after dist calibration={cal_score_after}\")\n",
    "print(\"=\"*25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.30345535e-05, 1.05764703e-01, 1.57659359e-01, 1.97080797e-01,\n",
       "       2.31788295e-01, 2.62724499e-01, 2.94475439e-01, 3.24760172e-01,\n",
       "       3.58738512e-01, 4.00818915e-01, 4.39831724e-01, 4.83572265e-01,\n",
       "       5.29090189e-01, 5.84218020e-01, 6.46472821e-01, 7.16700115e-01,\n",
       "       7.94666101e-01, 8.84678976e-01, 9.67842391e-01, 1.00000000e+00])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_calibrator.inverse_recalibrator.predict(torch.Tensor(np.linspace(0, 1, num=num_buckets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0526, 0.1053, 0.1579, 0.2105, 0.2632, 0.3158, 0.3684, 0.4211,\n",
       "        0.4737, 0.5263, 0.5789, 0.6316, 0.6842, 0.7368, 0.7895, 0.8421, 0.8947,\n",
       "        0.9474, 1.0000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(np.linspace(0, 1, num=num_buckets))\n",
    "# num_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4128]) torch.Size([4128])\n",
      "tensor([0.1040, 0.2976, 0.5547,  ..., 0.7264, 0.3942, 0.2152])\n",
      "tensor([0.0516, 0.3203, 0.6555,  ..., 0.7972, 0.4625, 0.1865])\n"
     ]
    }
   ],
   "source": [
    "print(input_cdf.shape, empirical_cdf.shape)\n",
    "print(input_cdf)\n",
    "print(empirical_cdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icml22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
