{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we demonstrate distribution calibration of probabilistic uncertainty over continuous output. \n",
    "\n",
    "At first, we import the necessary files. For this demo, we use the  [California Housing Dataset](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from torchuq.transform.distcal_continuous import *\n",
    "\n",
    "from torchuq.evaluate.distribution_cal import *\n",
    "from torchuq.dataset.regression import *\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torchuq.evaluate import quantile as q_eval\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#uci_dataset = [\"wine\", \"crime\", \"naval\", \"protein\", \"superconductivity\"]\n",
    "\n",
    "subset_uci = [\"cal_housing\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use Bayesian Ridge Regression as the base model to predict probabilistic outcome, represented by the mean and standard deviation as parameters of a Gaussian outcome distribution.\n",
    "\n",
    "We use object of the **DistCalibrator** class to train a recalibrator that takes the probabilistic outcome from the base model and outputs the recalibrated distribution parameterized by a fixed number of equispaced quantiles. In this example, we use 20 equispaced quantiles to featurize the outcome distribution. \n",
    "\n",
    "We use an independent calibration dataset to train the DistCalibrator. We evaluate the quality of probabilistic uncertainty with the check score and calibration score as defined [here](https://arxiv.org/pdf/2112.07184). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset cal_housing....\n",
      "Splitting into train/val/test with 12384/4128/4128 samples\n",
      "Done loading dataset cal_housing\n",
      "Coeff of determination (R^2) on Train: 0.61\n",
      "Coeff of determination (R^2) on Test: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [01:51<00:00, 13.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "[Calibration Split] Check score before calibration=3.1939690113067627, Check score after calibration=2.9653549194335938\n",
      "[Calibration Split] Calibration score before calibration=0.04242164668846845, Calibration score after calibration=0.009161099773619456\n",
      "=========================\n",
      "[Test Split] Check score before calibration=3.199695587158203, Check score after calibration=2.962188243865967\n",
      "[Test Split] Calibration score before calibration=0.03905351046967815, Calibration score after calibration=0.009016458873366034\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "# Number of evaluation buckets\n",
    "num_buckets=20\n",
    "\n",
    "for name in subset_uci:\n",
    "\t# 60% Train, 20% Calibration, 20% Test dataset\n",
    "\tdataset = get_regression_datasets(name, val_fraction=0.2, test_fraction=0.2, split_seed=0, normalize=True, verbose=True)\n",
    "\t\n",
    "\ttrain_dataset, cal_dataset, test_dataset = dataset\n",
    "\tX_train, y_train = train_dataset[:][0], train_dataset[:][1]\n",
    "\tX_cal, y_cal = cal_dataset[:][0], cal_dataset[:][1]\n",
    "\tX_test, y_test = test_dataset[:][0], test_dataset[:][1]\n",
    "\t\n",
    "\t# Bayesian Ridge Regression to obtain probabilistic outcomes parameterized by the mean and std deviation of Gaussian outcome for each data-point\n",
    "\treg = BayesianRidge().fit(X_train, y_train)\n",
    "\tprint(f\"Coeff of determination (R^2) on Train: {reg.score(X_train, y_train):.2}\")\n",
    "\tprint(f\"Coeff of determination (R^2) on Test: {reg.score(X_test, y_test):.2}\")\n",
    "\t\n",
    "\n",
    "\n",
    "\t# Predict mean and std deviation of the outcome distribution on the calibration and test datasets \n",
    "\tmean_cal, std_dev_cal = reg.predict(X_cal.numpy(), return_std=True)\n",
    "\tmean_cal, std_dev_cal = torch.Tensor(mean_cal), torch.Tensor(std_dev_cal)\n",
    "\n",
    "\tmean_test, std_dev_test = reg.predict(X_test.numpy(), return_std=True)\n",
    "\tmean_test, std_dev_test = torch.Tensor(mean_test), torch.Tensor(std_dev_test)\n",
    "\n",
    "\tparams_cal = torch.cat((mean_cal.reshape(-1, 1), std_dev_cal.reshape(-1, 1)), axis=1)\n",
    "\tparams_test = torch.cat((mean_test.reshape(-1, 1), std_dev_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "\t# Convert probabilistic predictions to quantiles\n",
    "\tquantiles_cal = convert_normal_to_quantiles(mean_cal, std_dev_cal, num_buckets)\n",
    "\tquantiles_test = convert_normal_to_quantiles(mean_test, std_dev_test, num_buckets)\n",
    "\n",
    "\t\n",
    "\n",
    "\t# Use the DistCalibrator class and train it on the calibration dataset\n",
    "\t# Here, the recalibrator uses a fixed number of equispaced quantiles as featurization of the probabilistic outcome\n",
    "\tcalibrator = DistCalibrator(num_buckets = num_buckets, quantile_input=True, verbose=True)\n",
    "\tcalibrator.train(quantiles_cal, torch.Tensor(y_cal))\n",
    "\n",
    "\t# Below code is needed if you featurized the Gaussian probabilistic outcome using their parameters mean and std deviation\n",
    "\t# calibrator = DistCalibrator(quantile_input=False, verbose=True)\n",
    "\t# calibrator.train(params_cal, torch.Tensor(y_cal))\n",
    "\n",
    "\t# Evaluation\n",
    "\t# \n",
    "\t\n",
    "\t# Compare check scores and weighted calibrations cores \n",
    "\tprint(\"=\"*25)\n",
    "\tcheck_score_before, check_score_after = comparison_quantile_check_score(quantiles_cal, torch.Tensor(y_cal), np.linspace(0, 1, num_buckets), model=calibrator)\n",
    "\n",
    "\tprint(f\"[Calibration Split] Check score before calibration={check_score_before}, Check score after calibration={check_score_after}\")\n",
    "\n",
    "\n",
    "\tcal_score_before, cal_score_after = comparison_quantile_calibration_scores(quantiles_cal, torch.Tensor(y_cal), np.linspace(0, 1, num_buckets), model=calibrator)\n",
    "\n",
    "\tprint(f\"[Calibration Split] Calibration score before calibration={cal_score_before}, Calibration score after calibration={cal_score_after}\")\n",
    "\n",
    "\tprint(\"=\"*25)\n",
    "\n",
    "\tcheck_score_before, check_score_after = comparison_quantile_check_score(quantiles_test, torch.Tensor(y_test), np.linspace(0, 1, num_buckets), model=calibrator)\n",
    "\n",
    "\tprint(f\"[Test Split] Check score before calibration={check_score_before}, Check score after calibration={check_score_after}\")\n",
    "\n",
    "\tcal_score_before, cal_score_after = comparison_quantile_calibration_scores(quantiles_test, torch.Tensor(y_test), np.linspace(0, 1, num_buckets), model=calibrator)\n",
    "\t\n",
    "\tprint(f\"[Test Split] Calibration score before calibration={cal_score_before}, Calibration score after calibration={cal_score_after}\")\n",
    "\tprint(\"=\"*25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchuq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
