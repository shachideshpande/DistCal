{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372bc096",
   "metadata": {},
   "source": [
    "Below, we demonstrate distribution calibration of probabilistic outcomes over discrete output. \n",
    "\n",
    "At first, we import the necessary files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b718fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from torchuq.transform.distcal_discrete import *\n",
    "from torchuq.evaluate.distribution_cal import *\n",
    "from torchuq.dataset.classification import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torchuq.evaluate import categorical\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e05693",
   "metadata": {},
   "source": [
    "For this demo, we use the [UCI Digit classification dataset](https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f11d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "uci_datasets = ['digits', 'adult', 'iris', 'breast-cancer']\n",
    "subset_uci = ['digits']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b559a",
   "metadata": {},
   "source": [
    "Below, we use Logistic Regression as a simple base model to predict the probabilistic outcome $(p_0, p_1,..,p_{9})$ over the discrete output ranging from 0 to 9. \n",
    "\n",
    "We use object of the *DiscreteDistCalibrator* class to train a recalibrator that takes the probabilistic outcome from the base model and outputs the recalibrated distribution represented as $(p'_0, p'_1,..,p'_{9})$ . \n",
    "\n",
    "We use an independent calibration dataset to train the DiscreteDistCalibrator. We evaluate the quality of probabilistic uncertainty with calibration score as defined [here](https://arxiv.org/pdf/2112.07184). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "365a8188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset digits....\n",
      "Splitting into train/val/test with 1079/359/359 samples\n",
      "Done loading dataset digits\n",
      "==================================================\n",
      "Classification accuracy on Train: 1.0\n",
      "Classification accuracy on Test: 0.96\n",
      "==================================================\n",
      "==================================================\n",
      "[Calibration Dataset] Log loss before calibration = 0.11842121565418677; After calibration=0.10612225788495672\n",
      "[Calibration Dataset] Calibration score before calibration = 0.11454739304221799, After calibration = 0.0562264395008079\n",
      "[Test Dataset] Log loss before calibration = 0.1602416572992105; After calibration=0.1602416572992105\n",
      "[Test Dataset] Calibration score before calibration = 0.07278033250383055, After calibration = 0.07278033250383055\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for name in subset_uci:\n",
    "\t# 60% Train, 20% Calibration, 20% Test dataset\n",
    "\tdataset = get_classification_datasets(name, val_fraction=0.2, test_fraction=0.2, split_seed=0, normalize=True, verbose=True)\n",
    "\t\n",
    "\ttrain_dataset, cal_dataset, test_dataset = dataset\n",
    "\tX_train, y_train = train_dataset[:][0], train_dataset[:][1]\n",
    "\tX_cal, y_cal = cal_dataset[:][0], cal_dataset[:][1]\n",
    "\tX_test, y_test = test_dataset[:][0], test_dataset[:][1]\n",
    "\t\n",
    "\t# Simple logistic regression classifier trained\n",
    "\treg = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\tprint(\"==\"*25)\n",
    "\tprint(f\"Classification accuracy on Train: {reg.score(X_train, y_train):.2}\")\n",
    "\tprint(f\"Classification accuracy on Test: {reg.score(X_test, y_test):.2}\")\n",
    "\tprint(\"==\"*25)\n",
    "\n",
    "\n",
    "\t# Predict probabilistic outcome on K classes, on the calibration and test datasets \n",
    "\tpred_cal = torch.Tensor(reg.predict_proba(X_cal.numpy()))\n",
    "\n",
    "\tpred_test = torch.Tensor(reg.predict_proba(X_test.numpy()))\n",
    "\n",
    "\t\n",
    "\n",
    "\t# Use the DiscreteDistCalibrator class and train it on the calibration dataset\n",
    "\n",
    "\tcalibrator = DiscreteDistCalibrator(verbose=True)\n",
    "\tcalibrator.train(pred_cal, torch.Tensor(y_cal))\n",
    "\n",
    "\toutput_cal = calibrator(pred_cal)\n",
    "\toutput_test = calibrator(pred_test)\n",
    "\n",
    "\t# Evaluation\n",
    "\tprint(\"==\"*25)\n",
    "\tprint(f\"[Calibration Dataset] Log loss before calibration = {discrete_cal_loss(y_cal, pred_cal)}; After calibration={discrete_cal_loss(y_cal, output_cal)}\")\n",
    "\tprint(f\"[Calibration Dataset] Calibration score before calibration = {discrete_cal_score(y_cal, pred_cal)}, After calibration = {discrete_cal_score(y_cal, output_cal)}\")\n",
    "\n",
    "\tprint(f\"[Test Dataset] Log loss before calibration = {discrete_cal_loss(y_test, output_test)}; After calibration={discrete_cal_loss(y_test, output_test)}\")\n",
    "\tprint(f\"[Test Dataset] Calibration score before calibration = {discrete_cal_score(y_test, output_test)}, After calibration = {discrete_cal_score(y_test, output_test)}\")\n",
    "\tprint(\"==\"*25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchuq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
